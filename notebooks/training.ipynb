{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By claude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1900\n",
      "Test samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing functions\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, img_size=416):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "        \n",
    "        # Resize image\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Load label (assuming YOLO format: class x_center y_center width height)\n",
    "        label_name = img_name.replace('.jpg', '.txt').replace('.png', '.txt').replace('.jpeg', '.txt')\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "        \n",
    "        boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center = float(parts[1])\n",
    "                        y_center = float(parts[2])\n",
    "                        width = float(parts[3])\n",
    "                        height = float(parts[4])\n",
    "                        boxes.append([class_id, x_center, y_center, width, height])\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            boxes = [[0, 0.5, 0.5, 0.1, 0.1]]  # dummy box if no annotations\n",
    "            \n",
    "        boxes = np.array(boxes, dtype=np.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.FloatTensor(image).permute(2, 0, 1)\n",
    "            \n",
    "        return image, torch.FloatTensor(boxes)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = ObjectDetectionDataset(\n",
    "    image_dir='../data/penyisihan-hology-8-0-2025-data-mining/train/images',\n",
    "    label_dir='../data/penyisihan-hology-8-0-2025-data-mining/train/labels',\n",
    "    img_size=416\n",
    ")\n",
    "\n",
    "test_dataset = ObjectDetectionDataset(\n",
    "    image_dir='../data/penyisihan-hology-8-0-2025-data-mining/test/images',\n",
    "    label_dir='../data/penyisihan-hology-8-0-2025-data-mining/test/labels',\n",
    "    img_size=416\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1469861\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN-based object detector\n",
    "class SimpleObjectDetector(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(SimpleObjectDetector, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Feature extraction backbone\n",
    "        self.backbone = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # Detection head\n",
    "        # Output: [confidence, x_center, y_center, width, height] per grid cell\n",
    "        self.detection_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 5, 1)  # 5 = 1 confidence + 4 bbox coordinates\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        detections = self.detection_head(features)\n",
    "        return detections\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleObjectDetector(num_classes=1).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def calculate_loss(predictions, targets, lambda_coord=5.0, lambda_noobj=0.5):\n",
    "    \"\"\"\n",
    "    Simple loss function for object detection\n",
    "    predictions: [batch, 5, grid_h, grid_w]\n",
    "    targets: [batch, max_objects, 5] (class, x, y, w, h)\n",
    "    \"\"\"\n",
    "    batch_size = predictions.size(0)\n",
    "    grid_h, grid_w = predictions.size(2), predictions.size(3)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        pred = predictions[b]  # [5, grid_h, grid_w]\n",
    "        target = targets[b]    # [max_objects, 5]\n",
    "        \n",
    "        # Simple approach: assign each target to nearest grid cell\n",
    "        for obj in target:\n",
    "            if obj[0] >= 0:  # valid object\n",
    "                # Convert normalized coordinates to grid coordinates\n",
    "                grid_x = int(obj[1] * grid_w)\n",
    "                grid_y = int(obj[2] * grid_h)\n",
    "                grid_x = min(max(grid_x, 0), grid_w - 1)\n",
    "                grid_y = min(max(grid_y, 0), grid_h - 1)\n",
    "                \n",
    "                # Get prediction for this grid cell\n",
    "                pred_conf = torch.sigmoid(pred[0, grid_y, grid_x])\n",
    "                pred_x = torch.sigmoid(pred[1, grid_y, grid_x])\n",
    "                pred_y = torch.sigmoid(pred[2, grid_y, grid_x])\n",
    "                pred_w = pred[3, grid_y, grid_x]\n",
    "                pred_h = pred[4, grid_y, grid_x]\n",
    "                \n",
    "                # Target values\n",
    "                target_conf = 1.0\n",
    "                target_x = obj[1] * grid_w - grid_x\n",
    "                target_y = obj[2] * grid_h - grid_y\n",
    "                target_w = torch.log(obj[3] + 1e-8)\n",
    "                target_h = torch.log(obj[4] + 1e-8)\n",
    "                \n",
    "                # Calculate losses\n",
    "                conf_loss = (pred_conf - target_conf) ** 2\n",
    "                coord_loss = (pred_x - target_x) ** 2 + (pred_y - target_y) ** 2\n",
    "                size_loss = (pred_w - target_w) ** 2 + (pred_h - target_h) ** 2\n",
    "                \n",
    "                total_loss += conf_loss + lambda_coord * (coord_loss + size_loss)\n",
    "    \n",
    "    return total_loss / batch_size\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    # Pad targets to same length\n",
    "    max_objects = max(len(t) for t in targets)\n",
    "    padded_targets = []\n",
    "    for target in targets:\n",
    "        if len(target) < max_objects:\n",
    "            padding = torch.full((max_objects - len(target), 5), -1)\n",
    "            target = torch.cat([target, padding])\n",
    "        padded_targets.append(target)\n",
    "    \n",
    "    targets = torch.stack(padded_targets)\n",
    "    return images, targets\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 0, Loss: 58.5837\n",
      "Epoch 1/10, Batch 10, Loss: 2.2620\n",
      "Epoch 1/10, Batch 20, Loss: 5.8031\n",
      "Epoch 1/10, Batch 30, Loss: 1.0368\n",
      "Epoch 1/10, Batch 40, Loss: 0.6572\n",
      "Epoch 1/10, Batch 50, Loss: 0.3007\n",
      "Epoch 1/10, Batch 60, Loss: 0.1756\n",
      "Epoch 1/10, Batch 70, Loss: 1.4898\n",
      "Epoch 1/10, Batch 80, Loss: 1.1880\n",
      "Epoch 1/10, Batch 90, Loss: 0.2016\n",
      "Epoch 1/10, Batch 100, Loss: 0.5907\n",
      "Epoch 1/10, Batch 110, Loss: 0.3762\n",
      "Epoch 1/10, Batch 120, Loss: 1.2543\n",
      "Epoch 1/10, Batch 130, Loss: 0.3786\n",
      "Epoch 1/10, Batch 140, Loss: 1.0079\n",
      "Epoch 1/10, Batch 150, Loss: 0.7158\n",
      "Epoch 1/10, Batch 160, Loss: 0.7488\n",
      "Epoch 1/10, Batch 170, Loss: 0.2759\n",
      "Epoch 1/10, Batch 180, Loss: 2.1280\n",
      "Epoch 1/10, Batch 190, Loss: 0.1947\n",
      "Epoch 1/10, Batch 200, Loss: 1.0549\n",
      "Epoch 1/10, Batch 210, Loss: 0.2264\n",
      "Epoch 1/10, Batch 220, Loss: 0.3937\n",
      "Epoch 1/10, Batch 230, Loss: 0.2216\n",
      "Epoch 1/10, Batch 240, Loss: 0.3559\n",
      "Epoch 1/10, Batch 250, Loss: 1.2183\n",
      "Epoch 1/10, Batch 260, Loss: 0.1152\n",
      "Epoch 1/10, Batch 270, Loss: 2.1991\n",
      "Epoch 1/10, Batch 280, Loss: 0.0580\n",
      "Epoch 1/10, Batch 290, Loss: 0.0790\n",
      "Epoch 1/10, Batch 300, Loss: 1.1261\n",
      "Epoch 1/10, Batch 310, Loss: 0.1208\n",
      "Epoch 1/10, Batch 320, Loss: 0.1403\n",
      "Epoch 1/10, Batch 330, Loss: 0.0791\n",
      "Epoch 1/10, Batch 340, Loss: 0.7187\n",
      "Epoch 1/10, Batch 350, Loss: 0.5537\n",
      "Epoch 1/10, Batch 360, Loss: 0.1414\n",
      "Epoch 1/10, Batch 370, Loss: 0.0211\n",
      "Epoch 1/10, Batch 380, Loss: 0.1778\n",
      "Epoch 1/10, Batch 390, Loss: 0.3755\n",
      "Epoch 1/10, Batch 400, Loss: 0.2187\n",
      "Epoch 1/10, Batch 410, Loss: 0.0277\n",
      "Epoch 1/10, Batch 420, Loss: 0.0501\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss = calculate_loss(predictions, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} completed. Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and visualization functions\n",
    "def predict_objects(model, image, conf_threshold=0.5, img_size=416):\n",
    "    \"\"\"\n",
    "    Predict objects in an image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess image\n",
    "    if isinstance(image, str):\n",
    "        image = cv2.imread(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    resized_image = cv2.resize(image, (img_size, img_size))\n",
    "    input_tensor = torch.FloatTensor(resized_image).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_tensor)\n",
    "    \n",
    "    # Post-process predictions\n",
    "    predictions = predictions.squeeze(0)  # Remove batch dimension\n",
    "    grid_h, grid_w = predictions.size(1), predictions.size(2)\n",
    "    \n",
    "    detections = []\n",
    "    for i in range(grid_h):\n",
    "        for j in range(grid_w):\n",
    "            conf = torch.sigmoid(predictions[0, i, j]).item()\n",
    "            \n",
    "            if conf > conf_threshold:\n",
    "                # Get bbox coordinates\n",
    "                x_offset = torch.sigmoid(predictions[1, i, j]).item()\n",
    "                y_offset = torch.sigmoid(predictions[2, i, j]).item()\n",
    "                w = torch.exp(predictions[3, i, j]).item()\n",
    "                h = torch.exp(predictions[4, i, j]).item()\n",
    "                \n",
    "                # Convert to absolute coordinates\n",
    "                x_center = (j + x_offset) / grid_w\n",
    "                y_center = (i + y_offset) / grid_h\n",
    "                \n",
    "                # Convert to original image coordinates\n",
    "                x_center *= orig_w\n",
    "                y_center *= orig_h\n",
    "                width = w * orig_w / grid_w\n",
    "                height = h * orig_h / grid_h\n",
    "                \n",
    "                x1 = int(x_center - width/2)\n",
    "                y1 = int(y_center - height/2)\n",
    "                x2 = int(x_center + width/2)\n",
    "                y2 = int(y_center + height/2)\n",
    "                \n",
    "                detections.append({\n",
    "                    'confidence': conf,\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'center': [x_center, y_center],\n",
    "                    'size': [width, height]\n",
    "                })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def visualize_predictions(image_path, detections, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize predictions on image\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det['bbox']\n",
    "        conf = det['confidence']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                           fill=False, color='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        # Add confidence text\n",
    "        plt.text(x1, y1-5, f'Conf: {conf:.3f}', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"red\", alpha=0.7),\n",
    "                fontsize=10, color='white')\n",
    "    \n",
    "    plt.title(f'Object Detection Results ({len(detections)} objects detected)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on sample images\n",
    "model.eval()\n",
    "\n",
    "# Get a sample test image\n",
    "test_image_dir = 'penyisihan-hology-8-0-2025-data-mining/test/images'\n",
    "sample_images = [f for f in os.listdir(test_image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "if sample_images:\n",
    "    # Test on first few images\n",
    "    for i, img_name in enumerate(sample_images[:3]):\n",
    "        img_path = os.path.join(test_image_dir, img_name)\n",
    "        print(f\"\\nTesting on image: {img_name}\")\n",
    "        \n",
    "        # Predict objects\n",
    "        detections = predict_objects(model, img_path, conf_threshold=0.3)\n",
    "        print(f\"Detected {len(detections)} objects\")\n",
    "        \n",
    "        for j, det in enumerate(detections):\n",
    "            print(f\"  Object {j+1}: Confidence = {det['confidence']:.3f}, \"\n",
    "                  f\"BBox = {det['bbox']}\")\n",
    "        \n",
    "        # Visualize results\n",
    "        visualize_predictions(img_path, detections)\n",
    "else:\n",
    "    print(\"No test images found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) of two bounding boxes\"\"\"\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x1_i = max(x1_1, x1_2)\n",
    "    y1_i = max(y1_1, y1_2)\n",
    "    x2_i = min(x2_1, x2_2)\n",
    "    y2_i = min(y2_1, y2_2)\n",
    "    \n",
    "    if x2_i <= x1_i or y2_i <= y1_i:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def evaluate_model(model, test_loader, iou_threshold=0.5):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                # Get predictions for single image\n",
    "                image = images[b:b+1].to(device)\n",
    "                # Convert tensor back to numpy for prediction function\n",
    "                img_np = (images[b].permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "                \n",
    "                detections = predict_objects(model, img_np, conf_threshold=0.3)\n",
    "                pred_boxes = [det['bbox'] for det in detections]\n",
    "                \n",
    "                # Get ground truth boxes\n",
    "                target = targets[b]\n",
    "                gt_boxes = []\n",
    "                for obj in target:\n",
    "                    if obj[0] >= 0:  # valid object\n",
    "                        # Convert normalized coordinates to absolute\n",
    "                        h, w = 416, 416  # assuming input size\n",
    "                        x_center = obj[1] * w\n",
    "                        y_center = obj[2] * h\n",
    "                        width = obj[3] * w\n",
    "                        height = obj[4] * h\n",
    "                        \n",
    "                        x1 = int(x_center - width/2)\n",
    "                        y1 = int(y_center - height/2)\n",
    "                        x2 = int(x_center + width/2)\n",
    "                        y2 = int(y_center + height/2)\n",
    "                        gt_boxes.append([x1, y1, x2, y2])\n",
    "                \n",
    "                # Calculate TP, FP, FN\n",
    "                matched_gt = set()\n",
    "                for pred_box in pred_boxes:\n",
    "                    best_iou = 0\n",
    "                    best_gt_idx = -1\n",
    "                    \n",
    "                    for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                        if gt_idx not in matched_gt:\n",
    "                            iou = calculate_iou(pred_box, gt_box)\n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_gt_idx = gt_idx\n",
    "                    \n",
    "                    if best_iou >= iou_threshold:\n",
    "                        total_tp += 1\n",
    "                        matched_gt.add(best_gt_idx)\n",
    "                    else:\n",
    "                        total_fp += 1\n",
    "                \n",
    "                total_fn += len(gt_boxes) - len(matched_gt)\n",
    "    \n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-Score: {f1_score:.3f}\")\n",
    "    print(f\"True Positives: {total_tp}\")\n",
    "    print(f\"False Positives: {total_fp}\")\n",
    "    print(f\"False Negatives: {total_fn}\")\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Evaluate the model\n",
    "precision, recall, f1_score = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'model_config': {\n",
    "        'num_classes': 1,\n",
    "        'img_size': 416\n",
    "    }\n",
    "}, 'simple_object_detector.pth')\n",
    "\n",
    "print(\"Model saved as 'simple_object_detector.pth'\")\n",
    "\n",
    "# Function to load model for inference\n",
    "def load_model(model_path):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = SimpleObjectDetector(num_classes=1).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "print(\"\\nObject Detection Pipeline Complete!\")\n",
    "print(\"You can now use the trained model to detect objects in new images.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
