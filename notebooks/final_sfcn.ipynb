{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112009,"databundleVersionId":13760973,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/FarrelAD/Hology-8-2025-Data-Mining-PRIVATE/blob/main/notebooks/vidi/best_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Spatial Fully Convolutional Network for Crowd Counting\n\nThis notebook implements SFCN with VGG16-BN backbone for crowd counting tasks.\n\n**Features:**\n- VGG16-BN backbone with spatial encoder\n- Adaptive density map generation with KNN-based sigma\n- Advanced data augmentation with patch-based training\n- Mixed precision training with gradient accumulation\n- Early stopping and model checkpointing\n- Optional empty patch avoidance for better training","metadata":{"id":"390f2cfc"}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{"id":"70a323fb"}},{"cell_type":"code","source":"# Import Required Libraries and Setup\nimport os\nimport re\nimport json\nimport math\nimport random\nimport argparse\nimport warnings\nfrom glob import glob\nfrom typing import List, Tuple\nimport sys\nimport platform\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import KDTree\n\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Constants\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\nprint(\"‚úÖ All libraries imported successfully!\")\n\n\n# Detect environment\ndef detect_environment():\n    \"\"\"Detect if running in Colab, Kaggle, or local environment\"\"\"\n    if 'google.colab' in sys.modules:\n        return 'colab'\n    elif 'kaggle_secrets' in sys.modules or os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n        return 'kaggle'\n    else:\n        return 'local'\n\nENV = detect_environment()\nprint(f\"üîç Detected environment: {ENV.upper()}\")\n\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")","metadata":{"id":"cfdbe9ee","outputId":"35db3420-8508-4016-c313-21a526f340e8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Download and Setup","metadata":{"id":"312c3af0"}},{"cell_type":"code","source":"# Environment-specific dataset setup\ndef setup_dataset_paths(env: str) -> dict[str, str]:\n    \"\"\"Setup dataset paths based on environment\"\"\"\n\n    if env == 'colab':\n        # Google Colab paths\n        dataset_name = \"penyisihan-hology-8-0-2025-data-mining\"\n        drive_path = \"/content/drive/MyDrive/PROJECTS/Cognivio/dataset\"\n        local_path = \"/content/dataset\"\n\n        # Download and setup dataset in Colab\n        if not os.path.exists(local_path):\n            print(\"üì• Setting up dataset in Colab...\")\n\n            # Check for kaggle.json in Drive first\n            kaggle_json_drive = \"/content/drive/MyDrive/kaggle.json\"\n            kaggle_json_local = \"/root/.kaggle/kaggle.json\"\n\n            if os.path.exists(kaggle_json_drive):\n                print('copy from drive')\n                # Copy from Drive\n                !mkdir -p /root/.kaggle\n                !cp \"{kaggle_json_drive}\" \"{kaggle_json_local}\"\n                !chmod 600 \"{kaggle_json_local}\"\n                print(\"‚úÖ Kaggle credentials loaded from Drive\")\n            else:\n                print(\"‚ö†Ô∏è  Please upload kaggle.json to your Google Drive root folder\")\n                print(\"   Or manually upload it when prompted below\")\n                from google.colab import files\n                uploaded = files.upload()\n                for fn in uploaded.keys():\n                    !mkdir -p /root/.kaggle\n                    !mv \"{fn}\" \"/root/.kaggle/kaggle.json\"\n                    !chmod 600 \"/root/.kaggle/kaggle.json\"\n                    print(f\"‚úÖ Kaggle credentials uploaded: {fn}\")\n\n            # Download dataset\n            !kaggle competitions download -c {dataset_name} -p /content\n\n            # Extract dataset\n            import zipfile\n            zip_path = f\"/content/{dataset_name}.zip\"\n            if os.path.exists(zip_path):\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(local_path)\n                print(f\"‚úÖ Dataset extracted to {local_path}\")\n            else:\n                raise FileNotFoundError(f\"Dataset zip not found: {zip_path}\")\n\n        return {\n            'img_dir': f\"{local_path}/train/images\",\n            'label_dir': f\"{local_path}/train/labels\",\n            'test_dir': f\"{local_path}/test/images\",\n            'save_dir': \"/content/drive/MyDrive/PROJECTS/Cognivio/models\"\n        }\n\n    elif env == 'kaggle':\n        # Kaggle paths\n        return {\n            'img_dir': \"/kaggle/input/penyisihan-hology-8-0-2025-data-mining/train/images\",\n            'label_dir': \"/kaggle/input/penyisihan-hology-8-0-2025-data-mining/train/labels\",\n            'test_dir': \"/kaggle/input/penyisihan-hology-8-0-2025-data-mining/test/images\",\n            'save_dir': \"/kaggle/working\"\n        }\n\n    else:  # local\n        # Local paths - modify these for your setup\n        base_path = \"data\"  # Change this to your local dataset path\n        return {\n            'img_dir': f\"{base_path}/train/images\",\n            'label_dir': f\"{base_path}/train/labels\",\n            'test_dir': f\"{base_path}/test/images\",\n            'save_dir': \"models\"\n        }\n\n# Setup paths\npaths = setup_dataset_paths(ENV)\nprint(f\"üìÅ Dataset paths configured for {ENV}:\")\nfor key, path in paths.items():\n    exists = \"‚úÖ\" if os.path.exists(path) else \"‚ö†Ô∏è\"\n    print(f\"   {key}: {path} {exists}\")\n\n# Create save directory\nos.makedirs(paths['save_dir'], exist_ok=True)","metadata":{"id":"0dc608aa","outputId":"2cca7823-3306-4828-c940-6480aa4073b9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Configuration","metadata":{"id":"2fc7e279"}},{"cell_type":"code","source":"# Training Configuration\nconfig = {\n    # Data paths (modify these for local setup)\n    'img_dir': paths['img_dir'],\n    'label_dir': paths['label_dir'],\n    'test_dir': paths['test_dir'],\n    'save_dir': paths['save_dir'],\n\n    # Model parameters\n    'base_size': 1024,\n    'down': 8,\n    'patch_size': 384,\n    'patches_per_image': 4,\n    'avoid_empty_patches': True,\n\n    # Training parameters\n    'batch_size': 12, # (default=12, testing=4) change to higher in high computational environment\n    'epochs': 120,  # (default=120, testing=2) Increase for full training\n    'lr': 2e-4, # or 1e-4\n    'criterion': 'mse',  # 'mse' or 'huber'\n    'count_loss_alpha': 0.0,  # Auxiliary count loss weight\n    'early_stop_patience': 15,\n\n    # Optimization\n    'num_workers': 4, # change to higher in high computational environment\n    'amp': True,  # Automatic Mixed Precision\n    'accum_steps': 2,  # Gradient accumulation steps\n\n    # Augmentation and preprocessing\n    'sigma_mode': 'adaptive',  # 'adaptive' or 'constant'\n\n    # Model saving\n    'save_path': os.path.join(paths['save_dir'], 'sfcn_best.pth'),\n    'seed': 1337\n}\n\nprint(\"üìã Configuration loaded:\")\nfor key, value in config.items():\n    print(f\"  {key}: {value}\")\n\n# Set random seed for reproducibility\ndef set_seed(seed: int = 1337) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(config['seed'])\nprint(f\"üé≤ Random seed set to {config['seed']}\")\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üîß Using device: {device}\")","metadata":{"id":"c876700e","outputId":"7772115e-f621-4311-9b72-cc4064e3e7b0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utility Functions","metadata":{"id":"2449169e"}},{"cell_type":"code","source":"def set_seed(\n    seed: int = 1337\n) -> None:\n    \"\"\"Set random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef imread_rgb(\n    path: str\n) -> np.ndarray:\n    \"\"\"Read an RGB image using OpenCV and convert BGR‚ÜíRGB.\"\"\"\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    if img is None:\n        raise ValueError(f\"Cannot read image: {path}\")\n    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\ndef letterbox(\n    img: np.ndarray,\n    target: int = 512\n) -> Tuple[np.ndarray, float, int, int]:\n    \"\"\"Resize and pad an image to a square canvas without distortion.\n\n    Returns the padded image, the scale factor used, and the left/top\n    padding applied. The output size is (target, target).\n    \"\"\"\n    h, w = img.shape[:2]\n    scale = min(target / h, target / w)\n    nh, nw = int(round(h * scale)), int(round(w * scale))\n    img_rs = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n    top = (target - nh) // 2\n    left = (target - nw) // 2\n    canvas = np.zeros((target, target, 3), dtype=img_rs.dtype)\n    canvas[top:top + nh, left:left + nw] = img_rs\n    return canvas, scale, left, top\n\ndef parse_points_from_json(\n    path: str\n) -> Tuple[np.ndarray, int]:\n    \"\"\"Parse annotated points from a JSON file.\n\n    Supports several common crowd counting annotation formats. Returns an\n    array of shape (N, 2) containing [x, y] coordinates and, if present\n    in the JSON, the declared number of people.\n    \"\"\"\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        obj = json.load(f)\n    pts: List[List[float]]\n    num = None\n    if isinstance(obj, dict) and \"points\" in obj:\n        pts = obj[\"points\"]\n        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n        if len(pts) > 0 and isinstance(pts[0], dict):\n            pts = [[p[\"x\"], p[\"y\"]] for p in pts if \"x\" in p and \"y\" in p]\n    elif isinstance(obj, dict) and \"annotations\" in obj:\n        pts = [[a[\"x\"], a[\"y\"]] for a in obj[\"annotations\"] if \"x\" in a and \"y\" in a]\n        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n    elif isinstance(obj, list):\n        pts = obj\n    else:\n        raise ValueError(f\"Unknown JSON schema: {path}\")\n    pts_arr = np.array(pts, dtype=np.float32) if len(pts) > 0 else np.zeros((0, 2), np.float32)\n    return pts_arr, num\n\ndef derive_json_path(\n    lbl_dir: str,\n    img_path: str\n) -> str:\n    \"\"\"Derive the corresponding JSON label path for a given image path.\"\"\"\n    name = os.path.splitext(os.path.basename(img_path))[0]\n    cand = os.path.join(lbl_dir, name + \".json\")\n    if os.path.exists(cand):\n        return cand\n    # Try matching trailing digits\n    m = re.findall(r\"\\d+\", name)\n    if m:\n        alt = os.path.join(lbl_dir, f\"{m[-1]}.json\")\n        if os.path.exists(alt):\n            return alt\n    # Fallback: any file starting with the same name\n    lst = glob(os.path.join(lbl_dir, f\"{name}*.json\"))\n    if lst:\n        return lst[0]\n    raise FileNotFoundError(f\"JSON label not found for {img_path}\")\n\ndef make_density_map(\n    points_xy: np.ndarray,\n    grid_size: int,\n    down: int = 8,\n    sigma_mode: str = \"adaptive\",\n    knn: int = 3,\n    beta: float = 0.3,\n    const_sigma: float = 2.0,\n) -> np.ndarray:\n    \"\"\"Generate a density map on a grid given annotated points.\n\n    The density map is of shape (grid_size//down, grid_size//down). Each\n    point is represented by a Gaussian whose sigma is either constant or\n    computed from the k-nearest neighbours.\n    \"\"\"\n    target = grid_size\n    dh, dw = target // down, target // down\n    den = np.zeros((dh, dw), dtype=np.float32)\n\n    if len(points_xy) == 0:\n        return den\n\n    # Scale points to the density map resolution\n    pts = points_xy.copy()\n    pts[:, 0] = pts[:, 0] * (dw / target)\n    pts[:, 1] = pts[:, 1] * (dh / target)\n    tree = KDTree(pts) if len(pts) > 1 else None\n\n    for (x, y) in pts:\n        # Determine sigma\n        if sigma_mode == \"adaptive\" and tree is not None and len(pts) > 3:\n            dists, _ = tree.query([x, y], k=min(knn + 1, len(pts)))\n            sigma = max(1.0, float(np.mean(dists[1:])) * beta)\n        else:\n            sigma = const_sigma\n\n        cx, cy = float(x), float(y)\n        rad = int(max(1, math.ceil(3 * sigma)))\n        x0, x1 = max(0, int(math.floor(cx - rad))), min(dw, int(math.ceil(cx + rad + 1)))\n        y0, y1 = max(0, int(math.floor(cy - rad))), min(dh, int(math.ceil(cy + rad + 1)))\n\n        if x1 <= x0 or y1 <= y0:\n            continue\n\n        xs = np.arange(x0, x1) - cx\n        ys = np.arange(y0, y1) - cy\n        xx, yy = np.meshgrid(xs, ys)\n        g = np.exp(-(xx**2 + yy**2) / (2 * sigma * sigma))\n        s = g.sum()\n\n        if s > 0:\n            den[y0:y1, x0:x1] += (g / s).astype(np.float32)\n\n    return den\n\nprint(\"‚úÖ Basic utility functions defined!\")","metadata":{"id":"8082edd2","outputId":"fff1f142-76a5-4d03-822c-cfe6ce80d19c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Implementation","metadata":{"id":"5ed13f38"}},{"cell_type":"code","source":"class CrowdDataset(Dataset):\n    \"\"\"Custom dataset for crowd counting.\n\n    Supports optional random patch cropping on training data. When\n    `avoid_empty_patches` is set, the dataset will attempt to choose a\n    patch containing at least one point, falling back to a random crop if\n    it fails after a number of retries.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_dir: str,\n        lbl_dir: str,\n        base_size: int = 768,\n        down: int = 8,\n        aug: bool = True,\n        mode: str = \"train\",\n        patch_size: int = 0,\n        patches_per_image: int = 1,\n        sigma_mode: str = \"adaptive\",\n        avoid_empty_patches: bool = False,\n    ) -> None:\n        super().__init__()\n        self.img_paths = sorted(glob(os.path.join(img_dir, \"*.*\")))\n        if len(self.img_paths) == 0:\n            raise ValueError(f\"No images found in {img_dir}\")\n        self.lbl_dir = lbl_dir\n        self.base_size = base_size\n        self.down = down\n        self.aug = aug\n        self.mode = mode\n        self.patch_size = patch_size\n        self.patches_per_image = max(1, int(patches_per_image))\n        self.sigma_mode = sigma_mode\n        self.avoid_empty_patches = avoid_empty_patches\n\n        self.to_tensor = transforms.ToTensor()\n        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n        self.color_jit = transforms.ColorJitter(0.1, 0.1, 0.1, 0.05)\n\n        # Compute effective length for patch training\n        if self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1:\n            self.effective_len = len(self.img_paths) * self.patches_per_image\n        else:\n            self.effective_len = len(self.img_paths)\n\n    def __len__(self) -> int:\n        return self.effective_len\n\n    def _load_img_pts(\n        self,\n        idx_base: int\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Load an image and its transformed annotation points.\"\"\"\n        pimg = self.img_paths[idx_base]\n        img = imread_rgb(pimg)\n        h, w = img.shape[:2]\n        plbl = derive_json_path(self.lbl_dir, pimg)\n        pts, _ = parse_points_from_json(plbl)\n\n        # Augment: horizontal flip\n        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n            img = img[:, ::-1, :].copy()\n            if len(pts) > 0:\n                pts = pts.copy()\n                pts[:, 0] = (w - 1) - pts[:, 0]\n\n        # Augment: colour jitter\n        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n            pil = transforms.ToPILImage()(img)\n            pil = self.color_jit(pil)\n            img = np.array(pil)\n\n        # Letterbox to base size\n        canvas, scale, left, top = letterbox(img, target=self.base_size)\n        if len(pts) > 0:\n            pts_tr = pts.copy()\n            pts_tr[:, 0] = pts_tr[:, 0] * scale + left\n            pts_tr[:, 1] = pts_tr[:, 1] * scale + top\n            # Clamp to canvas bounds\n            m = (\n                (pts_tr[:, 0] >= 0)\n                & (pts_tr[:, 0] < self.base_size)\n                & (pts_tr[:, 1] >= 0)\n                & (pts_tr[:, 1] < self.base_size)\n            )\n            pts_tr = pts_tr[m]\n        else:\n            pts_tr = np.zeros((0, 2), np.float32)\n        return canvas, pts_tr\n\n    def __getitem__(\n        self,\n        index: int\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        # Map global index to an image index (for patch training)\n        if self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1:\n            idx_base = index // self.patches_per_image\n        else:\n            idx_base = index\n        idx_base %= len(self.img_paths)\n\n        img_lb, pts_tr = self._load_img_pts(idx_base)\n\n        # Optional patch cropping\n        if self.mode == \"train\" and self.patch_size > 0:\n            ps = self.patch_size\n            if ps > self.base_size:\n                raise ValueError(\"patch_size must be <= base_size\")\n\n            max_off = self.base_size - ps\n            pts_out = np.zeros((0, 2), np.float32)\n\n            # Attempt to find a non-empty patch when requested\n            for attempt in range(10) if self.avoid_empty_patches else [0]:\n                ox = 0 if max_off <= 0 else random.randint(0, max_off)\n                oy = 0 if max_off <= 0 else random.randint(0, max_off)\n                crop = img_lb[oy : oy + ps, ox : ox + ps, :]\n                if len(pts_tr) > 0:\n                    pts_c = pts_tr.copy()\n                    pts_c[:, 0] -= ox\n                    pts_c[:, 1] -= oy\n                    m = (\n                        (pts_c[:, 0] >= 0)\n                        & (pts_c[:, 0] < ps)\n                        & (pts_c[:, 1] >= 0)\n                        & (pts_c[:, 1] < ps)\n                    )\n                    pts_c = pts_c[m]\n                else:\n                    pts_c = np.zeros((0, 2), np.float32)\n                # If avoid_empty_patches is False, we break immediately (no retries)\n                if not self.avoid_empty_patches or len(pts_c) > 0 or attempt == 9:\n                    pts_out = pts_c\n                    img_out = crop\n                    break\n            grid = ps\n        else:\n            img_out = img_lb\n            pts_out = pts_tr\n            grid = self.base_size\n\n        # Build density map\n        den = make_density_map(\n            pts_out,\n            grid_size=grid,\n            down=self.down,\n            sigma_mode=self.sigma_mode,\n        )\n\n        # Convert to tensors and normalise\n        t = self.to_tensor(img_out)\n        t = self.normalize(t)\n        d = torch.from_numpy(den).unsqueeze(0)\n        c = torch.tensor([float(len(pts_out))], dtype=torch.float32)\n        return t, d, c\n\nprint(\"‚úÖ CrowdDataset class implemented!\")","metadata":{"id":"a4c87e4a","outputId":"732b0313-ee47-4662-92e1-bf7a8bdb8800"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture","metadata":{"id":"09b01a97"}},{"cell_type":"code","source":"class SpatialEncoder(nn.Module):\n    \"\"\"Simple spatial encoder that propagates information in four directions.\"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        k: int = 9\n    ) -> None:\n        super().__init__()\n        p = k // 2\n        self.h1 = nn.Conv2d(\n            in_channels=channels,\n            out_channels=channels,\n            kernel_size=(1, k),\n            padding=(0, p),\n            groups=channels,\n            bias=False\n        )\n        self.h2 = nn.Conv2d(\n            in_channels=channels,\n            out_channels=channels,\n            kernel_size=(1, k),\n            padding=(0, p),\n            groups=channels,\n            bias=False\n        )\n        self.v1 = nn.Conv2d(\n            in_channels=channels,\n            out_channels=channels,\n            kernel_size=(k, 1),\n            padding=(p, 0),\n            groups=channels,\n            bias=False\n        )\n        self.v2 = nn.Conv2d(\n            in_channels=channels,\n            out_channels=channels,\n            kernel_size=(k, 1),\n            padding=(p, 0),\n            groups=channels,\n            bias=False\n        )\n        self.proj = nn.Conv2d(\n            in_channels=channels * 4,\n            out_channels=channels,\n            kernel_size=1,\n            bias=False\n        )\n        self.act = nn.ReLU(inplace=True)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        y = torch.cat([self.h1(x), self.h2(x), self.v1(x), self.v2(x)], dim=1)\n        return self.act(self.proj(y))\n\n\nclass SFCN_VGG(nn.Module):\n    \"\"\"Simplified SFCN with VGG-16 backbone and spatial encoder.\"\"\"\n\n    def __init__(self, pretrained: bool = True) -> None:\n        super().__init__()\n        vgg = models.vgg16_bn(\n            weights=models.VGG16_BN_Weights.IMAGENET1K_V1 if pretrained else None\n        )\n        # Use features up to conv4_3 (stride 8)\n        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n        self.senc = SpatialEncoder(512, k=9)\n        self.head = nn.Sequential(\n            nn.Conv2d(512, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, 1),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.frontend(x)\n        x = self.senc(x)\n        x = self.head(x)\n        return torch.nn.functional.softplus(x)\n\nprint(\"‚úÖ SFCN model architecture implemented!\")\n\n# Test model instantiation\nmodel_test = SFCN_VGG(pretrained=False)\nprint(f\"üìä Model parameters: {sum(p.numel() for p in model_test.parameters()):,}\")\n\n# Test forward pass\ndummy_input = torch.randn(1, 3, 384, 384)\nwith torch.no_grad():\n    output = model_test(dummy_input)\n    print(f\"üîç Input shape: {dummy_input.shape}\")\n    print(f\"üéØ Output shape: {output.shape}\")\n\ndel model_test, dummy_input, output","metadata":{"id":"907f78bd","outputId":"10babacb-f0e0-41d3-da55-bd2b9de47dd5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{"id":"9d333605"}},{"cell_type":"markdown","source":"## Training Utility Functions","metadata":{"id":"cd2d4797"}},{"cell_type":"code","source":"def train_epoch(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    optimizer: torch.optim.Optimizer,\n    scaler: torch.cuda.amp.GradScaler = None,\n    accum_steps: int = 1,\n    criterion: str = \"mse\",\n    count_loss_alpha: float = 0.0,\n) -> float:\n    \"\"\"Train the model for one epoch and return the mean absolute error.\"\"\"\n    model.train()\n\n    if criterion == \"mse\":\n        crit = nn.MSELoss()\n    elif criterion == \"huber\":\n        crit = nn.SmoothL1Loss()\n    else:\n        raise ValueError(\"criterion must be 'mse' or 'huber'\")\n\n    running_mae, nimg = 0.0, 0\n    total_pred_count, total_gt_count = 0.0, 0.0\n    optimizer.zero_grad(set_to_none=True)\n\n    for step, (imgs, dens, _) in enumerate(tqdm(loader, desc=\"Train\", leave=False), 1):\n        imgs = imgs.to(device)\n        dens = dens.to(device)\n        if scaler is not None:\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                preds = model(imgs)\n                map_loss = crit(preds, dens)\n                if count_loss_alpha > 0.0:\n                    pred_cnt = preds.sum(dim=(1, 2, 3))\n                    gt_cnt = dens.sum(dim=(1, 2, 3))\n                    cnt_loss = F.mse_loss(pred_cnt, gt_cnt)\n                    total_loss = (map_loss + count_loss_alpha * cnt_loss) / accum_steps\n                else:\n                    total_loss = map_loss / accum_steps\n\n            scaler.scale(total_loss).backward()\n            if step % accum_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n        else:\n            preds = model(imgs)\n            map_loss = crit(preds, dens)\n            if count_loss_alpha > 0.0:\n                pred_cnt = preds.sum(dim=(1, 2, 3))\n                gt_cnt = dens.sum(dim=(1, 2, 3))\n                cnt_loss = F.mse_loss(pred_cnt, gt_cnt)\n                total_loss = (map_loss + count_loss_alpha * cnt_loss) / accum_steps\n            else:\n                total_loss = map_loss / accum_steps\n\n            total_loss.backward()\n\n            if step % accum_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n\n        with torch.no_grad():\n            pc = preds.sum(dim=(1, 2, 3)).detach().cpu().numpy()\n            gc = dens.sum(dim=(1, 2, 3)).detach().cpu().numpy()\n            running_mae += np.abs(pc - gc).sum()\n            total_pred_count += pc.sum()\n            total_gt_count += gc.sum()\n            nimg += imgs.size(0)\n\n    avg_pred = total_pred_count / max(1, nimg)\n    avg_gt = total_gt_count / max(1, nimg)\n    print(f\"Avg pred count: {avg_pred:.1f} vs GT {avg_gt:.1f}\")\n    return running_mae / max(1, nimg)\n\n\n@torch.no_grad()\ndef evaluate(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Evaluate the model on a validation set and compute MAE and RMSE.\"\"\"\n    model.eval()\n    mae, mse, nimg = 0.0, 0.0, 0\n\n    for imgs, dens, _ in tqdm(loader, desc=\"Val\", leave=False):\n        imgs = imgs.to(device)\n        dens = dens.to(device)\n        pred = model(imgs)\n        diff = (pred.sum(dim=(1, 2, 3)) - dens.sum(dim=(1, 2, 3))).detach().cpu().numpy()\n        mae += np.abs(diff).sum()\n        mse += (diff ** 2).sum()\n        nimg += imgs.size(0)\n    return mae / max(1, nimg), math.sqrt(mse / max(1, nimg))\n\nprint(\"‚úÖ Training and evaluation functions implemented!\")","metadata":{"id":"5c48bc5a","outputId":"88d79164-bcb6-48c5-825d-68cf88297688"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading and Preparation","metadata":{"id":"1b5d33e4"}},{"cell_type":"code","source":"# Check if data directories exist\nimg_dir = config['img_dir']\nlabel_dir = config['label_dir']\n\nprint(f\"üîç Checking data directories...\")\nprint(f\"   Images: {img_dir} -> {'‚úÖ Exists' if os.path.exists(img_dir) else '‚ùå Not found'}\")\nprint(f\"   Labels: {label_dir} -> {'‚úÖ Exists' if os.path.exists(label_dir) else '‚ùå Not found'}\")\n\nif not os.path.exists(img_dir):\n    raise Exception(\"‚ùå Image directory not found. Please check the path and try again.\")\n\n# Build list of images and randomly shuffle before splitting\nall_imgs = sorted(glob(os.path.join(img_dir, \"*.*\")))\nrandom.shuffle(all_imgs)\nn_images = len(all_imgs)\n\nif n_images < 2:\n    raise ValueError(\"Need at least 2 images for training and validation\")\n\nn_val = max(1, int(0.1 * n_images))  # 10% for validation\nn_train = n_images - n_val\ntrain_imgs = all_imgs[:n_train]\nval_imgs = all_imgs[n_train:]\n\nprint(f\"üìä Data split: {n_train} train, {n_val} validation\")\n\n# Instantiate datasets\ntrain_ds = CrowdDataset(\n    config['img_dir'],\n    config['label_dir'],\n    base_size=config['base_size'],\n    down=config['down'],\n    aug=True,\n    mode=\"train\",\n    patch_size=config['patch_size'],\n    patches_per_image=config['patches_per_image'],\n    sigma_mode=config['sigma_mode'],\n    avoid_empty_patches=config['avoid_empty_patches'],\n)\nval_ds = CrowdDataset(\n    config['img_dir'],\n    config['label_dir'],\n    base_size=config['base_size'],\n    down=config['down'],\n    aug=False,\n    mode=\"val\",\n    patch_size=0,\n    patches_per_image=1,\n    sigma_mode=config['sigma_mode'],\n    avoid_empty_patches=False,\n)\n\n# Override image paths after shuffling\ntrain_ds.img_paths = train_imgs\nval_ds.img_paths = val_imgs\n\n# Recompute effective lengths for patch training\nif train_ds.mode == \"train\" and train_ds.patch_size > 0 and train_ds.patches_per_image > 1:\n    train_ds.effective_len = len(train_ds.img_paths) * train_ds.patches_per_image\nelse:\n    train_ds.effective_len = len(train_ds.img_paths)\n\nval_ds.effective_len = len(val_ds.img_paths)\n\nprint(f\"üì¶ Training dataset: {len(train_ds)} samples\")\nprint(f\"üì¶ Validation dataset: {len(val_ds)} samples\")\n\n# Data loaders\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=config['batch_size'],\n    shuffle=True,\n    num_workers=config['num_workers'],\n    pin_memory=True,\n    drop_last=True,\n)\nval_loader = DataLoader(\n    val_ds,\n    batch_size=config['batch_size'],\n    shuffle=False,\n    num_workers=config['num_workers'],\n    pin_memory=True,\n)\n\nprint(f\"üöÄ Data loaders created!\")\nprint(f\"   Train batches: {len(train_loader)}\")\nprint(f\"   Val batches: {len(val_loader)}\")","metadata":{"id":"34701acd","outputId":"ad3b9d66-9ba5-4334-aecb-8d63b3524480"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop","metadata":{"id":"152244f4"}},{"cell_type":"code","source":"# Initialize model and training components\nprint(\"üèóÔ∏è  Initializing model and training components...\")\n\n# Create model\nmodel = SFCN_VGG(pretrained=True).to(device)\nprint(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Optimizer and scheduler\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=config['lr'],\n    weight_decay=1e-4\n)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=config['epochs']\n)\n\n# Mixed precision scaler\nscaler = torch.cuda.amp.GradScaler() if (config['amp'] and device.type == \"cuda\") else None\nprint(f\"‚ö° Mixed precision: {'Enabled' if scaler is not None else 'Disabled'}\")\n\n# Training state\nbest_mae = float(\"inf\")\npatience = config['early_stop_patience']\nbad_epochs = 0\ntraining_history = {\n    'train_mae': [],\n    'val_mae': [],\n    'val_rmse': [],\n    'epoch': []\n}\n\nprint(\"‚úÖ Setup complete! Starting training...\")\nprint(\"=\"*60)","metadata":{"id":"503e3cc2","outputId":"7d2d91f4-e003-4325-bbb0-b3134075eaf3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main Training Loop\nfor epoch in range(1, config['epochs'] + 1):\n    print(f\"\\nüìà Epoch {epoch}/{config['epochs']}\")\n\n    # Training phase\n    tr_mae = train_epoch(\n        model,\n        train_loader,\n        device,\n        optimizer,\n        scaler,\n        accum_steps=config['accum_steps'],\n        criterion=config['criterion'],\n        count_loss_alpha=config['count_loss_alpha'],\n    )\n\n    # Validation phase\n    va_mae, va_rmse = evaluate(model, val_loader, device)\n\n    # Learning rate scheduling\n    scheduler.step()\n\n    # Update training history\n    training_history['train_mae'].append(tr_mae)\n    training_history['val_mae'].append(va_mae)\n    training_history['val_rmse'].append(va_rmse)\n    training_history['epoch'].append(epoch)\n\n    # Print metrics\n    print(f\"üìä Train MAE: {tr_mae:.3f} | Val MAE: {va_mae:.3f} | Val RMSE: {va_rmse:.3f}\")\n    print(f\"üéØ Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n\n    # Model checkpointing\n    if va_mae + 1e-6 < best_mae:\n        best_mae = va_mae\n        bad_epochs = 0\n\n        # Save checkpoint\n        checkpoint = {\n            \"model\": model.state_dict(),\n            \"epoch\": epoch,\n            \"val_mae\": va_mae,\n            \"config\": config,\n            \"training_history\": training_history\n        }\n        torch.save(checkpoint, config['save_path'])\n        print(f\"‚úÖ New best model saved! MAE: {best_mae:.3f}\")\n    else:\n        bad_epochs += 1\n        print(f\"‚è≥ No improvement for {bad_epochs} epoch(s)\")\n\n        # Try to remove early stopping\n        # if bad_epochs >= patience:\n        #     print(f\"üõë Early stopping at epoch {epoch}. Best Val MAE: {best_mae:.3f}\")\n        #     break\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"üéâ Training completed! Best validation MAE: {best_mae:.3f}\")\nprint(f\"üíæ Best model saved to: {config['save_path']}\")","metadata":{"id":"kveGEJdfsfwK","outputId":"8cedca45-9b0e-44a9-f1e4-455a49f49988"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results Visualization and Analysis","metadata":{"id":"55052e6c"}},{"cell_type":"markdown","source":"## Summarize Performance","metadata":{"id":"f2e33735"}},{"cell_type":"code","source":"# Plot training curves\nif not len(training_history['epoch']):\n    raise Exception(\"‚ö†Ô∏è  No training history available. Please run the training loop first.\")\n\nplt.figure(figsize=(15, 5))\n\n# Training and validation MAE\nplt.subplot(1, 3, 1)\nplt.plot(training_history['epoch'], training_history['train_mae'], 'b-', label='Train MAE', linewidth=2)\nplt.plot(training_history['epoch'], training_history['val_mae'], 'r-', label='Val MAE', linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('MAE')\nplt.title('Training Progress')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Validation RMSE\nplt.subplot(1, 3, 2)\nplt.plot(training_history['epoch'], training_history['val_rmse'], 'g-', label='Val RMSE', linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('RMSE')\nplt.title('Validation RMSE')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Learning curve analysis\nplt.subplot(1, 3, 3)\ntrain_mae = np.array(training_history['train_mae'])\nval_mae = np.array(training_history['val_mae'])\ngap = val_mae - train_mae\nplt.plot(training_history['epoch'], gap, 'purple', label='Val-Train Gap', linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('MAE Gap')\nplt.title('Generalization Gap')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print final statistics\nprint(\"üìà Training Statistics:\")\nprint(f\"   Final Train MAE: {training_history['train_mae'][-1]:.3f}\")\nprint(f\"   Final Val MAE: {training_history['val_mae'][-1]:.3f}\")\nprint(f\"   Final Val RMSE: {training_history['val_rmse'][-1]:.3f}\")\nprint(f\"   Best Val MAE: {min(training_history['val_mae']):.3f}\")\nprint(f\"   Total Epochs: {len(training_history['epoch'])}\")","metadata":{"id":"87b6a0b9","outputId":"4b523e98-a6bb-4d20-aa73-722d741ea34b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation Coefficient","metadata":{"id":"d80e92c7"}},{"cell_type":"code","source":"# Load best model for final evaluation\nif os.path.exists(config['save_path']):\n    print(\"üìÅ Loading best model checkpoint...\")\n    checkpoint = torch.load(config['save_path'], map_location=device, weights_only=False)\n    model.load_state_dict(checkpoint[\"model\"])\n    print(f\"‚úÖ Loaded model from epoch {checkpoint['epoch']} with MAE {checkpoint['val_mae']:.3f}\")\n\n# Final evaluation with detailed predictions\nprint(\"\\nüéØ Final Evaluation - Ground Truth vs Predictions:\")\nprint(\"=\"*60)\n\nmodel.eval()\npredictions = []\nground_truths = []\nimage_names = []\n\nwith torch.no_grad():\n    for batch_idx, (imgs, dens, _) in enumerate(val_loader):\n        imgs = imgs.to(device)\n        dens = dens.to(device)\n        pred = model(imgs)\n\n        pred_cnt_batch = pred.sum((1, 2, 3))\n        gt_cnt_batch = dens.sum((1, 2, 3))\n\n        # Store results\n        for j in range(pred_cnt_batch.size(0)):\n            idx = batch_idx * config['batch_size'] + j\n            if idx < len(val_ds.img_paths):\n                image_name = os.path.basename(val_ds.img_paths[idx])\n                image_names.append(image_name)\n                predictions.append(pred_cnt_batch[j].item())\n                ground_truths.append(gt_cnt_batch[j].item())\n\n                print(f\"{image_name:20s}: GT {gt_cnt_batch[j].item():6.1f}, Pred {pred_cnt_batch[j].item():6.1f}, \"\n                      f\"Error {abs(gt_cnt_batch[j].item() - pred_cnt_batch[j].item()):5.1f}\")\n\n# Calculate final metrics\nif len(predictions) <= 0:\n    raise Exception(\"‚ö†Ô∏è  No validation data processed.\")\n\npredictions = np.array(predictions)\nground_truths = np.array(ground_truths)\n\nmae = np.mean(np.abs(predictions - ground_truths))\nmse = np.mean((predictions - ground_truths)**2)\nrmse = np.sqrt(mse)\n\nprint(\"\\nüìä Final Metrics:\")\nprint(f\"   MAE:  {mae:.3f}\")\nprint(f\"   MSE:  {mse:.3f}\")\nprint(f\"   RMSE: {rmse:.3f}\")\n\n# Scatter plot\nplt.figure(figsize=(10, 8))\nplt.scatter(ground_truths, predictions, alpha=0.6, s=50)\n\n# Perfect prediction line\nmax_val = max(np.max(ground_truths), np.max(predictions))\nmin_val = min(np.min(ground_truths), np.min(predictions))\nplt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n\nplt.xlabel('Ground Truth Count')\nplt.ylabel('Predicted Count')\nplt.title(f'Ground Truth vs Predictions (MAE: {mae:.2f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Add correlation coefficient\ncorrelation = np.corrcoef(ground_truths, predictions)[0, 1]\nplt.text(\n    0.05,\n    0.95,\n    f'Correlation: {correlation:.3f}',\n    transform=plt.gca().transAxes,\n    bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5)\n)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüîó Correlation coefficient: {correlation:.3f}\")","metadata":{"id":"ed09f209","outputId":"65c5720c-f836-4b94-f317-5bc3c14fe65e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Density Map Visualization","metadata":{"id":"9525b741"}},{"cell_type":"code","source":"# Visualize sample predictions with density maps\ndef visualize_predictions(\n    model,\n    dataset,\n    device,\n    num_samples=3\n) -> None:\n    \"\"\"Visualize model predictions with density maps.\"\"\"\n    model.eval()\n\n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n    if num_samples == 1:\n        axes = axes.reshape(1, -1)\n\n    with torch.no_grad():\n        for i in range(num_samples):\n            if i >= len(dataset):\n                break\n\n            # Get sample\n            img_tensor, gt_density, gt_count = dataset[i]\n            img_tensor = img_tensor.unsqueeze(0).to(device)\n\n            # Model prediction\n            pred_density = model(img_tensor)\n\n            # Convert to numpy\n            img_np = img_tensor.squeeze().cpu().permute(1, 2, 0).numpy()\n            img_np = img_np * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)  # Denormalize\n            img_np = np.clip(img_np, 0, 1)\n\n            gt_density_np = gt_density.squeeze().numpy()\n            pred_density_np = pred_density.squeeze().cpu().numpy()\n\n            # Calculate counts\n            gt_count_val = gt_density_np.sum()\n            pred_count_val = pred_density_np.sum()\n\n            # Plot original image\n            axes[i, 0].imshow(img_np)\n            axes[i, 0].set_title(f'Original Image')\n            axes[i, 0].axis('off')\n\n            # Plot ground truth density\n            im1 = axes[i, 1].imshow(gt_density_np, cmap='jet')\n            axes[i, 1].set_title(f'GT Density (Count: {gt_count_val:.1f})')\n            axes[i, 1].axis('off')\n            plt.colorbar(im1, ax=axes[i, 1], fraction=0.046, pad=0.04)\n\n            # Plot predicted density\n            im2 = axes[i, 2].imshow(pred_density_np, cmap='jet')\n            axes[i, 2].set_title(f'Pred Density (Count: {pred_count_val:.1f})')\n            axes[i, 2].axis('off')\n            plt.colorbar(im2, ax=axes[i, 2], fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    plt.show()\n\n# Visualize predictions if validation dataset exists\ntry:\n    print(\"üé® Visualizing sample predictions...\")\n    visualize_predictions(model, val_ds, device, num_samples=20)\nexcept NameError:\n    print(\"‚ö†Ô∏è  Validation dataset not available. Please run the data preparation cells first.\")\n\nprint(\"\\n‚úÖ Training and evaluation complete!\")\nprint(f\"üéØ Final model saved at: {config['save_path']}\")\nprint(\"üìä Training history and visualizations have been generated above.\")","metadata":{"id":"1437ac6b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Prediction and Generate Submission","metadata":{"id":"3dc00417"}},{"cell_type":"code","source":"# Create test dataset for inference\nclass TestDataset(Dataset):\n    def __init__(\n        self,\n        img_dir: str,\n        base_size: int = 768\n    ) -> None:\n        self.img_paths: List[str] = sorted(glob(os.path.join(img_dir, \"*.*\")))\n        self.base_size: int = base_size\n        self.to_tensor = transforms.ToTensor()\n        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n\n    def __len__(self) -> int:\n        return len(self.img_paths)\n\n    def __getitem__(self, index: int) -> Tuple[Tensor, str]:\n        img_path: str = self.img_paths[index]\n        img = imread_rgb(img_path)\n        canvas, _, _, _ = letterbox(img, target=self.base_size)\n\n        t: Tensor = self.to_tensor(canvas)\n        t = self.normalize(t)\n\n        image_name: str = os.path.basename(img_path)\n        return t, image_name\n\n\n# Setup test dataset and loader\ntest_ds = TestDataset(config['test_dir'], base_size=config['base_size'])\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=16,\n    shuffle=False,\n    num_workers=config['num_workers'],\n    pin_memory=True\n)\n\nprint(f\"üì¶ Test dataset: {len(test_ds)} samples\")\nprint(f\"üöÄ Test loader: {len(test_loader)} batches\")\n\n\ndef generate_submission(\n    model: torch.nn.Module,\n    test_loader: DataLoader,\n    output_file: str = \"submission.csv\"\n) -> pd.DataFrame:\n    model.eval()\n    predictions: List[int] = []\n    image_names: List[str] = []\n\n    with torch.no_grad():\n        for images, names in test_loader:\n            images = images.to(device)\n            pred: Tensor = model(images)\n            batch_preds: Tensor = pred.sum((1, 2, 3))\n\n            # Round the result so predicted_count will store as int\n            predictions.extend(batch_preds.cpu().numpy().round().astype(int).tolist())\n            image_names.extend(names)\n\n    submission_df: pd.DataFrame = pd.DataFrame({\n        \"image_id\": image_names,\n        \"predicted_count\": predictions\n    })\n\n    # Sort by image ID (assuming numeric names)\n    submission_df[\"sort_key\"] = submission_df[\"image_id\"].apply(\n        lambda x: int(os.path.splitext(x)[0])\n    )\n    submission_df = (\n        submission_df\n        .sort_values(\"sort_key\")\n        .drop(\"sort_key\", axis=1)\n        .reset_index(drop=True)\n    )\n\n    submission_df.to_csv(output_file, index=False)\n    print(f\"‚úÖ Submission saved to {output_file}\")\n    return submission_df\n\n\n# Generate submission\nsubmission_df = generate_submission(model, test_loader)\nprint(\"\\nüìã Sample submission:\")\nsubmission_df.head(50)","metadata":{"id":"79e11a9e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving Model","metadata":{"id":"9Zj4yPk9r9I1"}},{"cell_type":"code","source":"# Model Saving for Competition Submission\nprint(\"üíæ Saving trained model\")\n\n# Save complete model (architecture + weights)\nmodel_complete_path = os.path.join(config['save_dir'], 'cognivio_sfcn_model_complete.pth')\ntorch.save(model, model_complete_path)\nprint(f\"‚úÖ Complete model saved to: {model_complete_path}\")\n\n# Save model state dict only\nmodel_weights_path = os.path.join(config['save_dir'], 'cognivio_sfcn_model_weights.pth')\ntorch.save(model.state_dict(), model_weights_path)\nprint(f\"‚úÖ Model weights saved to: {model_weights_path}\")","metadata":{"id":"hkZ6ji1Fr-uA"},"outputs":[],"execution_count":null}]}