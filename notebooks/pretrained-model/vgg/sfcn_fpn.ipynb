{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarrelAD/Hology-8-2025-Data-Mining-PRIVATE/blob/dev%2Ffarrel/notebooks/vidi/SFCN/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c8ca3e",
      "metadata": {
        "id": "45c8ca3e"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarrelAD/Hology-8-2025-Data-Mining-PRIVATE/blob/dev%2Fvidi/notebooks/vidi/SFCN/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae00ccb0",
      "metadata": {
        "id": "ae00ccb0"
      },
      "source": [
        "# Crowd-counting with method SFCN + FPN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3638d25b",
      "metadata": {
        "id": "3638d25b",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "A new method that actually has better results than the other models weâ€™ve tried so far."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782f1a55",
      "metadata": {
        "id": "782f1a55"
      },
      "source": [
        "# 1. Project Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3736ee4",
      "metadata": {
        "id": "b3736ee4",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5562fbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5562fbc",
        "outputId": "990e2dc8-ba7e-4946-c6ea-db3cee31804b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from glob import glob\n",
        "from typing import List, Tuple, Any\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.spatial import KDTree\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Optimizer\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bb228f",
      "metadata": {
        "id": "93bb228f"
      },
      "source": [
        "## Dataset Download and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3364cead",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "3364cead",
        "outputId": "185cbcb6-3e94-433f-f732-f080951b4cd1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-136fdaa9-46e5-496f-a1bf-658f167dddc9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-136fdaa9-46e5-496f-a1bf-658f167dddc9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 64 bytes\n"
          ]
        }
      ],
      "source": [
        "# @title Setup Kaggle secret key\n",
        "!pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "94f15866",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94f15866",
        "outputId": "7261d333-a7ba-41fb-f2c6-bbba7ccd6038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset already exists, skipping download.\n",
            "Dataset already extracted at: /content/drive/MyDrive/PROJECTS/Cognivio/Percobaan Hology 8 2025/dataset\n",
            "Dataset already available in Colab local storage.\n"
          ]
        }
      ],
      "source": [
        "# @title Setup dataset in Colab\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "zip_path = \"/content/penyisihan-hology-8-0-2025-data-mining.zip\"\n",
        "drive_extract_path = \"/content/drive/MyDrive/PROJECTS/Cognivio/Percobaan Hology 8 2025/dataset\"\n",
        "local_dataset_path = \"/content/dataset\"  # for current session\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Download zip (if not exists in /content)\n",
        "# ---------------------------\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Dataset not found locally, downloading...\")\n",
        "    !kaggle competitions download -c penyisihan-hology-8-0-2025-data-mining -p /content\n",
        "else:\n",
        "    print(\"Dataset already exists, skipping download.\")\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Extract to Google Drive (for backup)\n",
        "# ---------------------------\n",
        "os.makedirs(drive_extract_path, exist_ok=True)\n",
        "\n",
        "if not os.listdir(drive_extract_path):  # Check if folder is empty\n",
        "    print(\"Extracting dataset to Google Drive...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(drive_extract_path)\n",
        "    print(\"Dataset extracted to:\", drive_extract_path)\n",
        "else:\n",
        "    print(\"Dataset already extracted at:\", drive_extract_path)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Copy dataset to local /content (faster training)\n",
        "# ---------------------------\n",
        "if not os.path.exists(local_dataset_path):\n",
        "    print(\"Copying dataset to Colab local storage (/content)...\")\n",
        "    !cp -r \"$drive_extract_path\" \"$local_dataset_path\"\n",
        "else:\n",
        "    print(\"Dataset already available in Colab local storage.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e0d7b5",
      "metadata": {
        "id": "94e0d7b5"
      },
      "source": [
        "## Some Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba7acc8",
      "metadata": {
        "id": "1ba7acc8"
      },
      "outputs": [],
      "source": [
        "# ImageNet mean and std for normalisation\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Dataset path\n",
        "TRAIN_IMG_DIR = os.path.join(local_dataset_path, \"train\", \"images\")\n",
        "TRAIN_LABEL_DIR = os.path.join(local_dataset_path, \"train\", \"labels\")\n",
        "TEST_IMG_DIR  = os.path.join(local_dataset_path, \"test\", \"images\")\n",
        "\n",
        "\n",
        "# Seed for better reproducibility\n",
        "SEED = 1337 # or 31\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "# PREPROCESSING CONFIGURATION\n",
        "CONTRAST_MODE = \"clahe\"\n",
        "CLAHE_CLIP = 2.0\n",
        "CLAHE_GRID = 8\n",
        "\n",
        "\n",
        "# TRAINING CONFIGURATION\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 120\n",
        "ACCUM_STEPS = 2\n",
        "EARLY_STOP_PATIENCE = 15\n",
        "CUDA_AMP = True         # CUDA Automatic Mixed Precision (AMP).\n",
        "\n",
        "BASE_SIZE = 896\n",
        "DOWN = 8\n",
        "PATCH_SIZE = 384\n",
        "PATCHES_PER_IMAGE = 6\n",
        "AVOID_EMPTY_PATCHES = True\n",
        "SIGMA_MODE = \"adaptive\"\n",
        "\n",
        "BATCH = 4\n",
        "NUM_WORKERS = 6\n",
        "\n",
        "CRITERION = \"huber\"\n",
        "COUNT_LESS_ALPHA = 0.2\n",
        "DET_LOSS_ALPHA = 1.0\n",
        "HYBRID_EVAL = True\n",
        "DENS_THRES = 0.25\n",
        "DET_PROB_THRES = 0.5\n",
        "\n",
        "SAVE_MODEL_NAME = \"sfcn_best.pth\"\n",
        "\n",
        "COMPARE_PRED = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da059d1",
      "metadata": {
        "id": "4da059d1"
      },
      "source": [
        "## Some Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b351e5",
      "metadata": {
        "id": "e7b351e5"
      },
      "outputs": [],
      "source": [
        "def derive_json_path(\n",
        "    label_dir: str,\n",
        "    img_path: str\n",
        ") -> str:\n",
        "    \"\"\"Derive the corresponding JSON label path for a given image path.\n",
        "\n",
        "    Tries to match the image basename to a JSON file in the label directory.\n",
        "    Falls back to matching digits if an exact name isn't found.\n",
        "    \"\"\"\n",
        "    name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    cand = os.path.join(label_dir, name + \".json\")\n",
        "    if os.path.exists(cand):\n",
        "        return cand\n",
        "    # Try matching trailing digits\n",
        "    m = re.findall(r\"\\d+\", name)\n",
        "    if m:\n",
        "        alt = os.path.join(label_dir, f\"{m[-1]}.json\")\n",
        "        if os.path.exists(alt):\n",
        "            return alt\n",
        "    # Fallback: any file starting with the same name\n",
        "    lst = glob(os.path.join(label_dir, f\"{name}*.json\"))\n",
        "    if lst:\n",
        "        return lst[0]\n",
        "    raise FileNotFoundError(f\"JSON label not found for {img_path}\")\n",
        "\n",
        "def parse_points_from_json(\n",
        "    path: str\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Parse annotated points from a JSON file.\n",
        "\n",
        "    Supports several common crowd counting annotation formats. Returns an\n",
        "    array of shape (N, 2) containing [x, y] coordinates and, if present\n",
        "    in the JSON, the declared number of people. If no `human_num` or\n",
        "    `num_human` field is present the count is returned as None.\n",
        "    \"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        obj = json.load(f)\n",
        "    \n",
        "    pts: List[List[float]]\n",
        "    num = None\n",
        "    \n",
        "    if isinstance(obj, dict) and \"points\" in obj:\n",
        "        pts = obj[\"points\"]\n",
        "        # unify possible keys for ground-truth count\n",
        "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
        "        # If points are dicts, extract x/y fields\n",
        "        if len(pts) > 0 and isinstance(pts[0], dict):\n",
        "            pts = [[p[\"x\"], p[\"y\"]] for p in pts if \"x\" in p and \"y\" in p]\n",
        "    elif isinstance(obj, dict) and \"annotations\" in obj:\n",
        "        pts = [[a[\"x\"], a[\"y\"]] for a in obj[\"annotations\"] if \"x\" in a and \"y\" in a]\n",
        "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
        "    elif isinstance(obj, list):\n",
        "        # direct list of [x,y]\n",
        "        pts = obj\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown JSON schema: {path}\")\n",
        "    \n",
        "    pts_arr = np.array(pts, dtype=np.float32) if len(pts) > 0 else np.zeros((0, 2), np.float32)\n",
        "    \n",
        "    return pts_arr, num\n",
        "\n",
        "def letterbox(\n",
        "    img: np.ndarray,\n",
        "    target: int = 512\n",
        ") -> Tuple[np.ndarray, float, int, int]:\n",
        "    \"\"\"Resize and pad an image to a square canvas without distortion.\n",
        "\n",
        "    Returns the padded image, the scale factor used, and the left/top\n",
        "    padding applied. The output size is (target, target).\n",
        "    \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    scale = min(target / h, target / w)\n",
        "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
        "    img_rs = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "    top = (target - nh) // 2\n",
        "    left = (target - nw) // 2\n",
        "    canvas = np.zeros((target, target, 3), dtype=img_rs.dtype)\n",
        "    canvas[top:top + nh, left:left + nw] = img_rs\n",
        "    return canvas, scale, left, top\n",
        "\n",
        "\n",
        "def make_density_map(\n",
        "    points_xy: np.ndarray,\n",
        "    grid_size: int,\n",
        "    down: int = 8,\n",
        "    sigma_mode: str = \"adaptive\",\n",
        "    knn: int = 3,\n",
        "    beta: float = 0.3,\n",
        "    const_sigma: float = 2.0,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Generate a density map on a grid given annotated points.\n",
        "\n",
        "    The density map is of shape (grid_size//down, grid_size//down). Each\n",
        "    point is represented by a Gaussian whose sigma is either constant or\n",
        "    computed from the k-nearest neighbours. The integral of the density map\n",
        "    approximates the number of points.\n",
        "    \"\"\"\n",
        "    target = grid_size\n",
        "    dh, dw = target // down, target // down\n",
        "    den = np.zeros((dh, dw), dtype=np.float32)\n",
        "    if len(points_xy) == 0:\n",
        "        return den\n",
        "    # Scale points to the density map resolution\n",
        "    pts = points_xy.copy()\n",
        "    pts[:, 0] = pts[:, 0] * (dw / target)\n",
        "    pts[:, 1] = pts[:, 1] * (dh / target)\n",
        "    tree = KDTree(pts) if len(pts) > 1 else None\n",
        "    for (x, y) in pts:\n",
        "        # Determine sigma\n",
        "        if sigma_mode == \"adaptive\" and tree is not None and len(pts) > 3:\n",
        "            dists, _ = tree.query([x, y], k=min(knn + 1, len(pts)))\n",
        "            sigma = max(1.0, float(np.mean(dists[1:])) * beta)\n",
        "        else:\n",
        "            sigma = const_sigma\n",
        "        cx, cy = float(x), float(y)\n",
        "        rad = int(max(1, math.ceil(3 * sigma)))\n",
        "        x0, x1 = max(0, int(math.floor(cx - rad))), min(dw, int(math.ceil(cx + rad + 1)))\n",
        "        y0, y1 = max(0, int(math.floor(cy - rad))), min(dh, int(math.ceil(cy + rad + 1)))\n",
        "        if x1 <= x0 or y1 <= y0:\n",
        "            continue\n",
        "        xs = np.arange(x0, x1) - cx\n",
        "        ys = np.arange(y0, y1) - cy\n",
        "        xx, yy = np.meshgrid(xs, ys)\n",
        "        g = np.exp(-(xx**2 + yy**2) / (2 * sigma * sigma))\n",
        "        s = g.sum()\n",
        "        if s > 0:\n",
        "            den[y0:y1, x0:x1] += (g / s).astype(np.float32)\n",
        "    return den\n",
        "\n",
        "def apply_contrast_enhancement(\n",
        "    img_rgb: np.ndarray, \n",
        "    mode: str = \"none\",\n",
        "    clahe_clip: float = 2.0, \n",
        "    clahe_grid: int = 8\n",
        ") -> np.ndarray:\n",
        "    if mode == \"none\": return img_rgb\n",
        "    if mode == \"clahe\":\n",
        "        lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n",
        "        l,a,b = cv2.split(lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=clahe_clip, tileGridSize=(clahe_grid,clahe_grid))\n",
        "        l2 = clahe.apply(l)\n",
        "        return cv2.cvtColor(cv2.merge([l2,a,b]), cv2.COLOR_LAB2RGB)\n",
        "    if mode == \"histeq\":\n",
        "        ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2YCrCb)\n",
        "        y,cr,cb = cv2.split(ycrcb)\n",
        "        y2 = cv2.equalizeHist(y)\n",
        "        return cv2.cvtColor(cv2.merge([y2,cr,cb]), cv2.COLOR_YCrCb2RGB)\n",
        "    return img_rgb\n",
        "\n",
        "\n",
        "def make_occupancy_map(\n",
        "    points_xy: np.ndarray,\n",
        "    grid_size: int,\n",
        "    down: int = 8,\n",
        "    radius: int = 1,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create an occupancy map marking regions around given points.\n",
        "\n",
        "    Args:\n",
        "        points_xy: Array of shape (N, 2) with x,y coordinates of points.\n",
        "        grid_size: Size of the square grid (image resolution).\n",
        "        down: Downsampling factor for the grid.\n",
        "        radius: Radius around each point to mark as occupied.\n",
        "\n",
        "    Returns:\n",
        "        A 2D occupancy map (numpy array) of shape (grid_size//down, grid_size//down).\n",
        "    \"\"\"\n",
        "    dh, dw = grid_size // down, grid_size // down\n",
        "    occupancy = np.zeros((dh, dw), dtype=np.float32)\n",
        "\n",
        "    if points_xy.size == 0:\n",
        "        return occupancy\n",
        "\n",
        "    # Normalize coordinates into grid coordinates\n",
        "    pts = points_xy.copy()\n",
        "    pts[:, 0] *= dw / grid_size\n",
        "    pts[:, 1] *= dh / grid_size\n",
        "\n",
        "    for x, y in pts:\n",
        "        xi, yi = int(round(x)), int(round(y))\n",
        "\n",
        "        x0, x1 = max(0, xi - radius), min(dw - 1, xi + radius)\n",
        "        y0, y1 = max(0, yi - radius), min(dh - 1, yi + radius)\n",
        "\n",
        "        occupancy[y0:y1 + 1, x0:x1 + 1] = 1.0\n",
        "\n",
        "    return occupancy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "404f3feb",
      "metadata": {
        "id": "404f3feb"
      },
      "source": [
        "# 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2adfcb54",
      "metadata": {
        "id": "2adfcb54"
      },
      "outputs": [],
      "source": [
        "# Build list of images and randomly shuffle before splitting\n",
        "all_imgs = sorted(glob(os.path.join(TRAIN_IMG_DIR, \"*.*\")))\n",
        "\n",
        "random.shuffle(all_imgs)\n",
        "n_images = len(all_imgs)\n",
        "\n",
        "if n_images < 2:\n",
        "    raise ValueError(\"Need at least 2 images for training and validation\")\n",
        "\n",
        "n_val = max(1, int(0.1 * n_images))\n",
        "n_train = n_images - n_val\n",
        "\n",
        "train_imgs = all_imgs[:n_train]\n",
        "val_imgs = all_imgs[n_train:]\n",
        "\n",
        "\n",
        "# TODO: put your any data preprocessing below it!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17100915",
      "metadata": {
        "id": "17100915"
      },
      "source": [
        "# 3. Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ca8430",
      "metadata": {
        "id": "57ca8430"
      },
      "outputs": [],
      "source": [
        "class CrowdDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_dir: str,\n",
        "        label_dir: str,\n",
        "        base_size: int = 768,\n",
        "        down: int = 8,\n",
        "        aug: bool = True,\n",
        "        mode: str = \"train\",\n",
        "        patch_size: int = 0,\n",
        "        patches_per_image: int = 1,\n",
        "        sigma_mode: str = \"adaptive\",\n",
        "        avoid_empty_patches: bool = False,\n",
        "        contrast_mode: str = \"none\",\n",
        "        clahe_clip: float = 2.0,\n",
        "        clahe_grid: int = 8,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_paths: List[str] = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
        "        if not self.img_paths:\n",
        "            raise ValueError(f\"No images found in {img_dir}\")\n",
        "\n",
        "        self.label_dir = label_dir\n",
        "        self.base_size = base_size\n",
        "        self.down = down\n",
        "        self.aug = aug\n",
        "        self.mode = mode\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_per_image = max(1, int(patches_per_image))\n",
        "        self.sigma_mode = sigma_mode\n",
        "        self.avoid_empty_patches = avoid_empty_patches\n",
        "        self.contrast_mode = contrast_mode\n",
        "        self.clahe_clip = clahe_clip\n",
        "        self.clahe_grid = clahe_grid\n",
        "\n",
        "        # Transforms\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "        self.color_jit = transforms.ColorJitter(0.1, 0.1, 0.1, 0.05)\n",
        "        self.extra_aug = transforms.Compose([\n",
        "            transforms.RandomApply([transforms.RandomGrayscale(p=1.0)], p=0.15),\n",
        "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.30),\n",
        "            transforms.RandomResizedCrop(size=base_size, scale=(0.80, 1.00), ratio=(0.90, 1.10)),\n",
        "        ])\n",
        "        self.rand_erase = transforms.RandomErasing(\n",
        "            p=0.25, scale=(0.02, 0.20), ratio=(0.3, 3.3), value=0\n",
        "        )\n",
        "\n",
        "        # Dataset length logic\n",
        "        is_training_with_patches = (\n",
        "            self.mode == \"train\"\n",
        "            and self.patch_size > 0\n",
        "            and self.patches_per_image > 1\n",
        "        )\n",
        "        self.effective_len = (\n",
        "            len(self.img_paths) * self.patches_per_image\n",
        "            if is_training_with_patches else len(self.img_paths)\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.effective_len\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple[Any, Tensor, Tensor, Tensor]:\n",
        "        idx_base = self._get_base_index(index)\n",
        "        img, points = self._load_img_and_points(idx_base)\n",
        "\n",
        "        if self.mode == \"train\" and self.patch_size > 0:\n",
        "            img, points, grid = self._sample_patch(img, points)\n",
        "        else:\n",
        "            grid = self.base_size\n",
        "\n",
        "        den = make_density_map(points, grid_size=grid, down=self.down, sigma_mode=self.sigma_mode)\n",
        "        occ = make_occupancy_map(points, grid_size=grid, down=self.down, radius=1)\n",
        "\n",
        "        tensor_img = self.to_tensor(img)\n",
        "        if self.mode == \"train\" and self.aug:\n",
        "            tensor_img = self.rand_erase(tensor_img)\n",
        "        tensor_img = self.normalize(tensor_img)\n",
        "\n",
        "        density = torch.from_numpy(den).unsqueeze(0)\n",
        "        occupancy = torch.from_numpy(occ).unsqueeze(0)\n",
        "        count = torch.tensor([float(len(points))], dtype=torch.float32)\n",
        "\n",
        "        return tensor_img, density, count, occupancy\n",
        "\n",
        "    def _get_base_index(self, index: int) -> int:\n",
        "        \"\"\"Convert effective dataset index into base image index.\"\"\"\n",
        "        if (\n",
        "            self.mode == \"train\"\n",
        "            and self.patch_size > 0\n",
        "            and self.patches_per_image > 1\n",
        "        ):\n",
        "            index //= self.patches_per_image\n",
        "        return index % len(self.img_paths)\n",
        "\n",
        "    def _load_img_and_points(\n",
        "        self, \n",
        "        idx: int\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Load an image and its corresponding points.\"\"\"\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        img = apply_contrast_enhancement(\n",
        "            img, self.contrast_mode, self.clahe_clip, self.clahe_grid\n",
        "        )\n",
        "\n",
        "        lbl_path = derive_json_path(self.label_dir, img_path)\n",
        "        points, _ = parse_points_from_json(lbl_path)\n",
        "\n",
        "        # Augmentations\n",
        "        if self.mode == \"train\" and self.aug:\n",
        "            img, points = self._apply_augmentations(img, points, w)\n",
        "\n",
        "        # Letterbox resize\n",
        "        canvas, scale, left, top = letterbox(img, target=self.base_size)\n",
        "        points = self._transform_points(points, scale, left, top)\n",
        "\n",
        "        return canvas, points\n",
        "\n",
        "    def _apply_augmentations(\n",
        "        self, img: np.ndarray, points: np.ndarray, width: int\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Apply random augmentations to image and points.\"\"\"\n",
        "        if random.random() < 0.5:  # Horizontal flip\n",
        "            img = img[:, ::-1, :].copy()\n",
        "            if points.size > 0:\n",
        "                points = points.copy()\n",
        "                points[:, 0] = (width - 1) - points[:, 0]\n",
        "\n",
        "        if random.random() < 0.5:  # Color jitter\n",
        "            pil_img = transforms.ToPILImage()(img)\n",
        "            img = np.array(self.color_jit(pil_img))\n",
        "\n",
        "        # Extra augmentations\n",
        "        pil_img = transforms.ToPILImage()(img)\n",
        "        img = np.array(self.extra_aug(pil_img))\n",
        "\n",
        "        return img, points\n",
        "\n",
        "    def _transform_points(\n",
        "        self, points: np.ndarray, scale: float, left: int, top: int\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Scale and shift points after letterboxing.\"\"\"\n",
        "        if points.size == 0:\n",
        "            return np.zeros((0, 2), np.float32)\n",
        "\n",
        "        pts = points.copy()\n",
        "        pts[:, 0] = pts[:, 0] * scale + left\n",
        "        pts[:, 1] = pts[:, 1] * scale + top\n",
        "\n",
        "        mask = (\n",
        "            (0 <= pts[:, 0]) & (pts[:, 0] < self.base_size) &\n",
        "            (0 <= pts[:, 1]) & (pts[:, 1] < self.base_size)\n",
        "        )\n",
        "        return pts[mask]\n",
        "\n",
        "    def _sample_patch(\n",
        "        self, img: np.ndarray, points: np.ndarray\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, int]:\n",
        "        \"\"\"Sample a random patch from image and points.\"\"\"\n",
        "        ps = self.patch_size\n",
        "        max_off = self.base_size - ps\n",
        "\n",
        "        attempts = range(10) if self.avoid_empty_patches else [0]\n",
        "        for attempt in attempts:\n",
        "            ox = random.randint(0, max_off) if max_off > 0 else 0\n",
        "            oy = random.randint(0, max_off) if max_off > 0 else 0\n",
        "\n",
        "            crop = img[oy:oy + ps, ox:ox + ps, :]\n",
        "            cropped_points = self._crop_points(points, ox, oy, ps)\n",
        "\n",
        "            if not self.avoid_empty_patches or cropped_points.size > 0 or attempt == 9:\n",
        "                return crop, cropped_points, ps\n",
        "\n",
        "        return img, points, ps  # Fallback\n",
        "\n",
        "    def _crop_points(\n",
        "        self, points: np.ndarray, ox: int, oy: int, patch_size: int\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Shift and filter points inside a patch.\"\"\"\n",
        "        if points.size == 0:\n",
        "            return np.zeros((0, 2), np.float32)\n",
        "\n",
        "        pts = points.copy()\n",
        "        pts[:, 0] -= ox\n",
        "        pts[:, 1] -= oy\n",
        "\n",
        "        mask = (\n",
        "            (0 <= pts[:, 0]) & (pts[:, 0] < patch_size) &\n",
        "            (0 <= pts[:, 1]) & (pts[:, 1] < patch_size)\n",
        "        )\n",
        "        return pts[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b23e5352",
      "metadata": {
        "id": "b23e5352"
      },
      "outputs": [],
      "source": [
        "# Instantiate datasets\n",
        "train_ds = CrowdDataset(\n",
        "    img_dir=TRAIN_IMG_DIR,\n",
        "    label_dir=TRAIN_LABEL_DIR,\n",
        "    base_size=BASE_SIZE,\n",
        "    down=DOWN,\n",
        "    aug=True,\n",
        "    mode=\"train\",\n",
        "    patch_size=PATCH_SIZE,\n",
        "    patches_per_image=PATCHES_PER_IMAGE,\n",
        "    sigma_mode=SIGMA_MODE,\n",
        "    avoid_empty_patches=AVOID_EMPTY_PATCHES,\n",
        ")\n",
        "val_ds = CrowdDataset(\n",
        "    img_dir=TRAIN_IMG_DIR,\n",
        "    label_dir=TRAIN_LABEL_DIR,\n",
        "    base_size=BASE_SIZE,\n",
        "    down=DOWN,\n",
        "    aug=False,\n",
        "    mode=\"val\",\n",
        "    patch_size=0,\n",
        "    patches_per_image=1,\n",
        "    sigma_mode=SIGMA_MODE,\n",
        "    avoid_empty_patches=False,\n",
        ")\n",
        "\n",
        "\n",
        "# Override image paths after shuffling\n",
        "train_ds.img_paths = train_imgs\n",
        "val_ds.img_paths = val_imgs\n",
        "\n",
        "# Recompute effective lengths for patch training\n",
        "if train_ds.mode == \"train\" and train_ds.patch_size > 0 and train_ds.patches_per_image > 1:\n",
        "    train_ds.effective_len = len(train_ds.img_paths) * train_ds.patches_per_image\n",
        "else:\n",
        "    train_ds.effective_len = len(train_ds.img_paths)\n",
        "val_ds.effective_len = len(val_ds.img_paths)\n",
        "\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size= BATCH,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77a8a0f9",
      "metadata": {
        "id": "77a8a0f9"
      },
      "source": [
        "# 4. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6419de",
      "metadata": {
        "id": "6e6419de"
      },
      "outputs": [],
      "source": [
        "class SpatialEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Depthwise spatial encoder using separate horizontal and vertical convolutions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int, k: int = 9) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        padding = k // 2\n",
        "\n",
        "        # Depthwise horizontal convolutions\n",
        "        self.h1: nn.Conv2d = nn.Conv2d(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=(1, k),\n",
        "            padding=(0, padding),\n",
        "            groups=channels,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.h2: nn.Conv2d = nn.Conv2d(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=(1, k),\n",
        "            padding=(0, padding),\n",
        "            groups=channels,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        # Depthwise vertical convolutions\n",
        "        self.v1: nn.Conv2d = nn.Conv2d(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=(k, 1),\n",
        "            padding=(padding, 0),\n",
        "            groups=channels,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.v2: nn.Conv2d = nn.Conv2d(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=(k, 1),\n",
        "            padding=(padding, 0),\n",
        "            groups=channels,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        # Projection to original channel size\n",
        "        self.proj: nn.Conv2d = nn.Conv2d(\n",
        "            in_channels=channels * 4,\n",
        "            out_channels=channels,\n",
        "            kernel_size=1,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        # Activation\n",
        "        self.act: nn.ReLU = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        h1_out: Tensor = self.h1(x)\n",
        "        h2_out: Tensor = self.h2(x)\n",
        "        v1_out: Tensor = self.v1(x)\n",
        "        v2_out: Tensor = self.v2(x)\n",
        "\n",
        "        cat_out: Tensor = torch.cat([h1_out, h2_out, v1_out, v2_out], dim=1)\n",
        "        proj_out: Tensor = self.proj(cat_out)\n",
        "        act_out: Tensor = self.act(proj_out)\n",
        "\n",
        "        return act_out\n",
        "\n",
        "\n",
        "class SFCN_VGG_FPN(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG16-BN backbone with FPN (C2/C3/C4 -> P2/P3/P4), stride-8 head.\n",
        "    Outputs (density_map, detection_logits).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        pretrained: bool = True, \n",
        "        use_spatial_encoder: bool = True\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # VGG16-BN backbone\n",
        "        weights = models.VGG16_BN_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        vgg = models.vgg16_bn(weights=weights)\n",
        "        self.features = vgg.features  # length 43\n",
        "\n",
        "        self.use_spatial = use_spatial_encoder\n",
        "        self.senc = SpatialEncoder(256, k=9) if self.use_spatial else nn.Identity()\n",
        "\n",
        "        # FPN lateral 1x1 convolutions\n",
        "        self.lat_c2 = nn.Conv2d(128, 256, kernel_size=1)\n",
        "        self.lat_c3 = nn.Conv2d(256, 256, kernel_size=1)\n",
        "        self.lat_c4 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "\n",
        "        # FPN smooth 3x3 convolutions\n",
        "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.smooth4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        # Density head\n",
        "        self.density_head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        # Detection head (logits)\n",
        "        self.detect_head = nn.Conv2d(256, 1, kernel_size=1)\n",
        "\n",
        "    def _forward_backbone(\n",
        "        self, \n",
        "        x: Tensor\n",
        "    ) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        \"\"\"Forward pass through VGG16 backbone, returning C2, C3, C4 features.\"\"\"\n",
        "        c2 = c3 = c4 = None\n",
        "        pool_count = 0\n",
        "\n",
        "        for layer in self.features:\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, nn.MaxPool2d):\n",
        "                pool_count += 1\n",
        "                if pool_count == 2:\n",
        "                    c2 = x\n",
        "                elif pool_count == 3:\n",
        "                    c3 = x\n",
        "                elif pool_count == 4:\n",
        "                    c4 = x\n",
        "\n",
        "        return c2, c3, c4 # type: ignore\n",
        "\n",
        "    def _upsample_add(\n",
        "        self, \n",
        "        x: Tensor, \n",
        "        y: Tensor\n",
        "    ) -> Tensor:\n",
        "        \"\"\"Upsample x to y's size and add.\"\"\"\n",
        "        return F.interpolate(x, size=y.shape[-2:], mode=\"nearest\") + y\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        x: Tensor\n",
        "    ) -> tuple[Tensor, Tensor]:\n",
        "        c2, c3, c4 = self._forward_backbone(x)\n",
        "\n",
        "        # FPN lateral + smooth\n",
        "        p4 = self.smooth4(self.lat_c4(c4))\n",
        "        p3 = self.smooth3(self._upsample_add(p4, self.lat_c3(c3)))\n",
        "        p2 = self.smooth2(self._upsample_add(p3, self.lat_c2(c2)))\n",
        "\n",
        "        # Use stride-8 features\n",
        "        f = self.senc(p3)\n",
        "\n",
        "        # Heads\n",
        "        dens = F.softplus(self.density_head(f))\n",
        "        det = self.detect_head(f)\n",
        "\n",
        "        return dens, det\n",
        "\n",
        "\n",
        "# Model, optimizer, scaler, and scheduler\n",
        "model = SFCN_VGG_FPN(pretrained=True).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "scaler = torch.cuda.amp.GradScaler() if (CUDA_AMP and device.type == \"cuda\") else None\n",
        "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a47450fd",
      "metadata": {},
      "source": [
        "# 5. Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c9fd8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model: torch.nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    dens_thres: float = 0.25,\n",
        "    det_prob_thres: float = 0.5,\n",
        "    hybrid: bool = True,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate model on a dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model.\n",
        "        loader: DataLoader for validation dataset.\n",
        "        device: Device to run inference on.\n",
        "        dens_thresh: Density threshold for hybrid counting.\n",
        "        det_prob_thr: Detection probability threshold (currently unused in code).\n",
        "        hybrid: If True, combine density and detection for counting.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (MAE, RMSE) over the dataset.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    mae = 0.0\n",
        "    mse = 0.0\n",
        "    nimg = 0\n",
        "\n",
        "    for imgs, dens, _, occ in tqdm(loader, desc=\"Val\", leave=False):\n",
        "        imgs = imgs.to(device)\n",
        "        dens = dens.to(device)\n",
        "        occ = occ.to(device)\n",
        "\n",
        "        pred_den, pred_det = model(imgs)\n",
        "\n",
        "        if hybrid:\n",
        "            prob = torch.sigmoid(pred_det)\n",
        "            w = (pred_den < dens_thres).float() * prob\n",
        "            combined = (1.0 - w) * pred_den + w * prob\n",
        "            pred_count = combined.sum((1, 2, 3))\n",
        "        else:\n",
        "            pred_count = pred_den.sum((1, 2, 3))\n",
        "\n",
        "        diff = (pred_count - dens.sum((1, 2, 3))).detach().cpu().numpy()\n",
        "        mae += np.abs(diff).sum()\n",
        "        mse += (diff**2).sum()\n",
        "        nimg += imgs.size(0)\n",
        "\n",
        "    return mae / max(1, nimg), math.sqrt(mse / max(1, nimg))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d30790dc",
      "metadata": {
        "id": "d30790dc"
      },
      "source": [
        "# 6. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01069a7a",
      "metadata": {
        "id": "01069a7a"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    optimizer: Optimizer,\n",
        "    scaler: GradScaler | None = None,\n",
        "    accum_steps: int = 1,\n",
        "    criterion: str = \"mse\",\n",
        "    count_loss_alpha: float = 0.0,\n",
        "    det_loss_alpha: float = 1.0,\n",
        ") -> float:\n",
        "    \"\"\"Train model for one epoch and return MAE over the dataset.\"\"\"\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    if criterion == \"mse\":\n",
        "        crit = nn.MSELoss()\n",
        "    elif criterion == \"huber\":\n",
        "        crit = nn.SmoothL1Loss()\n",
        "    else:\n",
        "        raise ValueError(\"criterion must be 'mse' or 'huber'\")\n",
        "\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    running_mae = 0.0\n",
        "    nimg = 0\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, (imgs, dens, _, occ) in enumerate(tqdm(loader, desc=\"Train\", leave=False), 1):\n",
        "        imgs = imgs.to(device)\n",
        "        dens = dens.to(device)\n",
        "        occ = occ.to(device)\n",
        "\n",
        "        # ---------------- Forward + Loss ---------------- #\n",
        "        if scaler is not None:\n",
        "            autocast_ctx = torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
        "        else:\n",
        "            autocast_ctx = torch.no_grad()  # dummy context\n",
        "\n",
        "        with autocast_ctx:\n",
        "            pred_den, pred_det = model(imgs)\n",
        "            map_loss = crit(pred_den, dens)\n",
        "            det_loss = bce(pred_det, occ)\n",
        "            total_loss = map_loss + det_loss_alpha * det_loss\n",
        "\n",
        "            if count_loss_alpha > 0.0:\n",
        "                count_loss = F.mse_loss(pred_den.sum((1, 2, 3)), dens.sum((1, 2, 3)))\n",
        "                total_loss = total_loss + count_loss_alpha * count_loss\n",
        "\n",
        "            total_loss = total_loss / accum_steps\n",
        "\n",
        "        # ---------------- Backward ---------------- #\n",
        "        if scaler is not None:\n",
        "            scaler.scale(total_loss).backward()\n",
        "            if step % accum_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "        else:\n",
        "            total_loss.backward()\n",
        "            if step % accum_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # ---------------- Metric ---------------- #\n",
        "        with torch.no_grad():\n",
        "            pred_count = pred_den.sum((1, 2, 3)).detach().cpu().numpy()\n",
        "            gt_count = dens.sum((1, 2, 3)).detach().cpu().numpy()\n",
        "            running_mae += np.abs(pred_count - gt_count).sum()\n",
        "            nimg += imgs.size(0)\n",
        "\n",
        "    return running_mae / max(1, nimg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a28040",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87a28040",
        "outputId": "b3bd51cb-1c42-4a55-ecd6-d672679fd30a"
      },
      "outputs": [],
      "source": [
        "best_mae = float(\"inf\")\n",
        "patience = EARLY_STOP_PATIENCE\n",
        "bad_epochs = 0\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "    tr_mae = train_epoch(\n",
        "        model,\n",
        "        loader=train_loader,\n",
        "        device=device,\n",
        "        optimizer=opt,\n",
        "        scaler=scaler,\n",
        "        accum_steps=ACCUM_STEPS,\n",
        "        criterion=CRITERION,\n",
        "        count_loss_alpha=COUNT_LESS_ALPHA,\n",
        "        det_loss_alpha=DET_LOSS_ALPHA\n",
        "    )\n",
        "    va_mae, va_rmse = evaluate(\n",
        "        model, \n",
        "        loader=val_loader, \n",
        "        device=device,\n",
        "        dens_thres=DENS_THRES,\n",
        "        det_prob_thres=DET_PROB_THRES,\n",
        "        hybrid=HYBRID_EVAL\n",
        "    )\n",
        "    sched.step()\n",
        "    print(\n",
        "        f\"Train MAE: {tr_mae:.3f} | Val MAE: {va_mae:.3f} | Val RMSE: {va_rmse:.3f}\"\n",
        "    )\n",
        "    \n",
        "    if COMPARE_PRED:\n",
        "        print(f\"Perbandingan ground truth vs prediksi pada validation set untuk epoch {epoch}:\")\n",
        "        # Evaluate on validation set with current model\n",
        "        model.eval()\n",
        "        idx = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, dens, _, occ in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                dens = dens.to(device)\n",
        "                pred_den, pred_det = model(imgs)\n",
        "                # Compute predicted counts using hybrid gating if enabled\n",
        "                if args.hybrid_eval:\n",
        "                    prob = torch.sigmoid(pred_det)\n",
        "                    gate_on = (prob >= args.det_prob_thr).float()\n",
        "                    w = (pred_den < args.dens_thresh).float() * gate_on\n",
        "                    comb = (1.0 - w) * pred_den + w * prob\n",
        "                    pred_cnt_batch = comb.sum((1, 2, 3))\n",
        "                else:\n",
        "                    pred_cnt_batch = pred_den.sum((1, 2, 3))\n",
        "                gt_cnt_batch = dens.sum((1, 2, 3))\n",
        "                # Iterate over each sample in the batch\n",
        "                for j in range(pred_cnt_batch.size(0)):\n",
        "                    # Safely retrieve filename; fallback to index if out of range\n",
        "                    if idx < len(val_ds.img_paths):\n",
        "                        image_name = os.path.basename(val_ds.img_paths[idx])\n",
        "                    else:\n",
        "                        image_name = f\"sample_{idx}\"\n",
        "                    print(f\"{image_name}: GT {gt_cnt_batch[j].item():.2f}, Pred {pred_cnt_batch[j].item():.2f}\")\n",
        "                    idx += 1\n",
        "    \n",
        "    \n",
        "    if va_mae + 1e-6 < best_mae:\n",
        "        best_mae = va_mae\n",
        "        bad_epochs = 0\n",
        "\n",
        "        # Save the model to .pth file if necessary\n",
        "        # torch.save(\n",
        "        #     {\n",
        "        #         \"model\": model.state_dict(),\n",
        "        #         \"epoch\": epoch,\n",
        "        #         \"val_mae\": va_mae,\n",
        "        #         \"\": vars(),\n",
        "        #     },\n",
        "        #     SAVE_MODEL_NAME,\n",
        "        # )\n",
        "        # print(f\"âœ… New best: {SAVE_MODEL_NAME} (Val MAE={va_mae:.3f})\")\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        print(f\"No improvement for {bad_epochs} epoch(s).\")\n",
        "        if bad_epochs >= patience:\n",
        "            print(\n",
        "                f\"Early stopping at epoch {epoch}. Best Val MAE: {best_mae:.3f}\"\n",
        "            )\n",
        "            break\n",
        "print(\"Training finished. Best Val MAE:\", best_mae)\n",
        "\n",
        "if COMPARE_PRED:\n",
        "    print(\"Perbandingan ground truth vs prediksi pada validation set:\")\n",
        "    # Load the best model from checkpoint if it exists to ensure evaluation uses the best model\n",
        "    if os.path.exists(SAVE_MODEL_NAME):\n",
        "        ckpt = torch.load(SAVE_MODEL_NAME, map_location=device)\n",
        "        # Some checkpoints save under 'model' key, others are raw state_dict\n",
        "        if isinstance(ckpt, dict) and \"model\" in ckpt:\n",
        "            model.load_state_dict(ckpt[\"model\"])\n",
        "        else:\n",
        "            model.load_state_dict(ckpt)\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Index to map validation samples to filenames\n",
        "    idx = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, dens, _, occ in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            dens = dens.to(device)\n",
        "            # Forward pass\n",
        "            pred_den, pred_det = model(imgs)\n",
        "            # Compute counts using hybrid gating if requested\n",
        "            if HYBRID_EVAL:\n",
        "                prob = torch.sigmoid(pred_det)\n",
        "                gate_on = (prob >= DET_PROB_THRES).float()\n",
        "                w = (pred_den < args.dens_thresh).float() * gate_on\n",
        "                comb = (1.0 - w) * pred_den + w * prob\n",
        "                pred_cnt_batch = comb.sum((1, 2, 3))\n",
        "            else:\n",
        "                pred_cnt_batch = pred_den.sum((1, 2, 3))\n",
        "            gt_cnt_batch = dens.sum((1, 2, 3))\n",
        "            # Iterate over each sample in the batch\n",
        "            for j in range(pred_cnt_batch.size(0)):\n",
        "                # Safely retrieve filename; fallback to index if out of range\n",
        "                if idx < len(val_ds.img_paths):\n",
        "                    image_name = os.path.basename(val_ds.img_paths[idx])\n",
        "                else:\n",
        "                    image_name = f\"sample_{idx}\"\n",
        "                print(f\"{image_name}: GT {gt_cnt_batch[j].item():.2f}, Pred {pred_cnt_batch[j].item():.2f}\")\n",
        "                idx += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4b32ff",
      "metadata": {
        "id": "2f4b32ff"
      },
      "source": [
        "# 7. Test Set Prediction and Submit Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d944b1b",
      "metadata": {
        "id": "1d944b1b"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset):\n",
        "    \"\"\"Dataset for test images.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_dir: str,\n",
        "        transform=None\n",
        "    ):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted([\n",
        "            f for f in os.listdir(image_dir) if f.endswith('.jpg')],\n",
        "            key=lambda x: int(os.path.splitext(x)[0]\n",
        "        ))\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        return image, img_name\n",
        "\n",
        "# --- Prediction Function ---\n",
        "def generate_sfcn_predictions(\n",
        "    model,\n",
        "    test_dir: str,\n",
        "    output_file: str = \"sfcn_submission.csv\",\n",
        "    batch_size: int = 16,\n",
        "    transform=None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Generate test predictions using the SFCN model.\"\"\"\n",
        "\n",
        "    print(\"Generating test predictions with SFCN model...\")\n",
        "\n",
        "    # Dataset + DataLoader\n",
        "    test_dataset = TestDataset(test_dir, transform)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    image_names = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, names in test_loader:\n",
        "            images = images.to(device)\n",
        "            pred_density, pred_count = model(images)\n",
        "\n",
        "            # Use count predictions for submission\n",
        "            batch_preds = pred_count.cpu().numpy()\n",
        "            predictions.extend([max(0, int(round(pred))) for pred in batch_preds])\n",
        "            image_names.extend(names)\n",
        "\n",
        "    # Create submission DataFrame\n",
        "    submission_df = pd.DataFrame({\n",
        "        'image_id': image_names,\n",
        "        'predicted_count': predictions\n",
        "    })\n",
        "\n",
        "    # Sort by image_id\n",
        "    submission_df['sort_key'] = submission_df['image_id'].apply(lambda x: int(os.path.splitext(x)[0]))\n",
        "    submission_df = submission_df.sort_values('sort_key').drop('sort_key', axis=1).reset_index(drop=True)\n",
        "\n",
        "    # Save submission\n",
        "    submission_df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Submission saved to {output_file}\")\n",
        "    print(f\"Predictions for {len(submission_df)} test images\")\n",
        "\n",
        "    # Statistics\n",
        "    pred_counts = submission_df['predicted_count'].values\n",
        "    print(f\"\\nTest Predictions Statistics:\")\n",
        "    print(f\"Min: {pred_counts.min()}\")\n",
        "    print(f\"Max: {pred_counts.max()}\")\n",
        "    print(f\"Mean: {pred_counts.mean():.2f}\")\n",
        "    print(f\"Median: {np.median(pred_counts):.2f}\")\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# Run predictions\n",
        "submission_df = generate_sfcn_predictions(\n",
        "    model,\n",
        "    test_dir=TEST_IMG_DIR,\n",
        "    batch_size=16,\n",
        "    # transform=test_transforms\n",
        ")\n",
        "submission_df.head(50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
