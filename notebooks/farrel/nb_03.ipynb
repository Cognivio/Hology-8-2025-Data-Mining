{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c25961",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/FarrelAD/Hology-8-2025-Data-Mining-PRIVATE/blob/dev%2Ffarrel/notebooks/farrel/nb_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b039c6",
   "metadata": {},
   "source": [
    "# Regression-Based Crowd Counting Model\n",
    "\n",
    "This notebook implements a **direct regression approach** for crowd counting, which is fundamentally different from the density map approach. Instead of generating pixel-wise density maps, we'll:\n",
    "\n",
    "1. **Extract global features** from images using pretrained CNN backbones\n",
    "2. **Directly predict crowd counts** using regression layers\n",
    "3. **Use simpler loss functions** (MSE/MAE) on scalar count values\n",
    "4. **Leverage transfer learning** from ImageNet pretrained models\n",
    "\n",
    "## Key Advantages of Regression Approach:\n",
    "- **Faster training** - no need to generate density maps\n",
    "- **Lower memory usage** - direct scalar prediction\n",
    "- **Transfer learning** - can use powerful pretrained backbones\n",
    "- **Interpretable outputs** - direct count prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01385b63",
   "metadata": {},
   "source": [
    "# 1. Project Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f7fa5",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Dict, Any, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b7cf6",
   "metadata": {},
   "source": [
    "## Dataset Download and Setup\n",
    "Keep the same dataset download code as reference notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64744dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Kaggle secret key\n",
    "!pip install -q kaggle\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ae9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup dataset in Colab\n",
    "import zipfile\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "zip_path = \"/content/penyisihan-hology-8-0-2025-data-mining.zip\"\n",
    "drive_extract_path = \"/content/drive/MyDrive/PROJECTS/Cognivio/Percobaan Hology 8 2025/dataset\"\n",
    "local_dataset_path = \"/content/dataset\"  # for current session\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Download zip (if not exists in /content)\n",
    "# ---------------------------\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Dataset not found locally, downloading...\")\n",
    "    !kaggle competitions download -c penyisihan-hology-8-0-2025-data-mining -p /content\n",
    "else:\n",
    "    print(\"Dataset already exists, skipping download.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Extract to Google Drive (for backup)\n",
    "# ---------------------------\n",
    "os.makedirs(drive_extract_path, exist_ok=True)\n",
    "\n",
    "if not os.listdir(drive_extract_path):  # Check if folder is empty\n",
    "    print(\"Extracting dataset to Google Drive...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(drive_extract_path)\n",
    "    print(\"Dataset extracted to:\", drive_extract_path)\n",
    "else:\n",
    "    print(\"Dataset already extracted at:\", drive_extract_path)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Copy dataset to local /content (faster training)\n",
    "# ---------------------------\n",
    "if not os.path.exists(local_dataset_path):\n",
    "    print(\"Copying dataset to Colab local storage (/content)...\")\n",
    "    !cp -r \"$drive_extract_path\" \"$local_dataset_path\"\n",
    "else:\n",
    "    print(\"Dataset already available in Colab local storage.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Define dataset paths for training\n",
    "# ---------------------------\n",
    "TRAIN_IMG_DIR = os.path.join(local_dataset_path, \"train\", \"images\")\n",
    "TRAIN_LBL_DIR = os.path.join(local_dataset_path, \"train\", \"labels\")\n",
    "TEST_IMG_DIR  = os.path.join(local_dataset_path, \"test\", \"images\")\n",
    "\n",
    "print(\"Train images:\", TRAIN_IMG_DIR)\n",
    "print(\"Train labels:\", TRAIN_LBL_DIR)\n",
    "print(\"Test images:\", TEST_IMG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5d8f2",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing and Augmentation\n",
    "\n",
    "For regression-based crowd counting, we focus on global image features rather than spatial density information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90797d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crowd_counts(label_dir: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Load crowd counts from JSON label files.\n",
    "    Returns a dictionary mapping image names to their crowd counts.\n",
    "    \"\"\"\n",
    "    crowd_counts = {}\n",
    "    label_files = [f for f in os.listdir(label_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        img_name = label_file.replace('.json', '.jpg')\n",
    "        label_path = os.path.join(label_dir, label_file)\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            crowd_counts[img_name] = data.get('human_num', 0)\n",
    "    \n",
    "    return crowd_counts\n",
    "\n",
    "def analyze_dataset_statistics(crowd_counts: Dict[str, int]) -> None:\n",
    "    \"\"\"Analyze and visualize dataset statistics.\"\"\"\n",
    "    counts = list(crowd_counts.values())\n",
    "    \n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(f\"Total images: {len(counts)}\")\n",
    "    print(f\"Min count: {min(counts)}\")\n",
    "    print(f\"Max count: {max(counts)}\")\n",
    "    print(f\"Mean count: {np.mean(counts):.2f}\")\n",
    "    print(f\"Median count: {np.median(counts):.2f}\")\n",
    "    print(f\"Std deviation: {np.std(counts):.2f}\")\n",
    "    \n",
    "    # Visualize distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(counts, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Crowd Count')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.title('Distribution of Crowd Counts')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(counts)\n",
    "    plt.ylabel('Crowd Count')\n",
    "    plt.title('Crowd Count Box Plot')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load and analyze crowd counts\n",
    "crowd_counts = load_crowd_counts(TRAIN_LBL_DIR)\n",
    "analyze_dataset_statistics(crowd_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation strategies\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Data preprocessing and augmentation transforms defined.\")\n",
    "print(\"Using ImageNet normalization for transfer learning compatibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a3ad1",
   "metadata": {},
   "source": [
    "# 3. Feature-based Regression Model Architecture\n",
    "\n",
    "We'll implement multiple architecture options using different pretrained backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdCountingRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Regression-based crowd counting model using pretrained CNN backbones.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        backbone_name: str = 'resnet50', \n",
    "        pretrained: bool = True, \n",
    "        dropout_rate: float = 0.5\n",
    "    ):\n",
    "        super(CrowdCountingRegressor, self).__init__()\n",
    "        \n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        # Load pretrained backbone\n",
    "        if backbone_name == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            feature_dim = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()  # Remove final classification layer\n",
    "            \n",
    "        elif backbone_name == 'efficientnet_b0':\n",
    "            self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "            feature_dim = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            \n",
    "        elif backbone_name == 'vgg16':\n",
    "            self.backbone = models.vgg16(pretrained=pretrained)\n",
    "            feature_dim = self.backbone.classifier[6].in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            \n",
    "        elif backbone_name == 'mobilenet_v2':\n",
    "            self.backbone = models.mobilenet_v2(pretrained=pretrained)\n",
    "            feature_dim = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "        \n",
    "        # Regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Global Average Pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1),  # Single output for count\n",
    "            nn.ReLU()  # Ensure non-negative predictions\n",
    "        )\n",
    "        \n",
    "        # Initialize regression head weights\n",
    "        self._initialize_regression_head()\n",
    "    \n",
    "    def _initialize_regression_head(self):\n",
    "        \"\"\"Initialize regression head weights.\"\"\"\n",
    "        for m in self.regression_head.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Handle different backbone output formats\n",
    "        if len(features.shape) == 4:  # For backbones that output 4D tensors\n",
    "            features = self.regression_head(features)\n",
    "        else:  # For backbones that already apply global pooling\n",
    "            features = features.view(features.size(0), -1)\n",
    "            features = self.regression_head[2:](features)  # Skip pooling and flatten\n",
    "            \n",
    "        return features.squeeze()\n",
    "\n",
    "def create_model(\n",
    "    backbone_name: str = 'resnet50', \n",
    "    pretrained: bool = True\n",
    ") -> CrowdCountingRegressor:\n",
    "    \"\"\"Factory function to create crowd counting models.\"\"\"\n",
    "    model = CrowdCountingRegressor(backbone_name=backbone_name, pretrained=pretrained)\n",
    "    return model\n",
    "\n",
    "# Test different model architectures\n",
    "available_backbones = ['resnet50', 'efficientnet_b0', 'vgg16', 'mobilenet_v2']\n",
    "\n",
    "for backbone in available_backbones:\n",
    "    try:\n",
    "        model = create_model(backbone_name=backbone)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"{backbone}:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {backbone}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d4288",
   "metadata": {},
   "source": [
    "# 4. Dataset Class for Regression Approach\n",
    "\n",
    "Custom PyTorch Dataset class optimized for regression-based crowd counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ad785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdCountingRegressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for regression-based crowd counting.\n",
    "    Returns (image, count) pairs for direct count prediction.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        crowd_counts: Dict[str, int],\n",
    "        image_files: List[str],\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "        return_meta: bool = False\n",
    "    ):\n",
    "        self.image_dir = image_dir\n",
    "        self.crowd_counts = crowd_counts\n",
    "        self.image_files = image_files\n",
    "        self.transform = transform\n",
    "        self.return_meta = return_meta\n",
    "        \n",
    "        # Filter files that have corresponding labels\n",
    "        self.valid_files = [f for f in image_files if f in crowd_counts]\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.valid_files)} images\")\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_files)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        img_name = self.valid_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get crowd count\n",
    "        count = float(self.crowd_counts[img_name])\n",
    "        count_tensor = torch.tensor(count, dtype=torch.float32)\n",
    "        \n",
    "        if self.return_meta:\n",
    "            return {\n",
    "                'image': image,\n",
    "                'count': count_tensor,\n",
    "                'image_name': img_name\n",
    "            }\n",
    "        \n",
    "        return image, count_tensor\n",
    "\n",
    "def create_train_val_split(\n",
    "    image_dir: str,\n",
    "    crowd_counts: Dict[str, int],\n",
    "    val_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Create stratified train-validation split based on crowd count ranges.\n",
    "    \"\"\"\n",
    "    image_files = list(crowd_counts.keys())\n",
    "    counts = [crowd_counts[f] for f in image_files]\n",
    "    \n",
    "    # Create stratification bins based on count ranges\n",
    "    bins = [0, 5, 15, 30, 50, 100, float('inf')]\n",
    "    strata = np.digitize(counts, bins)\n",
    "    \n",
    "    train_files, val_files = train_test_split(\n",
    "        image_files,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=strata\n",
    "    )\n",
    "    \n",
    "    return train_files, val_files\n",
    "\n",
    "# Create train-validation split\n",
    "train_files, val_files = create_train_val_split(TRAIN_IMG_DIR, crowd_counts, val_size=0.2)\n",
    "\n",
    "print(f\"Training set: {len(train_files)} images\")\n",
    "print(f\"Validation set: {len(val_files)} images\")\n",
    "\n",
    "# Analyze split distribution\n",
    "train_counts = [crowd_counts[f] for f in train_files]\n",
    "val_counts = [crowd_counts[f] for f in val_files]\n",
    "\n",
    "print(f\"\\nTraining set statistics:\")\n",
    "print(f\"  Mean: {np.mean(train_counts):.2f}, Std: {np.std(train_counts):.2f}\")\n",
    "print(f\"Validation set statistics:\")\n",
    "print(f\"  Mean: {np.mean(val_counts):.2f}, Std: {np.std(val_counts):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206abbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CrowdCountingRegressionDataset(\n",
    "    image_dir=TRAIN_IMG_DIR,\n",
    "    crowd_counts=crowd_counts,\n",
    "    image_files=train_files,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = CrowdCountingRegressionDataset(\n",
    "    image_dir=TRAIN_IMG_DIR,\n",
    "    crowd_counts=crowd_counts,\n",
    "    image_files=val_files,\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Visualize sample batch\n",
    "def visualize_sample_batch(data_loader, dataset_name=\"Dataset\"):\n",
    "    \"\"\"Visualize a sample batch from the data loader.\"\"\"\n",
    "    images, counts = next(iter(data_loader))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(8, len(images))):\n",
    "        img = images[i]\n",
    "        # Denormalize image for visualization\n",
    "        img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'Count: {counts[i].item():.0f}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Sample Batch from {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample_batch(train_loader, \"Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7303d7",
   "metadata": {},
   "source": [
    "# 5. Model Training with Feature Extraction\n",
    "\n",
    "Implement comprehensive training pipeline with validation monitoring and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 0.001,\n",
    "    patience: int = 10,\n",
    "    loss_function: str = 'mse'\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the crowd counting regression model.\n",
    "    \"\"\"\n",
    "    # Loss function selection\n",
    "    if loss_function == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_function == 'mae':\n",
    "        criterion = nn.L1Loss()\n",
    "    elif loss_function == 'huber':\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss_function}\")\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=0.01)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_mae': [],\n",
    "        'val_mae': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Starting training with {loss_function.upper()} loss...\")\n",
    "    print(f\"Model: {model.backbone_name}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mae = 0.0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_mae += nn.L1Loss()(outputs, targets).item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                val_mae += nn.L1Loss()(outputs, targets).item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_mae /= len(train_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['val_mae'].append(val_mae)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1:3d}/{num_epochs} | '\n",
    "                  f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | '\n",
    "                  f'Train MAE: {train_mae:.4f} | Val MAE: {val_mae:.4f} | '\n",
    "                  f'LR: {current_lr:.6f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    return history\n",
    "\n",
    "# Initialize and train model\n",
    "model = create_model(backbone_name='resnet50', pretrained=True)\n",
    "\n",
    "# Train the model\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    patience=10,\n",
    "    loss_function='mse'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8304047",
   "metadata": {},
   "source": [
    "# 6. Evaluation and Performance Metrics\n",
    "\n",
    "Comprehensive evaluation using multiple regression metrics and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b949bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    dataset_name: str = \"Dataset\"\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(all_predictions)\n",
    "    targets = np.array(all_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((targets - predictions) / (targets + 1e-8))) * 100  # Add small epsilon to avoid division by zero\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    \n",
    "    # Additional metrics\n",
    "    correlation = np.corrcoef(targets, predictions)[0, 1]\n",
    "    max_error = np.max(np.abs(targets - predictions))\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R²': r2,\n",
    "        'Correlation': correlation,\n",
    "        'Max Error': max_error,\n",
    "        'Mean Target': np.mean(targets),\n",
    "        'Mean Prediction': np.mean(predictions)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Evaluation Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric:15s}: {value:8.4f}\")\n",
    "    \n",
    "    return metrics, predictions, targets\n",
    "\n",
    "# Evaluate on training and validation sets\n",
    "train_metrics, train_preds, train_targets = evaluate_model(model, train_loader, \"Training Set\")\n",
    "val_metrics, val_preds, val_targets = evaluate_model(model, val_loader, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ebb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: Dict[str, List[float]]) -> None:\n",
    "    \"\"\"Plot training history curves.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(history['train_loss'], label='Training Loss', color='blue')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation Loss', color='red')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE curves\n",
    "    axes[0, 1].plot(history['train_mae'], label='Training MAE', color='blue')\n",
    "    axes[0, 1].plot(history['val_mae'], label='Validation MAE', color='red')\n",
    "    axes[0, 1].set_title('Training and Validation MAE')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Mean Absolute Error')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    axes[1, 0].plot(history['learning_rate'], color='green')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss difference\n",
    "    loss_diff = np.array(history['val_loss']) - np.array(history['train_loss'])\n",
    "    axes[1, 1].plot(loss_diff, color='purple')\n",
    "    axes[1, 1].set_title('Validation - Training Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss Difference')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74cf60",
   "metadata": {},
   "source": [
    "# 7. Visualization of Predictions\n",
    "\n",
    "Create comprehensive visualizations to analyze model performance and prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ecbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_visualizations(\n",
    "    predictions: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    dataset_name: str = \"Dataset\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create comprehensive prediction visualization plots.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Scatter plot: Predictions vs Targets\n",
    "    axes[0, 0].scatter(targets, predictions, alpha=0.6, s=30)\n",
    "    axes[0, 0].plot([targets.min(), targets.max()], [targets.min(), targets.max()], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('True Count')\n",
    "    axes[0, 0].set_ylabel('Predicted Count')\n",
    "    axes[0, 0].set_title(f'{dataset_name}: Predictions vs True Values')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    correlation = np.corrcoef(targets, predictions)[0, 1]\n",
    "    axes[0, 0].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                    transform=axes[0, 0].transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "    \n",
    "    # 2. Residual plot\n",
    "    residuals = predictions - targets\n",
    "    axes[0, 1].scatter(targets, residuals, alpha=0.6, s=30)\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('True Count')\n",
    "    axes[0, 1].set_ylabel('Residuals (Predicted - True)')\n",
    "    axes[0, 1].set_title(f'{dataset_name}: Residual Plot')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Error distribution\n",
    "    axes[0, 2].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 2].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[0, 2].set_xlabel('Residuals')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title(f'{dataset_name}: Error Distribution')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Absolute error vs true count\n",
    "    abs_errors = np.abs(residuals)\n",
    "    axes[1, 0].scatter(targets, abs_errors, alpha=0.6, s=30)\n",
    "    axes[1, 0].set_xlabel('True Count')\n",
    "    axes[1, 0].set_ylabel('Absolute Error')\n",
    "    axes[1, 0].set_title(f'{dataset_name}: Absolute Error vs True Count')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Percentage error vs true count\n",
    "    percentage_errors = np.abs(residuals) / (targets + 1e-8) * 100\n",
    "    axes[1, 1].scatter(targets, percentage_errors, alpha=0.6, s=30)\n",
    "    axes[1, 1].set_xlabel('True Count')\n",
    "    axes[1, 1].set_ylabel('Absolute Percentage Error (%)')\n",
    "    axes[1, 1].set_title(f'{dataset_name}: Percentage Error vs True Count')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Cumulative error distribution\n",
    "    sorted_abs_errors = np.sort(abs_errors)\n",
    "    cumulative_percentages = np.arange(1, len(sorted_abs_errors) + 1) / len(sorted_abs_errors) * 100\n",
    "    axes[1, 2].plot(sorted_abs_errors, cumulative_percentages)\n",
    "    axes[1, 2].set_xlabel('Absolute Error')\n",
    "    axes[1, 2].set_ylabel('Cumulative Percentage (%)')\n",
    "    axes[1, 2].set_title(f'{dataset_name}: Cumulative Error Distribution')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print error statistics\n",
    "    print(f\"\\n{dataset_name} Error Analysis:\")\n",
    "    print(f\"Mean Absolute Error: {np.mean(abs_errors):.2f}\")\n",
    "    print(f\"Median Absolute Error: {np.median(abs_errors):.2f}\")\n",
    "    print(f\"90th Percentile Error: {np.percentile(abs_errors, 90):.2f}\")\n",
    "    print(f\"95th Percentile Error: {np.percentile(abs_errors, 95):.2f}\")\n",
    "    print(f\"Max Absolute Error: {np.max(abs_errors):.2f}\")\n",
    "\n",
    "# Create visualizations for both datasets\n",
    "create_prediction_visualizations(train_preds, train_targets, \"Training Set\")\n",
    "create_prediction_visualizations(val_preds, val_targets, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_predictions(\n",
    "    model: nn.Module,\n",
    "    dataset: CrowdCountingRegressionDataset,\n",
    "    num_samples: int = 12,\n",
    "    sort_by_error: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize sample predictions with images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions for entire dataset\n",
    "    dataset.return_meta = True\n",
    "    sample_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for data in sample_loader:\n",
    "            image = data['image'].to(device)\n",
    "            true_count = data['count'].item()\n",
    "            image_name = data['image_name'][0]\n",
    "            \n",
    "            pred_count = model(image).item()\n",
    "            error = abs(pred_count - true_count)\n",
    "            \n",
    "            # Denormalize image for visualization\n",
    "            img_viz = data['image'].squeeze()\n",
    "            img_viz = img_viz * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            img_viz = torch.clamp(img_viz, 0, 1).permute(1, 2, 0).numpy()\n",
    "            \n",
    "            samples.append({\n",
    "                'image': img_viz,\n",
    "                'true_count': true_count,\n",
    "                'pred_count': pred_count,\n",
    "                'error': error,\n",
    "                'image_name': image_name\n",
    "            })\n",
    "    \n",
    "    # Sort by error if requested\n",
    "    if sort_by_error:\n",
    "        samples.sort(key=lambda x: x['error'], reverse=True)\n",
    "    \n",
    "    # Create visualization\n",
    "    cols = 4\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 4 * rows))\n",
    "    axes = axes.flatten() if rows > 1 else [axes] if cols == 1 else axes\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        if i < len(samples):\n",
    "            sample = samples[i]\n",
    "            axes[i].imshow(sample['image'])\n",
    "            axes[i].set_title(f\"True: {sample['true_count']:.0f}, Pred: {sample['pred_count']:.1f}\\n\"\n",
    "                             f\"Error: {sample['error']:.1f}\", fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    title = \"Worst Predictions\" if sort_by_error else \"Sample Predictions\"\n",
    "    plt.suptitle(f'{title} - Validation Set', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    dataset.return_meta = False  # Reset\n",
    "\n",
    "# Visualize worst and best predictions\n",
    "visualize_sample_predictions(model, val_dataset, num_samples=12, sort_by_error=True)\n",
    "visualize_sample_predictions(model, val_dataset, num_samples=12, sort_by_error=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6486e",
   "metadata": {},
   "source": [
    "# 8. Test Set Prediction and Submission\n",
    "\n",
    "Generate predictions for the test set and create the final submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ff78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"Dataset class for test images without labels.\"\"\"\n",
    "    def __init__(self, image_dir: str, transform: transforms.Compose):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')],\n",
    "                                 key=lambda x: int(os.path.splitext(x)[0]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, img_name\n",
    "\n",
    "def generate_test_predictions(\n",
    "    model: nn.Module,\n",
    "    test_dir: str,\n",
    "    output_file: str = 'submission.csv'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate predictions for test set and create submission file.\n",
    "    \"\"\"\n",
    "    print(\"Generating test set predictions...\")\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_dataset = TestDataset(test_dir, test_transforms)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_names = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, names in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert to numpy and round to integers\n",
    "            batch_preds = outputs.cpu().numpy()\n",
    "            predictions.extend([max(0, int(round(pred))) for pred in batch_preds])  # Ensure non-negative integers\n",
    "            image_names.extend(names)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'image_id': image_names,\n",
    "        'predicted_count': predictions\n",
    "    })\n",
    "    \n",
    "    # Sort by image_id to ensure correct order\n",
    "    submission_df['sort_key'] = submission_df['image_id'].apply(lambda x: int(os.path.splitext(x)[0]))\n",
    "    submission_df = submission_df.sort_values('sort_key').drop('sort_key', axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Submission file '{output_file}' created successfully!\")\n",
    "    print(f\"Generated predictions for {len(submission_df)} test images\")\n",
    "    \n",
    "    # Display statistics\n",
    "    pred_counts = submission_df['predicted_count'].values\n",
    "    print(f\"\\nTest Predictions Statistics:\")\n",
    "    print(f\"Min: {pred_counts.min()}\")\n",
    "    print(f\"Max: {pred_counts.max()}\")\n",
    "    print(f\"Mean: {pred_counts.mean():.2f}\")\n",
    "    print(f\"Median: {np.median(pred_counts):.2f}\")\n",
    "    print(f\"Std: {pred_counts.std():.2f}\")\n",
    "    \n",
    "    # Visualize prediction distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(pred_counts, bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Predicted Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Test Predictions')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(pred_counts)\n",
    "    plt.ylabel('Predicted Count')\n",
    "    plt.title('Test Predictions Box Plot')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "# Generate test predictions\n",
    "submission_df = generate_test_predictions(model, TEST_IMG_DIR, 'regression_submission.csv')\n",
    "\n",
    "# Display first 10 rows of submission\n",
    "print(\"\\nFirst 10 rows of submission:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd6fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample test predictions\n",
    "def visualize_test_predictions(\n",
    "    model: nn.Module,\n",
    "    test_dir: str,\n",
    "    submission_df: pd.DataFrame,\n",
    "    num_samples: int = 12\n",
    ") -> None:\n",
    "    \"\"\"Visualize sample test predictions.\"\"\"\n",
    "    test_dataset = TestDataset(test_dir, test_transforms)\n",
    "    \n",
    "    # Sample random images\n",
    "    indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            image, img_name = test_dataset[idx]\n",
    "            \n",
    "            # Get prediction\n",
    "            image_batch = image.unsqueeze(0).to(device)\n",
    "            pred_count = model(image_batch).item()\n",
    "            \n",
    "            # Denormalize image for visualization\n",
    "            img_viz = image\n",
    "            img_viz = img_viz * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            img_viz = torch.clamp(img_viz, 0, 1).permute(1, 2, 0).numpy()\n",
    "            \n",
    "            axes[i].imshow(img_viz)\n",
    "            axes[i].set_title(f'{img_name}\\nPredicted: {pred_count:.1f}')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Test Predictions', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_test_predictions(model, TEST_IMG_DIR, submission_df, num_samples=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e096601e",
   "metadata": {},
   "source": [
    "## Model Summary and Next Steps\n",
    "\n",
    "### Model Performance Summary:\n",
    "- **Architecture**: ResNet50 + Regression Head\n",
    "- **Training Strategy**: Transfer learning with ImageNet pretrained weights\n",
    "- **Loss Function**: MSE with L2 regularization\n",
    "- **Validation MAE**: {val_metrics['MAE']:.2f}\n",
    "- **Validation R²**: {val_metrics['R²']:.3f}\n",
    "\n",
    "### Key Advantages of Regression Approach:\n",
    "1. **Direct count prediction** - No need for density map generation\n",
    "2. **Fast inference** - Single forward pass gives count\n",
    "3. **Transfer learning** - Leverage powerful pretrained features\n",
    "4. **Memory efficient** - Lower memory usage than density approaches\n",
    "\n",
    "### Potential Improvements:\n",
    "1. **Ensemble methods** - Combine multiple backbone architectures\n",
    "2. **Data augmentation** - More sophisticated augmentation strategies\n",
    "3. **Loss function tuning** - Experiment with Huber loss or custom losses\n",
    "4. **Multi-scale training** - Train on different image resolutions\n",
    "5. **Feature engineering** - Add crowd density-specific features\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different backbones (EfficientNet, Vision Transformers)\n",
    "2. Implement model ensembling\n",
    "3. Analyze failure cases and improve data preprocessing\n",
    "4. Consider hybrid approaches combining regression with attention mechanisms"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
