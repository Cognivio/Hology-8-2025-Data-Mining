{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556d8318",
   "metadata": {},
   "source": [
    "# SFCN Training Notebook\n",
    "# Scale-aware Feature Pyramid Network for Crowd Counting\n",
    "\n",
    "This notebook implements SFCN with FPN backbone, hybrid detection mechanism, and comprehensive training pipeline for crowd counting tasks.\n",
    "\n",
    "## Features:\n",
    "- VGG16-BN backbone with Feature Pyramid Network (FPN)\n",
    "- Hybrid detection combining density regression and detection heads\n",
    "- Advanced data augmentation with CLAHE/HistEq contrast enhancement\n",
    "- Mixed precision training with gradient accumulation\n",
    "- Adaptive density map generation with KNN-based sigma\n",
    "- Early stopping and model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c06325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import random\n",
    "import warnings\n",
    "from glob import glob\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Constants\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05705a",
   "metadata": {},
   "source": [
    "## Dataset Download and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de3a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-136fdaa9-46e5-496f-a1bf-658f167dddc9\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-136fdaa9-46e5-496f-a1bf-658f167dddc9\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n",
      "User uploaded file \"kaggle.json\" with length 64 bytes\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Kaggle secret key\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "You don't need to run this cell if you're not in Google Colab environment\n",
    "\"\"\"\n",
    "\n",
    "!pip install -q kaggle\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Dataset already exists, skipping download.\n",
      "Dataset already extracted at: /content/drive/MyDrive/PROJECTS/Cognivio/Percobaan Hology 8 2025/dataset\n",
      "Dataset already available in Colab local storage.\n"
     ]
    }
   ],
   "source": [
    "# @title Setup dataset in Colab\n",
    "\n",
    "\"\"\"\n",
    "You don't need to run this cell if you're not in Google Colab environment\n",
    "\"\"\"\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "zip_path = \"/content/penyisihan-hology-8-0-2025-data-mining.zip\"\n",
    "drive_extract_path = \"/content/drive/MyDrive/PROJECTS/Cognivio/Percobaan Hology 8 2025/dataset\"\n",
    "local_dataset_path = \"/content/dataset\"  # for current session\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Download zip (if not exists in /content)\n",
    "# ---------------------------\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Dataset not found locally, downloading...\")\n",
    "    !kaggle competitions download -c penyisihan-hology-8-0-2025-data-mining -p /content\n",
    "else:\n",
    "    print(\"Dataset already exists, skipping download.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Extract to Google Drive (for backup)\n",
    "# ---------------------------\n",
    "os.makedirs(drive_extract_path, exist_ok=True)\n",
    "\n",
    "if not os.listdir(drive_extract_path):  # Check if folder is empty\n",
    "    print(\"Extracting dataset to Google Drive...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(drive_extract_path)\n",
    "    print(\"Dataset extracted to:\", drive_extract_path)\n",
    "else:\n",
    "    print(\"Dataset already extracted at:\", drive_extract_path)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Copy dataset to local /content (faster training)\n",
    "# ---------------------------\n",
    "if not os.path.exists(local_dataset_path):\n",
    "    print(\"Copying dataset to Colab local storage (/content)...\")\n",
    "    !cp -r \"$drive_extract_path\" \"$local_dataset_path\"\n",
    "else:\n",
    "    print(\"Dataset already available in Colab local storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754911c9",
   "metadata": {},
   "source": [
    "## Configuration and Hyperparameters\n",
    "\n",
    "Set up all training configuration parameters. You can modify these values to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data paths\n",
    "    'img_dir': Path(local_dataset_path) / \"train\" / \"images\", # 'data/train/images' for local path\n",
    "    'label_dir': Path(local_dataset_path) / \"train\" / \"labels\",   # # 'data/train/labels' for local path\n",
    "    \n",
    "    # Model parameters\n",
    "    'base_size': 896,\n",
    "    'down': 8,\n",
    "    'patch_size': 384,\n",
    "    'patches_per_image': 6,\n",
    "    'avoid_empty_patches': True,\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 4,\n",
    "    'epochs': 2,   # change it to 120 \n",
    "    'lr': 1e-4,\n",
    "    'criterion': 'huber',  # 'mse' or 'huber'\n",
    "    'count_loss_alpha': 0.2,\n",
    "    'det_loss_alpha': 1.0,\n",
    "    'early_stop_patience': 40,\n",
    "    \n",
    "    # Optimization\n",
    "    'num_workers': 6,\n",
    "    'amp': True,  # Automatic Mixed Precision\n",
    "    'accum_steps': 2,  # Gradient accumulation steps\n",
    "    \n",
    "    # Augmentation and preprocessing\n",
    "    'sigma_mode': 'adaptive',  # 'adaptive' or 'constant'\n",
    "    'contrast_mode': 'clahe',  # 'none', 'clahe', 'histeq'\n",
    "    'clahe_clip': 2.0,\n",
    "    'clahe_grid': 8,\n",
    "    \n",
    "    # Evaluation\n",
    "    'hybrid_eval': True,\n",
    "    'dens_thresh': 0.25,\n",
    "    'det_prob_thr': 0.5,\n",
    "    'compare_pred': True,\n",
    "    \n",
    "    # Model saving\n",
    "    'save_path': 'sfcn_fpn_hybrid_896p384_b4_e300.pth',\n",
    "    'seed': 1337\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed: int = 1337) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(config['seed'])\n",
    "print(f\"üé≤ Random seed set to {config['seed']}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145bee6",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Implement essential utility functions for image processing, point parsing, and density map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c53e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread_rgb(\n",
    "    path: str\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Load image in RGB format.\"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read image: {path}\")\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def letterbox(\n",
    "    img: np.ndarray, \n",
    "    target: int = 512\n",
    ") -> Tuple[np.ndarray, float, int, int]:\n",
    "    \"\"\"Resize image to target size while maintaining aspect ratio with letterboxing.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    scale = min(target / h, target / w)\n",
    "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "    img_rs = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "    top = (target - nh) // 2\n",
    "    left = (target - nw) // 2\n",
    "    canvas = np.zeros((target, target, 3), dtype=img_rs.dtype)\n",
    "    canvas[top:top + nh, left:left + nw] = img_rs\n",
    "    return canvas, scale, left, top\n",
    "\n",
    "def parse_points_from_json(\n",
    "    path: str\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Parse annotation points from JSON file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    \n",
    "    pts, num = [], None\n",
    "    if isinstance(obj, dict) and \"points\" in obj:\n",
    "        pts = obj[\"points\"]\n",
    "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
    "        if len(pts) and isinstance(pts[0], dict):\n",
    "            pts = [[p[\"x\"], p[\"y\"]] for p in pts if \"x\" in p and \"y\" in p]\n",
    "    elif isinstance(obj, dict) and \"annotations\" in obj:\n",
    "        pts = [[a[\"x\"], a[\"y\"]] for a in obj[\"annotations\"] if \"x\" in a and \"y\" in a]\n",
    "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
    "    elif isinstance(obj, list):\n",
    "        pts = obj\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown JSON schema: {path}\")\n",
    "    \n",
    "    pts_arr = np.array(pts, dtype=np.float32) if len(pts) > 0 else np.zeros((0, 2), np.float32)\n",
    "    return pts_arr, num\n",
    "\n",
    "def derive_json_path(\n",
    "    label_dir: str, \n",
    "    img_path: str\n",
    ") -> str:\n",
    "    \"\"\"Find corresponding JSON label file for image.\"\"\"\n",
    "    name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    cand = os.path.join(label_dir, name + \".json\")\n",
    "    if os.path.exists(cand):\n",
    "        return cand\n",
    "    \n",
    "    m = re.findall(r\"\\d+\", name)\n",
    "    if m:\n",
    "        alt = os.path.join(label_dir, f\"{m[-1]}.json\")\n",
    "        if os.path.exists(alt):\n",
    "            return alt\n",
    "    \n",
    "    lst = glob(os.path.join(label_dir, f\"{name}*.json\"))\n",
    "    if lst:\n",
    "        return lst[0]\n",
    "    \n",
    "    raise FileNotFoundError(f\"JSON label not found for {img_path}\")\n",
    "\n",
    "print(\"‚úÖ Basic utility functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_map(\n",
    "    points_xy: np.ndarray, \n",
    "    grid_size: int, \n",
    "    down: int = 8,\n",
    "    sigma_mode: str = \"adaptive\", \n",
    "    knn: int = 3,\n",
    "    beta: float = 0.3, \n",
    "    const_sigma: float = 2.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate density map from point annotations.\"\"\"\n",
    "    target = grid_size\n",
    "    dh, dw = target // down, target // down\n",
    "    den = np.zeros((dh, dw), dtype=np.float32)\n",
    "    \n",
    "    if len(points_xy) == 0:\n",
    "        return den\n",
    "    \n",
    "    pts = points_xy.copy()\n",
    "    pts[:, 0] = pts[:, 0] * (dw / target)\n",
    "    pts[:, 1] = pts[:, 1] * (dh / target)\n",
    "    \n",
    "    tree = KDTree(pts) if len(pts) > 1 else None\n",
    "    \n",
    "    for (x, y) in pts:\n",
    "        if sigma_mode == \"adaptive\" and tree is not None and len(pts) > 3:\n",
    "            dists, _ = tree.query([x, y], k=min(knn + 1, len(pts)))\n",
    "            sigma = max(1.0, float(np.mean(dists[1:])) * beta)\n",
    "        else:\n",
    "            sigma = const_sigma\n",
    "        \n",
    "        cx, cy = float(x), float(y)\n",
    "        rad = int(max(1, math.ceil(3 * sigma)))\n",
    "        x0, x1 = max(0, int(math.floor(cx - rad))), min(dw, int(math.ceil(cx + rad + 1)))\n",
    "        y0, y1 = max(0, int(math.floor(cy - rad))), min(dh, int(math.ceil(cy + rad + 1)))\n",
    "        \n",
    "        if x1 <= x0 or y1 <= y0:\n",
    "            continue\n",
    "        \n",
    "        xs = np.arange(x0, x1) - cx\n",
    "        ys = np.arange(y0, y1) - cy\n",
    "        xx, yy = np.meshgrid(xs, ys)\n",
    "        g = np.exp(-(xx**2 + yy**2) / (2 * sigma * sigma))\n",
    "        s = g.sum()\n",
    "        if s > 0:\n",
    "            den[y0:y1, x0:x1] += (g / s).astype(np.float32)\n",
    "    \n",
    "    return den\n",
    "\n",
    "def make_occupancy_map(\n",
    "    points_xy: np.ndarray, \n",
    "    grid_size: int, \n",
    "    down: int = 8, \n",
    "    radius: int = 1\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate binary occupancy map from point annotations.\"\"\"\n",
    "    target = grid_size\n",
    "    dh, dw = target // down, target // down\n",
    "    occ = np.zeros((dh, dw), dtype=np.float32)\n",
    "    \n",
    "    if len(points_xy) == 0:\n",
    "        return occ\n",
    "    \n",
    "    pts = points_xy.copy()\n",
    "    pts[:, 0] = pts[:, 0] * (dw / target)\n",
    "    pts[:, 1] = pts[:, 1] * (dh / target)\n",
    "    \n",
    "    for (x, y) in pts:\n",
    "        xi, yi = int(round(x)), int(round(y))\n",
    "        x0, x1 = max(0, xi - radius), min(dw - 1, xi + radius)\n",
    "        y0, y1 = max(0, yi - radius), min(dh - 1, yi + radius)\n",
    "        occ[y0:y1 + 1, x0:x1 + 1] = 1.0\n",
    "    \n",
    "    return occ\n",
    "\n",
    "def apply_contrast_enhancement(\n",
    "    img_rgb: np.ndarray, \n",
    "    mode: str = \"none\",\n",
    "    clahe_clip: float = 2.0, \n",
    "    clahe_grid: int = 8\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply contrast enhancement to image.\"\"\"\n",
    "    if mode == \"none\":\n",
    "        return img_rgb\n",
    "    elif mode == \"clahe\":\n",
    "        lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=clahe_clip, tileGridSize=(clahe_grid, clahe_grid))\n",
    "        l2 = clahe.apply(l)\n",
    "        return cv2.cvtColor(cv2.merge([l2, a, b]), cv2.COLOR_LAB2RGB)\n",
    "    elif mode == \"histeq\":\n",
    "        ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2YCrCb)\n",
    "        y, cr, cb = cv2.split(ycrcb)\n",
    "        y2 = cv2.equalizeHist(y)\n",
    "        return cv2.cvtColor(cv2.merge([y2, cr, cb]), cv2.COLOR_YCrCb2RGB)\n",
    "    \n",
    "    return img_rgb\n",
    "\n",
    "print(\"‚úÖ Density map and contrast enhancement functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5fce8",
   "metadata": {},
   "source": [
    "## Dataset Implementation\n",
    "\n",
    "Create the CrowdDataset class with comprehensive data augmentation and preprocessing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be620ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_dir: str, \n",
    "        label_dir: str, \n",
    "        base_size: int = 768, \n",
    "        down: int = 8,\n",
    "        aug: bool = True, \n",
    "        mode: str = \"train\", \n",
    "        patch_size: int = 0, \n",
    "        patches_per_image: int = 1,\n",
    "        sigma_mode: str = \"adaptive\", \n",
    "        avoid_empty_patches: bool = False,\n",
    "        contrast_mode: str = \"none\", \n",
    "        clahe_clip: float = 2.0, \n",
    "        clahe_grid: int = 8\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_paths = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "        if len(self.img_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {img_dir}\")\n",
    "        \n",
    "        self.label_dir = label_dir\n",
    "        self.base_size = base_size\n",
    "        self.down = down\n",
    "        self.aug = aug\n",
    "        self.mode = mode\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_per_image = max(1, int(patches_per_image))\n",
    "        self.sigma_mode = sigma_mode\n",
    "        self.avoid_empty_patches = avoid_empty_patches\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.clahe_clip = clahe_clip\n",
    "        self.clahe_grid = clahe_grid\n",
    "\n",
    "        # Transform pipeline\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        self.color_jit = transforms.ColorJitter(0.1, 0.1, 0.1, 0.05)\n",
    "        self.extra_aug = transforms.Compose([\n",
    "            transforms.RandomApply([transforms.RandomGrayscale(p=1.0)], p=0.15),\n",
    "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.30),\n",
    "            transforms.RandomResizedCrop(size=base_size, scale=(0.80, 1.00), ratio=(0.90, 1.10)),\n",
    "        ])\n",
    "        self.rand_erase = transforms.RandomErasing(\n",
    "            p=0.25, scale=(0.02, 0.20), ratio=(0.3, 3.3), value=0\n",
    "        )\n",
    "        \n",
    "        self.effective_len = (len(self.img_paths) * self.patches_per_image \n",
    "                             if (self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1) \n",
    "                             else len(self.img_paths))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.effective_len\n",
    "\n",
    "    def _load_img_pts(\n",
    "        self, \n",
    "        idx_base: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load and preprocess image with corresponding points.\"\"\"\n",
    "        pimg = self.img_paths[idx_base]\n",
    "        img = imread_rgb(pimg)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Apply contrast enhancement\n",
    "        img = apply_contrast_enhancement(img, self.contrast_mode, self.clahe_clip, self.clahe_grid)\n",
    "        \n",
    "        # Load points\n",
    "        plbl = derive_json_path(self.label_dir, pimg)\n",
    "        pts, _ = parse_points_from_json(plbl)\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n",
    "            # Horizontal flip\n",
    "            img = img[:, ::-1, :].copy()\n",
    "            if len(pts) > 0:\n",
    "                pts = pts.copy()\n",
    "                pts[:, 0] = (w - 1) - pts[:, 0]\n",
    "\n",
    "        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n",
    "            # Color jitter\n",
    "            pil = transforms.ToPILImage()(img)\n",
    "            pil = self.color_jit(pil)\n",
    "            img = np.array(pil)\n",
    "\n",
    "        if self.mode == \"train\" and self.aug:\n",
    "            # Extra augmentations\n",
    "            pil = transforms.ToPILImage()(img)\n",
    "            pil = self.extra_aug(pil)\n",
    "            img = np.array(pil)\n",
    "\n",
    "        # Letterbox resize\n",
    "        canvas, scale, left, top = letterbox(img, target=self.base_size)\n",
    "        if len(pts) > 0:\n",
    "            pts_tr = pts.copy()\n",
    "            pts_tr[:, 0] = pts_tr[:, 0] * scale + left\n",
    "            pts_tr[:, 1] = pts_tr[:, 1] * scale + top\n",
    "            # Filter points within canvas\n",
    "            m = ((pts_tr[:, 0] >= 0) & (pts_tr[:, 0] < self.base_size) & \n",
    "                 (pts_tr[:, 1] >= 0) & (pts_tr[:, 1] < self.base_size))\n",
    "            pts_tr = pts_tr[m]\n",
    "        else:\n",
    "            pts_tr = np.zeros((0, 2), np.float32)\n",
    "        \n",
    "        return canvas, pts_tr\n",
    "\n",
    "    def __getitem__(\n",
    "        self, \n",
    "        index: int\n",
    "    ):\n",
    "        idx_base = (index // self.patches_per_image \n",
    "                   if (self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1) \n",
    "                   else index)\n",
    "        idx_base %= len(self.img_paths)\n",
    "        \n",
    "        img_lb, pts_tr = self._load_img_pts(idx_base)\n",
    "\n",
    "        # Patch-based training\n",
    "        if self.mode == \"train\" and self.patch_size > 0:\n",
    "            ps = self.patch_size\n",
    "            max_off = self.base_size - ps\n",
    "            for attempt in range(10 if self.avoid_empty_patches else 1):\n",
    "                ox = 0 if max_off <= 0 else random.randint(0, max_off)\n",
    "                oy = 0 if max_off <= 0 else random.randint(0, max_off)\n",
    "                crop = img_lb[oy:oy + ps, ox:ox + ps, :]\n",
    "                \n",
    "                if len(pts_tr) > 0:\n",
    "                    pts_c = pts_tr.copy()\n",
    "                    pts_c[:, 0] -= ox\n",
    "                    pts_c[:, 1] -= oy\n",
    "                    m = ((pts_c[:, 0] >= 0) & (pts_c[:, 0] < ps) & \n",
    "                         (pts_c[:, 1] >= 0) & (pts_c[:, 1] < ps))\n",
    "                    pts_c = pts_c[m]\n",
    "                else:\n",
    "                    pts_c = np.zeros((0, 2), np.float32)\n",
    "                \n",
    "                if not self.avoid_empty_patches or len(pts_c) > 0 or attempt == 9:\n",
    "                    img_out = crop\n",
    "                    pts_out = pts_c\n",
    "                    break\n",
    "            grid = ps\n",
    "        else:\n",
    "            img_out = img_lb\n",
    "            pts_out = pts_tr\n",
    "            grid = self.base_size\n",
    "\n",
    "        # Generate density and occupancy maps\n",
    "        den = make_density_map(pts_out, grid_size=grid, down=self.down, sigma_mode=self.sigma_mode)\n",
    "        occ = make_occupancy_map(pts_out, grid_size=grid, down=self.down, radius=1)\n",
    "\n",
    "        # Convert to tensors\n",
    "        t = self.to_tensor(img_out)\n",
    "        if self.mode == \"train\" and self.aug:\n",
    "            t = self.rand_erase(t)\n",
    "        t = self.normalize(t)\n",
    "        \n",
    "        d = torch.from_numpy(den).unsqueeze(0)\n",
    "        o = torch.from_numpy(occ).unsqueeze(0)\n",
    "        c = torch.tensor([float(len(pts_out))], dtype=torch.float32)\n",
    "        \n",
    "        return t, d, c, o\n",
    "\n",
    "print(\"‚úÖ CrowdDataset class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3fa7d5",
   "metadata": {},
   "source": [
    "## Model Architecture (SFCN with FPN)\n",
    "\n",
    "Implement the SFCN model with VGG16-BN backbone, Feature Pyramid Network, and dual prediction heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEncoder(nn.Module):\n",
    "    \"\"\"Spatial Feature Encoder with horizontal and vertical convolutions.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        channels: int, \n",
    "        k: int = 9\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        p = k // 2\n",
    "        self.h1 = nn.Conv2d(channels, channels, (1, k), padding=(0, p), groups=channels, bias=False)\n",
    "        self.h2 = nn.Conv2d(channels, channels, (1, k), padding=(0, p), groups=channels, bias=False)\n",
    "        self.v1 = nn.Conv2d(channels, channels, (k, 1), padding=(p, 0), groups=channels, bias=False)\n",
    "        self.v2 = nn.Conv2d(channels, channels, (k, 1), padding=(p, 0), groups=channels, bias=False)\n",
    "        self.proj = nn.Conv2d(channels * 4, channels, 1, bias=False)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Collect outputs\n",
    "        features = [self.h1(x), self.h2(x), self.v1(x), self.v2(x)]\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        concatenated = torch.cat(features, dim=1)\n",
    "        \n",
    "        # Apply projection and activation\n",
    "        projected = self.proj(concatenated)\n",
    "        activated = self.act(projected)\n",
    "        \n",
    "        return activated\n",
    "\n",
    "\n",
    "class SFCN_VGG_FPN(nn.Module):\n",
    "    \"\"\"\n",
    "    SFCN with VGG16-BN backbone and Feature Pyramid Network.\n",
    "    Outputs (density_map, detection_logits) for hybrid counting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained: bool = True, \n",
    "        use_spatial_encoder: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        self.features = vgg.features  # VGG16-BN features\n",
    "        self.use_spatial = use_spatial_encoder\n",
    "        \n",
    "        # Lateral connections (1x1 conv to 256 channels)\n",
    "        self.lat_c2 = nn.Conv2d(128, 256, 1)  # stride 4\n",
    "        self.lat_c3 = nn.Conv2d(256, 256, 1)  # stride 8\n",
    "        self.lat_c4 = nn.Conv2d(512, 256, 1)  # stride 16\n",
    "        \n",
    "        # Smooth layers (3x3 conv)\n",
    "        self.smooth2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.smooth3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.smooth4 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        \n",
    "        # Prediction heads\n",
    "        self.senc = SpatialEncoder(256, k=9) if self.use_spatial else nn.Identity()\n",
    "        \n",
    "        # Density regression head\n",
    "        self.density_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, 1),\n",
    "        )\n",
    "        \n",
    "        # Detection head (for hybrid gating)\n",
    "        self.detect_head = nn.Conv2d(256, 1, 1)\n",
    "\n",
    "    def _forward_backbone(self, x):\n",
    "        \"\"\"Forward through VGG backbone and extract multi-scale features.\"\"\"\n",
    "        c2 = c3 = c4 = None\n",
    "        pool_count = 0\n",
    "        \n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                pool_count += 1\n",
    "                if pool_count == 2:\n",
    "                    c2 = x  # stride 4, 128 channels\n",
    "                elif pool_count == 3:\n",
    "                    c3 = x  # stride 8, 256 channels\n",
    "                elif pool_count == 4:\n",
    "                    c4 = x  # stride 16, 512 channels\n",
    "        \n",
    "        return c2, c3, c4\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        \"\"\"Upsample x and add to y.\"\"\"\n",
    "        return F.interpolate(x, size=y.shape[-2:], mode=\"nearest\") + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract multi-scale features\n",
    "        c2, c3, c4 = self._forward_backbone(x)\n",
    "        \n",
    "        # Build feature pyramid\n",
    "        p4 = self.lat_c4(c4)\n",
    "        p3 = self._upsample_add(p4, self.lat_c3(c3))\n",
    "        p2 = self._upsample_add(p3, self.lat_c2(c2))\n",
    "        \n",
    "        # Smooth features\n",
    "        p4 = self.smooth4(p4)\n",
    "        p3 = self.smooth3(p3)\n",
    "        p2 = self.smooth2(p2)\n",
    "        \n",
    "        # Use stride-8 features for prediction\n",
    "        f = p3\n",
    "        f = self.senc(f)\n",
    "        \n",
    "        # Dual predictions\n",
    "        dens = F.softplus(self.density_head(f))  # Density map\n",
    "        det = self.detect_head(f)  # Detection logits\n",
    "        \n",
    "        return dens, det\n",
    "\n",
    "# Backward compatibility alias\n",
    "SFCN_VGG = SFCN_VGG_FPN\n",
    "\n",
    "print(\"‚úÖ SFCN model architecture implemented!\")\n",
    "\n",
    "# Test model instantiation\n",
    "model_test = SFCN_VGG_FPN(pretrained=False)\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model_test.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 3, 384, 384)\n",
    "with torch.no_grad():\n",
    "    dens_out, det_out = model_test(dummy_input)\n",
    "    print(f\"üîç Input shape: {dummy_input.shape}\")\n",
    "    print(f\"üéØ Density output shape: {dens_out.shape}\")\n",
    "    print(f\"üéØ Detection output shape: {det_out.shape}\")\n",
    "\n",
    "del model_test, dummy_input, dens_out, det_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dcb02d",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n",
    "\n",
    "Implement training and evaluation functions with hybrid loss computation and mixed precision support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b760cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model, \n",
    "    loader, \n",
    "    device, \n",
    "    optimizer, \n",
    "    scaler=None, \n",
    "    accum_steps=1,\n",
    "    criterion=\"mse\", \n",
    "    count_loss_alpha=0.0, \n",
    "    det_loss_alpha=1.0\n",
    ") -> float:\n",
    "    \"\"\"Train model for one epoch with hybrid loss computation.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Loss functions\n",
    "    if criterion == \"mse\":\n",
    "        crit = nn.MSELoss()\n",
    "    elif criterion == \"huber\":\n",
    "        crit = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        raise ValueError(\"criterion must be 'mse' or 'huber'\")\n",
    "    \n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    running_mae, nimg = 0.0, 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for step, (imgs, dens, _, occ) in enumerate(tqdm(loader, desc=\"Train\", leave=False), 1):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        occ = occ.to(device)\n",
    "        \n",
    "        if scaler is not None:\n",
    "            # Mixed precision training\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                pred_den, pred_det = model(imgs)\n",
    "                \n",
    "                # Compute losses\n",
    "                map_loss = crit(pred_den, dens)\n",
    "                det_loss = bce(pred_det, occ)\n",
    "                total_loss = map_loss + det_loss_alpha * det_loss\n",
    "                \n",
    "                if count_loss_alpha > 0.0:\n",
    "                    count_loss = F.mse_loss(pred_den.sum((1, 2, 3)), dens.sum((1, 2, 3)))\n",
    "                    total_loss = total_loss + count_loss_alpha * count_loss\n",
    "                \n",
    "                total_loss = total_loss / accum_steps\n",
    "            \n",
    "            scaler.scale(total_loss).backward()\n",
    "            \n",
    "            if step % accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            # Regular training\n",
    "            pred_den, pred_det = model(imgs)\n",
    "            \n",
    "            map_loss = crit(pred_den, dens)\n",
    "            det_loss = bce(pred_det, occ)\n",
    "            total_loss = map_loss + det_loss_alpha * det_loss\n",
    "            \n",
    "            if count_loss_alpha > 0.0:\n",
    "                count_loss = F.mse_loss(pred_den.sum((1, 2, 3)), dens.sum((1, 2, 3)))\n",
    "                total_loss = total_loss + count_loss_alpha * count_loss\n",
    "            \n",
    "            total_loss = total_loss / accum_steps\n",
    "            total_loss.backward()\n",
    "            \n",
    "            if step % accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Update metrics\n",
    "        with torch.no_grad():\n",
    "            pc = pred_den.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            gc = dens.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            running_mae += np.abs(pc - gc).sum()\n",
    "            nimg += imgs.size(0)\n",
    "    \n",
    "    return running_mae / max(1, nimg)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model, \n",
    "    loader, \n",
    "    device, \n",
    "    dens_thresh: float = 0.25, \n",
    "    det_prob_thr: float = 0.5,\n",
    "    hybrid: bool = True\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate model with optional hybrid gating mechanism.\"\"\"\n",
    "    model.eval()\n",
    "    mae = mse = nimg = 0.0\n",
    "    \n",
    "    for imgs, dens, _, occ in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        occ = occ.to(device)\n",
    "        \n",
    "        pred_den, pred_det = model(imgs)\n",
    "        \n",
    "        if hybrid:\n",
    "            # Hybrid gating: use detection probabilities to gate density predictions\n",
    "            prob = torch.sigmoid(pred_det)\n",
    "            w = (pred_den < dens_thresh).float() * prob\n",
    "            comb = (1.0 - w) * pred_den + w * prob\n",
    "            pred_cnt = comb.sum((1, 2, 3))\n",
    "        else:\n",
    "            pred_cnt = pred_den.sum((1, 2, 3))\n",
    "        \n",
    "        diff = (pred_cnt - dens.sum((1, 2, 3))).detach().cpu().numpy()\n",
    "        mae += np.abs(diff).sum()\n",
    "        mse += (diff**2).sum()\n",
    "        nimg += imgs.size(0)\n",
    "    \n",
    "    return mae / max(1, nimg), math.sqrt(mse / max(1, nimg))\n",
    "\n",
    "print(\"‚úÖ Training and evaluation functions implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c72b6",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Set up datasets and data loaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ff0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data directories exist\n",
    "img_dir = config['img_dir']\n",
    "label_dir = config['label_dir']\n",
    "\n",
    "print(f\"üîç Checking data directories...\")\n",
    "print(f\"   Images: {img_dir} -> {'‚úÖ Exists' if os.path.exists(img_dir) else '‚ùå Not found'}\")\n",
    "print(f\"   Labels: {label_dir} -> {'‚úÖ Exists' if os.path.exists(label_dir) else '‚ùå Not found'}\")\n",
    "\n",
    "if os.path.exists(img_dir):\n",
    "    all_imgs = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "    print(f\"üì∏ Found {len(all_imgs)} images\")\n",
    "    \n",
    "    if len(all_imgs) >= 2:\n",
    "        # Split into train/validation\n",
    "        random.shuffle(all_imgs)\n",
    "        n_images = len(all_imgs)\n",
    "        n_val = max(1, int(0.1 * n_images))  # 10% for validation\n",
    "        n_train = n_images - n_val\n",
    "        train_imgs = all_imgs[:n_train]\n",
    "        val_imgs = all_imgs[n_train:]\n",
    "        \n",
    "        print(f\"üìä Data split: {n_train} train, {n_val} validation\")\n",
    "        \n",
    "        # Create datasets\n",
    "        print(\"üî® Creating datasets...\")\n",
    "        train_ds = CrowdDataset(\n",
    "            img_dir=config['img_dir'],\n",
    "            label_dir=config['label_dir'],\n",
    "            base_size=config['base_size'],\n",
    "            down=config['down'],\n",
    "            aug=True,\n",
    "            mode=\"train\",\n",
    "            patch_size=config['patch_size'],\n",
    "            patches_per_image=config['patches_per_image'],\n",
    "            sigma_mode=config['sigma_mode'],\n",
    "            avoid_empty_patches=config['avoid_empty_patches'],\n",
    "            contrast_mode=config['contrast_mode'],\n",
    "            clahe_clip=config['clahe_clip'],\n",
    "            clahe_grid=config['clahe_grid']\n",
    "        )\n",
    "        \n",
    "        val_ds = CrowdDataset(\n",
    "            img_dir=config['img_dir'],\n",
    "            label_dir=config['label_dir'],\n",
    "            base_size=config['base_size'],\n",
    "            down=config['down'],\n",
    "            aug=False,\n",
    "            mode=\"val\",\n",
    "            patch_size=0,\n",
    "            patches_per_image=1,\n",
    "            sigma_mode=config['sigma_mode'],\n",
    "            avoid_empty_patches=False,\n",
    "            contrast_mode=config['contrast_mode'],\n",
    "            clahe_clip=config['clahe_clip'],\n",
    "            clahe_grid=config['clahe_grid']\n",
    "        )\n",
    "        \n",
    "        # Set image paths\n",
    "        train_ds.img_paths = train_imgs\n",
    "        val_ds.img_paths = val_imgs\n",
    "        train_ds.effective_len = (len(train_ds.img_paths) * train_ds.patches_per_image \n",
    "                                 if (train_ds.mode == \"train\" and train_ds.patch_size > 0 and train_ds.patches_per_image > 1) \n",
    "                                 else len(train_ds.img_paths))\n",
    "        val_ds.effective_len = len(val_ds.img_paths)\n",
    "        \n",
    "        print(f\"üì¶ Training dataset: {len(train_ds)} samples\")\n",
    "        print(f\"üì¶ Validation dataset: {len(val_ds)} samples\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=config['num_workers'],\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=config['num_workers'],\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"üöÄ Data loaders created!\")\n",
    "        print(f\"   Train batches: {len(train_loader)}\")\n",
    "        print(f\"   Val batches: {len(val_loader)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Need at least 2 images for train/val split\")\n",
    "else:\n",
    "    print(\"‚ùå Image directory not found. Please check the path and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc1181",
   "metadata": {},
   "source": [
    "## Model Training Loop\n",
    "\n",
    "Initialize the model, optimizer, and execute the main training loop with early stopping and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and training components\n",
    "print(\"üèóÔ∏è  Initializing model and training components...\")\n",
    "\n",
    "# Create model\n",
    "model = SFCN_VGG_FPN(pretrained=True).to(device)\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if (config['amp'] and device.type == \"cuda\") else None\n",
    "print(f\"‚ö° Mixed precision: {'Enabled' if scaler is not None else 'Disabled'}\")\n",
    "\n",
    "# Training state\n",
    "best_mae = float(\"inf\")\n",
    "patience = config['early_stop_patience']\n",
    "bad_epochs = 0\n",
    "training_history = {\n",
    "    'train_mae': [],\n",
    "    'val_mae': [],\n",
    "    'val_rmse': [],\n",
    "    'epoch': []\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Setup complete! Starting training...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31728a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Loop\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    print(f\"\\nüìà Epoch {epoch}/{config['epochs']}\")\n",
    "    \n",
    "    # Training phase\n",
    "    tr_mae = train_epoch(\n",
    "        model, train_loader, device, optimizer, scaler, \n",
    "        accum_steps=config['accum_steps'],\n",
    "        criterion=config['criterion'], \n",
    "        count_loss_alpha=config['count_loss_alpha'], \n",
    "        det_loss_alpha=config['det_loss_alpha']\n",
    "    )\n",
    "    \n",
    "    # Validation phase\n",
    "    va_mae, va_rmse = evaluate(\n",
    "        model, val_loader, device, \n",
    "        dens_thresh=config['dens_thresh'], \n",
    "        det_prob_thr=config['det_prob_thr'], \n",
    "        hybrid=config['hybrid_eval']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Update training history\n",
    "    training_history['train_mae'].append(tr_mae)\n",
    "    training_history['val_mae'].append(va_mae)\n",
    "    training_history['val_rmse'].append(va_rmse)\n",
    "    training_history['epoch'].append(epoch)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"üìä Train MAE: {tr_mae:.3f} | Val MAE: {va_mae:.3f} | Val RMSE: {va_rmse:.3f}\")\n",
    "    print(f\"üéØ Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Model checkpointing\n",
    "    if va_mae + 1e-6 < best_mae:\n",
    "        best_mae = va_mae\n",
    "        bad_epochs = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"val_mae\": va_mae,\n",
    "            \"config\": config,\n",
    "            \"training_history\": training_history\n",
    "        }\n",
    "        torch.save(checkpoint, config['save_path'])\n",
    "        print(f\"‚úÖ New best model saved! MAE: {best_mae:.3f}\")\n",
    "        \n",
    "        # Optional: Print GT vs Pred comparison for validation set\n",
    "        if config['compare_pred'] and epoch % 10 == 0:  # Every 10 epochs to avoid spam\n",
    "            print(f\"üìã Sample GT vs Pred comparison (Epoch {epoch}):\")\n",
    "            model.eval()\n",
    "            sample_count = 0\n",
    "            with torch.no_grad():\n",
    "                for imgs, dens, _, occ in val_loader:\n",
    "                    if sample_count >= 5:  # Show only first 5 samples\n",
    "                        break\n",
    "                    \n",
    "                    imgs = imgs.to(device)\n",
    "                    dens = dens.to(device)\n",
    "                    pred_den, pred_det = model(imgs)\n",
    "                    \n",
    "                    if config['hybrid_eval']:\n",
    "                        prob = torch.sigmoid(pred_det)\n",
    "                        w = (pred_den < config['dens_thresh']).float() * prob\n",
    "                        comb = (1.0 - w) * pred_den + w * prob\n",
    "                        pred_cnt_batch = comb.sum((1, 2, 3))\n",
    "                    else:\n",
    "                        pred_cnt_batch = pred_den.sum((1, 2, 3))\n",
    "                    \n",
    "                    gt_cnt_batch = dens.sum((1, 2, 3))\n",
    "                    \n",
    "                    for j in range(min(pred_cnt_batch.size(0), 5 - sample_count)):\n",
    "                        idx = sample_count\n",
    "                        if idx < len(val_ds.img_paths):\n",
    "                            image_name = os.path.basename(val_ds.img_paths[idx])\n",
    "                        else:\n",
    "                            image_name = f\"sample_{idx}\"\n",
    "                        \n",
    "                        print(f\"   {image_name}: GT {gt_cnt_batch[j].item():.1f}, Pred {pred_cnt_batch[j].item():.1f}\")\n",
    "                        sample_count += 1\n",
    "                    \n",
    "                    if sample_count >= 5:\n",
    "                        break\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        print(f\"‚è≥ No improvement for {bad_epochs} epoch(s)\")\n",
    "        \n",
    "        if bad_epochs >= patience:\n",
    "            print(f\"üõë Early stopping at epoch {epoch}. Best Val MAE: {best_mae:.3f}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üéâ Training completed! Best validation MAE: {best_mae:.3f}\")\n",
    "print(f\"üíæ Best model saved to: {config['save_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4dda7",
   "metadata": {},
   "source": [
    "## Results Visualization and Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ae551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if len(training_history['epoch']) > 0:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Training and validation MAE\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(training_history['epoch'], training_history['train_mae'], 'b-', label='Train MAE', linewidth=2)\n",
    "    plt.plot(training_history['epoch'], training_history['val_mae'], 'r-', label='Val MAE', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation RMSE\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(training_history['epoch'], training_history['val_rmse'], 'g-', label='Val RMSE', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Validation RMSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning curve analysis\n",
    "    plt.subplot(1, 3, 3)\n",
    "    train_mae = np.array(training_history['train_mae'])\n",
    "    val_mae = np.array(training_history['val_mae'])\n",
    "    gap = val_mae - train_mae\n",
    "    plt.plot(training_history['epoch'], gap, 'purple', label='Val-Train Gap', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE Gap')\n",
    "    plt.title('Generalization Gap')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"üìà Training Statistics:\")\n",
    "    print(f\"   Final Train MAE: {training_history['train_mae'][-1]:.3f}\")\n",
    "    print(f\"   Final Val MAE: {training_history['val_mae'][-1]:.3f}\")\n",
    "    print(f\"   Final Val RMSE: {training_history['val_rmse'][-1]:.3f}\")\n",
    "    print(f\"   Best Val MAE: {min(training_history['val_mae']):.3f}\")\n",
    "    print(f\"   Total Epochs: {len(training_history['epoch'])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training history available. Please run the training loop first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final evaluation\n",
    "if os.path.exists(config['save_path']):\n",
    "    print(\"üìÅ Loading best model checkpoint...\")\n",
    "    checkpoint = torch.load(config['save_path'], map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(f\"‚úÖ Loaded model from epoch {checkpoint['epoch']} with MAE {checkpoint['val_mae']:.3f}\")\n",
    "\n",
    "# Final evaluation with detailed predictions\n",
    "print(\"\\nüéØ Final Evaluation - Ground Truth vs Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "image_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs, dens, _, occ) in enumerate(val_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        pred_den, pred_det = model(imgs)\n",
    "        \n",
    "        # Use hybrid gating if enabled\n",
    "        if config['hybrid_eval']:\n",
    "            prob = torch.sigmoid(pred_det)\n",
    "            w = (pred_den < config['dens_thresh']).float() * prob\n",
    "            comb = (1.0 - w) * pred_den + w * prob\n",
    "            pred_cnt_batch = comb.sum((1, 2, 3))\n",
    "        else:\n",
    "            pred_cnt_batch = pred_den.sum((1, 2, 3))\n",
    "        \n",
    "        gt_cnt_batch = dens.sum((1, 2, 3))\n",
    "        \n",
    "        # Store results\n",
    "        for j in range(pred_cnt_batch.size(0)):\n",
    "            idx = batch_idx * config['batch_size'] + j\n",
    "            if idx < len(val_ds.img_paths):\n",
    "                image_name = os.path.basename(val_ds.img_paths[idx])\n",
    "                image_names.append(image_name)\n",
    "                predictions.append(pred_cnt_batch[j].item())\n",
    "                ground_truths.append(gt_cnt_batch[j].item())\n",
    "                \n",
    "                print(f\"{image_name:20s}: GT {gt_cnt_batch[j].item():6.1f}, Pred {pred_cnt_batch[j].item():6.1f}, \"\n",
    "                      f\"Error {abs(gt_cnt_batch[j].item() - pred_cnt_batch[j].item()):5.1f}\")\n",
    "\n",
    "# Calculate final metrics\n",
    "if len(predictions) > 0:\n",
    "    predictions = np.array(predictions)\n",
    "    ground_truths = np.array(ground_truths)\n",
    "    \n",
    "    mae = np.mean(np.abs(predictions - ground_truths))\n",
    "    mse = np.mean((predictions - ground_truths)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(\"\\nüìä Final Metrics:\")\n",
    "    print(f\"   MAE:  {mae:.3f}\")\n",
    "    print(f\"   MSE:  {mse:.3f}\")\n",
    "    print(f\"   RMSE: {rmse:.3f}\")\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(ground_truths, predictions, alpha=0.6, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    max_val = max(np.max(ground_truths), np.max(predictions))\n",
    "    min_val = min(np.min(ground_truths), np.min(predictions))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    plt.xlabel('Ground Truth Count')\n",
    "    plt.ylabel('Predicted Count')\n",
    "    plt.title(f'Ground Truth vs Predictions (MAE: {mae:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    correlation = np.corrcoef(ground_truths, predictions)[0, 1]\n",
    "    plt.text(\n",
    "        0.05, \n",
    "        0.95, \n",
    "        f'Correlation: {correlation:.3f}', \n",
    "        transform=plt.gca().transAxes, \n",
    "        bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5)\\\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüîó Correlation coefficient: {correlation:.3f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No validation data processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ea92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions with density maps\n",
    "def visualize_predictions(\n",
    "    model, \n",
    "    dataset, \n",
    "    device, \n",
    "    num_samples=4\n",
    "):\n",
    "    \"\"\"Visualize model predictions with density maps.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(20, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            if i >= len(dataset):\n",
    "                break\n",
    "                \n",
    "            # Get sample\n",
    "            img_tensor, gt_density, gt_count, gt_occ = dataset[i]\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Model prediction\n",
    "            pred_density, pred_det = model(img_tensor)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            img_np = img_tensor.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "            img_np = img_np * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)  # Denormalize\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            \n",
    "            gt_density_np = gt_density.squeeze().numpy()\n",
    "            pred_density_np = pred_density.squeeze().cpu().numpy()\n",
    "            pred_det_np = torch.sigmoid(pred_det).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Calculate counts\n",
    "            gt_count_val = gt_density_np.sum()\n",
    "            pred_count_val = pred_density_np.sum()\n",
    "            \n",
    "            # Hybrid count if enabled\n",
    "            if config['hybrid_eval']:\n",
    "                prob = pred_det_np\n",
    "                w = (pred_density_np < config['dens_thresh']).astype(float) * prob\n",
    "                comb = (1.0 - w) * pred_density_np + w * prob\n",
    "                hybrid_count = comb.sum()\n",
    "            else:\n",
    "                hybrid_count = pred_count_val\n",
    "            \n",
    "            # Plot original image\n",
    "            axes[i, 0].imshow(img_np)\n",
    "            axes[i, 0].set_title(f'Original Image')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Plot ground truth density\n",
    "            im1 = axes[i, 1].imshow(gt_density_np, cmap='jet')\n",
    "            axes[i, 1].set_title(f'GT Density (Count: {gt_count_val:.1f})')\n",
    "            axes[i, 1].axis('off')\n",
    "            plt.colorbar(im1, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # Plot predicted density\n",
    "            im2 = axes[i, 2].imshow(pred_density_np, cmap='jet')\n",
    "            axes[i, 2].set_title(f'Pred Density (Count: {pred_count_val:.1f})')\n",
    "            axes[i, 2].axis('off')\n",
    "            plt.colorbar(im2, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # Plot detection map\n",
    "            im3 = axes[i, 3].imshow(pred_det_np, cmap='hot', vmin=0, vmax=1)\n",
    "            axes[i, 3].set_title(f'Detection Map (Hybrid: {hybrid_count:.1f})')\n",
    "            axes[i, 3].axis('off')\n",
    "            plt.colorbar(im3, ax=axes[i, 3], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions if validation dataset exists\n",
    "try:\n",
    "    print(\"üé® Visualizing sample predictions...\")\n",
    "    visualize_predictions(model, val_ds, device, num_samples=3)\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è  Validation dataset not available. Please run the data preparation cells first.\")\n",
    "\n",
    "print(\"\\n‚úÖ Training and evaluation complete!\")\n",
    "print(f\"üéØ Final model saved at: {config['save_path']}\")\n",
    "print(\"üìä Training history and visualizations have been generated above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
