{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c8ca3e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/FarrelAD/Hology-8-2025-Data-Mining-PRIVATE/blob/dev%2Fvidi/notebooks/vidi/SFCN/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00ccb0",
   "metadata": {},
   "source": [
    "# Crowd-counting with method SFCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638d25b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "A new method that actually has better results than the other models we’ve tried so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f1a55",
   "metadata": {},
   "source": [
    "# 1. Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3736ee4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5562fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb228f",
   "metadata": {},
   "source": [
    "## Dataset Download and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3364cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Kaggle secret key\n",
    "!pip install -q kaggle\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f15866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup dataset in Colab\n",
    "import zipfile\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "zip_path = \"/content/penyisihan-hology-8-0-2025-data-mining.zip\"\n",
    "drive_extract_path = \"/content/drive/MyDrive/PROJECTS/Cognivio/Percobaan Hology 8 2025/dataset\"\n",
    "local_dataset_path = \"/content/dataset\"  # for current session\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Download zip (if not exists in /content)\n",
    "# ---------------------------\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Dataset not found locally, downloading...\")\n",
    "    !kaggle competitions download -c penyisihan-hology-8-0-2025-data-mining -p /content\n",
    "else:\n",
    "    print(\"Dataset already exists, skipping download.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Extract to Google Drive (for backup)\n",
    "# ---------------------------\n",
    "os.makedirs(drive_extract_path, exist_ok=True)\n",
    "\n",
    "if not os.listdir(drive_extract_path):  # Check if folder is empty\n",
    "    print(\"Extracting dataset to Google Drive...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(drive_extract_path)\n",
    "    print(\"Dataset extracted to:\", drive_extract_path)\n",
    "else:\n",
    "    print(\"Dataset already extracted at:\", drive_extract_path)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Copy dataset to local /content (faster training)\n",
    "# ---------------------------\n",
    "if not os.path.exists(local_dataset_path):\n",
    "    print(\"Copying dataset to Colab local storage (/content)...\")\n",
    "    !cp -r \"$drive_extract_path\" \"$local_dataset_path\"\n",
    "else:\n",
    "    print(\"Dataset already available in Colab local storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0d7b5",
   "metadata": {},
   "source": [
    "## Some Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet mean and std for normalisation\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Dataset path\n",
    "TRAIN_IMG_DIR = os.path.join(local_dataset_path, \"train\", \"images\")\n",
    "TRAIN_LABEL_DIR = os.path.join(local_dataset_path, \"train\", \"labels\")\n",
    "TEST_IMG_DIR  = os.path.join(local_dataset_path, \"test\", \"images\")\n",
    "\n",
    "\n",
    "# Seed for better reproducibility\n",
    "SEED = 1337 # or 31\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "EPOCHS = 120\n",
    "ACCUM_STEPS = 2\n",
    "EARLY_STOP_PATIENCE = 15\n",
    "CUDA_AMP = True         # CUDA Automatic Mixed Precision (AMP).\n",
    "\n",
    "BASE_SIZE = 768\n",
    "DOWN = 8\n",
    "PATCH_SIZE = 384\n",
    "PATCHES_PER_IMAGE = 4\n",
    "AVOID_EMPTY_PATCHES = True\n",
    "SIGMA_MODE = \"adaptive\"\n",
    "\n",
    "BATCH = 4\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da059d1",
   "metadata": {},
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b351e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_json_path(\n",
    "    lbl_dir: str, \n",
    "    img_path: str\n",
    ") -> str:\n",
    "    \"\"\"Derive the corresponding JSON label path for a given image path.\n",
    "\n",
    "    Tries to match the image basename to a JSON file in the label directory.\n",
    "    Falls back to matching digits if an exact name isn't found.\n",
    "    \"\"\"\n",
    "    name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    cand = os.path.join(lbl_dir, name + \".json\")\n",
    "    if os.path.exists(cand):\n",
    "        return cand\n",
    "    # Try matching trailing digits\n",
    "    m = re.findall(r\"\\d+\", name)\n",
    "    if m:\n",
    "        alt = os.path.join(lbl_dir, f\"{m[-1]}.json\")\n",
    "        if os.path.exists(alt):\n",
    "            return alt\n",
    "    # Fallback: any file starting with the same name\n",
    "    lst = glob(os.path.join(lbl_dir, f\"{name}*.json\"))\n",
    "    if lst:\n",
    "        return lst[0]\n",
    "    raise FileNotFoundError(f\"JSON label not found for {img_path}\")\n",
    "\n",
    "def parse_points_from_json(\n",
    "    path: str\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Parse annotated points from a JSON file.\n",
    "\n",
    "    Supports several common crowd counting annotation formats. Returns an\n",
    "    array of shape (N, 2) containing [x, y] coordinates and, if present\n",
    "    in the JSON, the declared number of people. If no `human_num` or\n",
    "    `num_human` field is present the count is returned as None.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    pts: List[List[float]]\n",
    "    num = None\n",
    "    if isinstance(obj, dict) and \"points\" in obj:\n",
    "        pts = obj[\"points\"]\n",
    "        # unify possible keys for ground-truth count\n",
    "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
    "        # If points are dicts, extract x/y fields\n",
    "        if len(pts) > 0 and isinstance(pts[0], dict):\n",
    "            pts = [[p[\"x\"], p[\"y\"]] for p in pts if \"x\" in p and \"y\" in p]\n",
    "    elif isinstance(obj, dict) and \"annotations\" in obj:\n",
    "        pts = [[a[\"x\"], a[\"y\"]] for a in obj[\"annotations\"] if \"x\" in a and \"y\" in a]\n",
    "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
    "    elif isinstance(obj, list):\n",
    "        # direct list of [x,y]\n",
    "        pts = obj\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown JSON schema: {path}\")\n",
    "    pts_arr = np.array(pts, dtype=np.float32) if len(pts) > 0 else np.zeros((0, 2), np.float32)\n",
    "    return pts_arr, num\n",
    "\n",
    "def letterbox(\n",
    "    img: np.ndarray, \n",
    "    target: int = 512\n",
    ") -> Tuple[np.ndarray, float, int, int]:\n",
    "    \"\"\"Resize and pad an image to a square canvas without distortion.\n",
    "\n",
    "    Returns the padded image, the scale factor used, and the left/top\n",
    "    padding applied. The output size is (target, target).\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    scale = min(target / h, target / w)\n",
    "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "    img_rs = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "    top = (target - nh) // 2\n",
    "    left = (target - nw) // 2\n",
    "    canvas = np.zeros((target, target, 3), dtype=img_rs.dtype)\n",
    "    canvas[top:top + nh, left:left + nw] = img_rs\n",
    "    return canvas, scale, left, top\n",
    "\n",
    "\n",
    "def make_density_map(\n",
    "    points_xy: np.ndarray,\n",
    "    grid_size: int,\n",
    "    down: int = 8,\n",
    "    sigma_mode: str = \"adaptive\",\n",
    "    knn: int = 3,\n",
    "    beta: float = 0.3,\n",
    "    const_sigma: float = 2.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate a density map on a grid given annotated points.\n",
    "\n",
    "    The density map is of shape (grid_size//down, grid_size//down). Each\n",
    "    point is represented by a Gaussian whose sigma is either constant or\n",
    "    computed from the k-nearest neighbours. The integral of the density map\n",
    "    approximates the number of points.\n",
    "    \"\"\"\n",
    "    target = grid_size\n",
    "    dh, dw = target // down, target // down\n",
    "    den = np.zeros((dh, dw), dtype=np.float32)\n",
    "    if len(points_xy) == 0:\n",
    "        return den\n",
    "    # Scale points to the density map resolution\n",
    "    pts = points_xy.copy()\n",
    "    pts[:, 0] = pts[:, 0] * (dw / target)\n",
    "    pts[:, 1] = pts[:, 1] * (dh / target)\n",
    "    tree = KDTree(pts) if len(pts) > 1 else None\n",
    "    for (x, y) in pts:\n",
    "        # Determine sigma\n",
    "        if sigma_mode == \"adaptive\" and tree is not None and len(pts) > 3:\n",
    "            dists, _ = tree.query([x, y], k=min(knn + 1, len(pts)))\n",
    "            sigma = max(1.0, float(np.mean(dists[1:])) * beta)\n",
    "        else:\n",
    "            sigma = const_sigma\n",
    "        cx, cy = float(x), float(y)\n",
    "        rad = int(max(1, math.ceil(3 * sigma)))\n",
    "        x0, x1 = max(0, int(math.floor(cx - rad))), min(dw, int(math.ceil(cx + rad + 1)))\n",
    "        y0, y1 = max(0, int(math.floor(cy - rad))), min(dh, int(math.ceil(cy + rad + 1)))\n",
    "        if x1 <= x0 or y1 <= y0:\n",
    "            continue\n",
    "        xs = np.arange(x0, x1) - cx\n",
    "        ys = np.arange(y0, y1) - cy\n",
    "        xx, yy = np.meshgrid(xs, ys)\n",
    "        g = np.exp(-(xx**2 + yy**2) / (2 * sigma * sigma))\n",
    "        s = g.sum()\n",
    "        if s > 0:\n",
    "            den[y0:y1, x0:x1] += (g / s).astype(np.float32)\n",
    "    return den"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f3feb",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfcb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of images and randomly shuffle before splitting\n",
    "all_imgs = sorted(glob(os.path.join(TRAIN_IMG_DIR, \"*.*\")))\n",
    "\n",
    "random.shuffle(all_imgs)\n",
    "n_images = len(all_imgs)\n",
    "\n",
    "if n_images < 2:\n",
    "    raise ValueError(\"Need at least 2 images for training and validation\")\n",
    "\n",
    "n_val = max(1, int(0.1 * n_images))\n",
    "n_train = n_images - n_val\n",
    "\n",
    "train_imgs = all_imgs[:n_train]\n",
    "val_imgs = all_imgs[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17100915",
   "metadata": {},
   "source": [
    "# 3. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdDataset(Dataset):\n",
    "    \"\"\"Custom dataset for crowd counting.\n",
    "\n",
    "    Supports optional random patch cropping on training data. When\n",
    "    `avoid_empty_patches` is set, the dataset will attempt to choose a\n",
    "    patch containing at least one point, falling back to a random crop if\n",
    "    it fails after a number of retries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir: str,\n",
    "        label_dir: str,\n",
    "        base_size: int = 768,\n",
    "        down: int = 8,\n",
    "        aug: bool = True,\n",
    "        mode: str = \"train\",\n",
    "        patch_size: int = 0,\n",
    "        patches_per_image: int = 1,\n",
    "        sigma_mode: str = \"adaptive\",\n",
    "        avoid_empty_patches: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_paths = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "        if len(self.img_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {img_dir}\")\n",
    "        self.label_dir = label_dir\n",
    "        self.base_size = base_size\n",
    "        self.down = down\n",
    "        self.aug = aug\n",
    "        self.mode = mode\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_per_image = max(1, int(patches_per_image))\n",
    "        self.sigma_mode = sigma_mode\n",
    "        self.avoid_empty_patches = avoid_empty_patches\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        self.color_jit = transforms.ColorJitter(0.1, 0.1, 0.1, 0.05)\n",
    "\n",
    "        # Compute effective length for patch training\n",
    "        if self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1:\n",
    "            self.effective_len = len(self.img_paths) * self.patches_per_image\n",
    "        else:\n",
    "            self.effective_len = len(self.img_paths)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.effective_len\n",
    "\n",
    "    def _load_img_pts(\n",
    "        self, \n",
    "        idx_base: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load an image and its transformed annotation points.\"\"\"\n",
    "        pimg = self.img_paths[idx_base]\n",
    "        img = cv2.imread(pimg, cv2.IMREAD_COLOR)\n",
    "        h, w = img.shape[:2]\n",
    "        plbl = derive_json_path(self.label_dir, pimg)\n",
    "        pts, _ = parse_points_from_json(plbl)\n",
    "        # Augment: horizontal flip\n",
    "        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n",
    "            img = img[:, ::-1, :].copy()\n",
    "            if len(pts) > 0:\n",
    "                pts = pts.copy()\n",
    "                pts[:, 0] = (w - 1) - pts[:, 0]\n",
    "        # Augment: colour jitter\n",
    "        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n",
    "            pil = transforms.ToPILImage()(img)\n",
    "            pil = self.color_jit(pil)\n",
    "            img = np.array(pil)\n",
    "        # Letterbox to base size\n",
    "        canvas, scale, left, top = letterbox(img, target=self.base_size)\n",
    "        if len(pts) > 0:\n",
    "            pts_tr = pts.copy()\n",
    "            pts_tr[:, 0] = pts_tr[:, 0] * scale + left\n",
    "            pts_tr[:, 1] = pts_tr[:, 1] * scale + top\n",
    "            # Clamp to canvas bounds\n",
    "            m = (\n",
    "                (pts_tr[:, 0] >= 0)\n",
    "                & (pts_tr[:, 0] < self.base_size)\n",
    "                & (pts_tr[:, 1] >= 0)\n",
    "                & (pts_tr[:, 1] < self.base_size)\n",
    "            )\n",
    "            pts_tr = pts_tr[m]\n",
    "        else:\n",
    "            pts_tr = np.zeros((0, 2), np.float32)\n",
    "        return canvas, pts_tr\n",
    "\n",
    "    def __getitem__(\n",
    "        self, \n",
    "        index: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Map global index to an image index (for patch training)\n",
    "        if self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1:\n",
    "            idx_base = index // self.patches_per_image\n",
    "        else:\n",
    "            idx_base = index\n",
    "        idx_base %= len(self.img_paths)\n",
    "\n",
    "        img_lb, pts_tr = self._load_img_pts(idx_base)\n",
    "\n",
    "        # Optional patch cropping\n",
    "        if self.mode == \"train\" and self.patch_size > 0:\n",
    "            ps = self.patch_size\n",
    "            if ps > self.base_size:\n",
    "                raise ValueError(\"patch_size must be <= base_size\")\n",
    "            max_off = self.base_size - ps\n",
    "            pts_out = np.zeros((0, 2), np.float32)\n",
    "            # Attempt to find a non-empty patch when requested\n",
    "            for attempt in range(10) if self.avoid_empty_patches else [0]:\n",
    "                ox = 0 if max_off <= 0 else random.randint(0, max_off)\n",
    "                oy = 0 if max_off <= 0 else random.randint(0, max_off)\n",
    "                crop = img_lb[oy : oy + ps, ox : ox + ps, :]\n",
    "                if len(pts_tr) > 0:\n",
    "                    pts_c = pts_tr.copy()\n",
    "                    pts_c[:, 0] -= ox\n",
    "                    pts_c[:, 1] -= oy\n",
    "                    m = (\n",
    "                        (pts_c[:, 0] >= 0)\n",
    "                        & (pts_c[:, 0] < ps)\n",
    "                        & (pts_c[:, 1] >= 0)\n",
    "                        & (pts_c[:, 1] < ps)\n",
    "                    )\n",
    "                    pts_c = pts_c[m]\n",
    "                else:\n",
    "                    pts_c = np.zeros((0, 2), np.float32)\n",
    "                # If avoid_empty_patches is False, we break immediately (no retries)\n",
    "                if not self.avoid_empty_patches or len(pts_c) > 0 or attempt == 9:\n",
    "                    pts_out = pts_c\n",
    "                    img_out = crop\n",
    "                    break\n",
    "            grid = ps\n",
    "        else:\n",
    "            img_out = img_lb\n",
    "            pts_out = pts_tr\n",
    "            grid = self.base_size\n",
    "\n",
    "        # Build density map\n",
    "        den = make_density_map(\n",
    "            pts_out,\n",
    "            grid_size=grid,\n",
    "            down=self.down,\n",
    "            sigma_mode=self.sigma_mode,\n",
    "        )\n",
    "\n",
    "        # Optional check: ensure density integrates to the number of points\n",
    "        if self.mode != \"train\" or random.random() < 0.02:\n",
    "            cnt = float(len(pts_out))\n",
    "            s = float(den.sum())\n",
    "            if abs(s - cnt) > 0.05:\n",
    "                print(f\"[warn] density sum {s:.3f} != count {cnt:.3f}\")\n",
    "\n",
    "        # Convert to tensors and normalise\n",
    "        t = self.to_tensor(img_out)\n",
    "        t = self.normalize(t)\n",
    "        d = torch.from_numpy(den).unsqueeze(0)\n",
    "        c = torch.tensor([float(len(pts_out))], dtype=torch.float32)\n",
    "        return t, d, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate datasets\n",
    "train_ds = CrowdDataset(\n",
    "    img_dir=TRAIN_IMG_DIR,\n",
    "    label_dir=TRAIN_LABEL_DIR,\n",
    "    base_size=BASE_SIZE,\n",
    "    down=DOWN,\n",
    "    aug=True,\n",
    "    mode=\"train\",\n",
    "    patch_size=PATCH_SIZE,\n",
    "    patches_per_image=PATCHES_PER_IMAGE,\n",
    "    sigma_mode=SIGMA_MODE,\n",
    "    avoid_empty_patches=AVOID_EMPTY_PATCHES,\n",
    ")\n",
    "val_ds = CrowdDataset(\n",
    "    img_dir=TRAIN_IMG_DIR,\n",
    "    label_dir=TRAIN_LABEL_DIR,\n",
    "    base_size=BASE_SIZE,\n",
    "    down=DOWN,\n",
    "    aug=False,\n",
    "    mode=\"val\",\n",
    "    patch_size=0,\n",
    "    patches_per_image=1,\n",
    "    sigma_mode=SIGMA_MODE,\n",
    "    avoid_empty_patches=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Override image paths after shuffling\n",
    "train_ds.img_paths = train_imgs\n",
    "val_ds.img_paths = val_imgs\n",
    "\n",
    "# Recompute effective lengths for patch training\n",
    "if train_ds.mode == \"train\" and train_ds.patch_size > 0 and train_ds.patches_per_image > 1:\n",
    "    train_ds.effective_len = len(train_ds.img_paths) * train_ds.patches_per_image\n",
    "else:\n",
    "    train_ds.effective_len = len(train_ds.img_paths)\n",
    "val_ds.effective_len = len(val_ds.img_paths)\n",
    "\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size= BATCH,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8a0f9",
   "metadata": {},
   "source": [
    "# 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6419de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEncoder(nn.Module):\n",
    "    \"\"\"Simple spatial encoder that propagates information in four directions.\"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, k: int = 9) -> None:\n",
    "        super().__init__()\n",
    "        p = k // 2\n",
    "        self.h1 = nn.Conv2d(channels, channels, (1, k), padding=(0, p), groups=channels, bias=False)\n",
    "        self.h2 = nn.Conv2d(channels, channels, (1, k), padding=(0, p), groups=channels, bias=False)\n",
    "        self.v1 = nn.Conv2d(channels, channels, (k, 1), padding=(p, 0), groups=channels, bias=False)\n",
    "        self.v2 = nn.Conv2d(channels, channels, (k, 1), padding=(p, 0), groups=channels, bias=False)\n",
    "        self.proj = nn.Conv2d(channels * 4, channels, 1, bias=False)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = torch.cat([self.h1(x), self.h2(x), self.v1(x), self.v2(x)], dim=1)\n",
    "        return self.act(self.proj(y))\n",
    "\n",
    "\n",
    "class SFCN_VGG(nn.Module):\n",
    "    \"\"\"Simplified SFCN with VGG-16 backbone and spatial encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16_bn(\n",
    "            weights=models.VGG16_BN_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        )\n",
    "        # Use features up to conv4_3 (stride 8)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n",
    "        self.senc = SpatialEncoder(512, k=9)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.frontend(x)\n",
    "        x = self.senc(x)\n",
    "        x = self.head(x)\n",
    "        return torch.nn.functional.softplus(x)\n",
    "    \n",
    "\n",
    "# Model, optimizer, scaler, and scheduler\n",
    "model = SFCN_VGG(pretrained=True).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler() if (CUDA_AMP and device.type == \"cuda\") else None\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30790dc",
   "metadata": {},
   "source": [
    "# 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01069a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: torch.cuda.amp.GradScaler = None,\n",
    "    accum_steps: int = 1,\n",
    "    criterion: str = \"mse\",\n",
    "    count_loss_alpha: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"Train the model for one epoch and return the mean absolute error.\n",
    "\n",
    "    Supports optional gradient accumulation and auxiliary count loss. The\n",
    "    auxiliary loss encourages the sum of the predicted density map to match\n",
    "    the ground-truth count.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    if criterion == \"mse\":\n",
    "        crit = nn.MSELoss()\n",
    "    elif criterion == \"huber\":\n",
    "        crit = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        raise ValueError(\"criterion must be 'mse' or 'huber'\")\n",
    "    running_mae, nimg = 0.0, 0\n",
    "    total_pred_count, total_gt_count = 0.0, 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, (imgs, dens, _) in enumerate(tqdm(loader, desc=\"Train\", leave=False), 1):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        if scaler is not None:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                preds = model(imgs)\n",
    "                map_loss = crit(preds, dens)\n",
    "                # Count loss: MSE between predicted and ground-truth counts\n",
    "                if count_loss_alpha > 0.0:\n",
    "                    pred_cnt = preds.sum(dim=(1, 2, 3))\n",
    "                    gt_cnt = dens.sum(dim=(1, 2, 3))\n",
    "                    cnt_loss = F.mse_loss(pred_cnt, gt_cnt)\n",
    "                    total_loss = (map_loss + count_loss_alpha * cnt_loss) / accum_steps\n",
    "                else:\n",
    "                    total_loss = map_loss / accum_steps\n",
    "            scaler.scale(total_loss).backward()\n",
    "            if step % accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            preds = model(imgs)\n",
    "            map_loss = crit(preds, dens)\n",
    "            if count_loss_alpha > 0.0:\n",
    "                pred_cnt = preds.sum(dim=(1, 2, 3))\n",
    "                gt_cnt = dens.sum(dim=(1, 2, 3))\n",
    "                cnt_loss = F.mse_loss(pred_cnt, gt_cnt)\n",
    "                total_loss = (map_loss + count_loss_alpha * cnt_loss) / accum_steps\n",
    "            else:\n",
    "                total_loss = map_loss / accum_steps\n",
    "            total_loss.backward()\n",
    "            if step % accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.no_grad():\n",
    "            pc = preds.sum(dim=(1, 2, 3)).detach().cpu().numpy()\n",
    "            gc = dens.sum(dim=(1, 2, 3)).detach().cpu().numpy()\n",
    "            running_mae += np.abs(pc - gc).sum()\n",
    "            total_pred_count += pc.sum()\n",
    "            total_gt_count += gc.sum()\n",
    "            nimg += imgs.size(0)\n",
    "            # Warn if the difference in count is large for any sample\n",
    "            if step % 10 == 0:\n",
    "                for i in range(len(pc)):\n",
    "                    if abs(pc[i] - gc[i]) > 1.0:\n",
    "                        print(\n",
    "                            f\"[warn] batch {step} sample {i}: density sum {gc[i]:.1f} != pred {pc[i]:.1f}\"\n",
    "                        )\n",
    "    avg_pred = total_pred_count / max(1, nimg)\n",
    "    avg_gt = total_gt_count / max(1, nimg)\n",
    "    print(f\"Avg pred count: {avg_pred:.1f} vs GT {avg_gt:.1f}\")\n",
    "    return running_mae / max(1, nimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a28040",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate the model on a validation set and compute MAE and RMSE.\"\"\"\n",
    "    model.eval()\n",
    "    mae, mse, nimg = 0.0, 0.0, 0\n",
    "    for imgs, dens, _ in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        pred = model(imgs)\n",
    "        diff = (pred.sum(dim=(1, 2, 3)) - dens.sum(dim=(1, 2, 3))).detach().cpu().numpy()\n",
    "        mae += np.abs(diff).sum()\n",
    "        mse += (diff ** 2).sum()\n",
    "        nimg += imgs.size(0)\n",
    "    import math\n",
    "\n",
    "    return mae / max(1, nimg), math.sqrt(mse / max(1, nimg))\n",
    "\n",
    "\n",
    "best_mae = float(\"inf\")\n",
    "patience = EARLY_STOP_PATIENCE\n",
    "bad_epochs = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    tr_mae = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        device,\n",
    "        opt,\n",
    "        scaler,\n",
    "        accum_steps=ACCUM_STEPS,\n",
    "        criterion=.criterion,\n",
    "        count_loss_alpha=.count_loss_alpha,\n",
    "    )\n",
    "    va_mae, va_rmse = evaluate(model, val_loader, device)\n",
    "    sched.step()\n",
    "    print(\n",
    "        f\"Train MAE: {tr_mae:.3f} | Val MAE: {va_mae:.3f} | Val RMSE: {va_rmse:.3f}\"\n",
    "    )\n",
    "    if va_mae + 1e-6 < best_mae:\n",
    "        best_mae = va_mae\n",
    "        bad_epochs = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"val_mae\": va_mae,\n",
    "                \"\": vars(),\n",
    "            },\n",
    "            save,\n",
    "        )\n",
    "        print(f\"✅ New best: {save} (Val MAE={va_mae:.3f})\")\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        print(f\"No improvement for {bad_epochs} epoch(s).\")\n",
    "        if bad_epochs >= patience:\n",
    "            print(\n",
    "                f\"Early stopping at epoch {epoch}. Best Val MAE: {best_mae:.3f}\"\n",
    "            )\n",
    "            break\n",
    "print(\"Training finished. Best Val MAE:\", best_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad10f5e",
   "metadata": {},
   "source": [
    "# 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd510ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f4b32ff",
   "metadata": {},
   "source": [
    "# 7. Test Set Prediction and Submit Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d944b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset1(Dataset):\n",
    "    \"\"\"Dataset for test images based on provided sample CSV order.\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_dir: str, \n",
    "        sample_csv: str, \n",
    "        transform=None\n",
    "    ):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load image_ids from sample CSV\n",
    "        with open(sample_csv, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader)\n",
    "            if header != [\"image_id\", \"predicted_count\"]:\n",
    "                raise ValueError(\n",
    "                    f\"Sample CSV header expected ['image_id','predicted_count'], got {header}\"\n",
    "                )\n",
    "            self.image_ids = [row[0] for row in reader]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_id + \".jpg\")\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            # If image not found, return None signal\n",
    "            return None, img_id\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        return image, img_id\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\"Dataset for test images.\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_dir: str, \n",
    "        transform=None\n",
    "    ):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')],\n",
    "                                 key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, img_name\n",
    "\n",
    "# --- Prediction Function ---\n",
    "def generate_sfcn_predictions(\n",
    "    model,\n",
    "    test_dir: str,\n",
    "    output_file: str = \"sfcn_submission.csv\",\n",
    "    batch_size: int = 16,\n",
    "    transform=None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate test predictions using the SFCN model.\"\"\"\n",
    "\n",
    "    print(\"Generating test predictions with SFCN model...\")\n",
    "\n",
    "    # Dataset + DataLoader\n",
    "    test_dataset = TestDataset(test_dir, transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_names = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, names in test_loader:\n",
    "            images = images.to(device)\n",
    "            pred_density, pred_count = model(images)\n",
    "\n",
    "            # Use count predictions for submission\n",
    "            batch_preds = pred_count.cpu().numpy()\n",
    "            predictions.extend([max(0, int(round(pred))) for pred in batch_preds])\n",
    "            image_names.extend(names)\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'image_id': image_names,\n",
    "        'predicted_count': predictions\n",
    "    })\n",
    "\n",
    "    # Sort by image_id\n",
    "    submission_df['sort_key'] = submission_df['image_id'].apply(lambda x: int(os.path.splitext(x)[0]))\n",
    "    submission_df = submission_df.sort_values('sort_key').drop('sort_key', axis=1).reset_index(drop=True)\n",
    "\n",
    "    # Save submission\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Submission saved to {output_file}\")\n",
    "    print(f\"Predictions for {len(submission_df)} test images\")\n",
    "\n",
    "    # Statistics\n",
    "    pred_counts = submission_df['predicted_count'].values\n",
    "    print(f\"\\nTest Predictions Statistics:\")\n",
    "    print(f\"Min: {pred_counts.min()}\")\n",
    "    print(f\"Max: {pred_counts.max()}\")\n",
    "    print(f\"Mean: {pred_counts.mean():.2f}\")\n",
    "    print(f\"Median: {np.median(pred_counts):.2f}\")\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "# Run predictions\n",
    "submission_df = generate_sfcn_predictions(\n",
    "    model,\n",
    "    test_dir=TEST_IMG_DIR,\n",
    "    batch_size=16,\n",
    "    # transform=test_transforms\n",
    ")\n",
    "submission_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
