{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390f2cfc",
   "metadata": {},
   "source": [
    "# Spatial Fully Convolutional Network for Crowd Counting\n",
    "\n",
    "This notebook implements SFCN with VGG16-BN backbone for crowd counting tasks.\n",
    "\n",
    "**Features:**\n",
    "- VGG16-BN backbone with spatial encoder\n",
    "- Adaptive density map generation with KNN-based sigma\n",
    "- Advanced data augmentation with patch-based training\n",
    "- Mixed precision training with gradient accumulation\n",
    "- Early stopping and model checkpointing\n",
    "- Optional empty patch avoidance for better training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a323fb",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdbe9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "from glob import glob\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Constants\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "\n",
    "\n",
    "# Detect environment\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running in Colab, Kaggle, or local environment\"\"\"\n",
    "    if 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    elif 'kaggle_secrets' in sys.modules or os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n",
    "        return 'kaggle'\n",
    "    else:\n",
    "        return 'local'\n",
    "\n",
    "ENV = detect_environment()\n",
    "print(f\"🔍 Detected environment: {ENV.upper()}\")\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c3af0",
   "metadata": {},
   "source": [
    "# Dataset Download and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc608aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment-specific dataset setup\n",
    "def setup_dataset_paths(env: str) -> dict[str, str]:\n",
    "    \"\"\"Setup dataset paths based on environment\"\"\"\n",
    "\n",
    "    if env == 'colab':\n",
    "        # Google Colab paths\n",
    "        dataset_name = \"penyisihan-hology-8-0-2025-data-mining\"\n",
    "        drive_path = \"/content/drive/MyDrive/PROJECTS/Cognivio/dataset\"\n",
    "        local_path = \"/content/dataset\"\n",
    "\n",
    "        # Download and setup dataset in Colab\n",
    "        if not os.path.exists(local_path):\n",
    "            print(\"📥 Setting up dataset in Colab...\")\n",
    "\n",
    "            # Check for kaggle.json in Drive first\n",
    "            kaggle_json_drive = \"/content/drive/MyDrive/kaggle.json\"\n",
    "            kaggle_json_local = \"/root/.kaggle/kaggle.json\"\n",
    "\n",
    "            if os.path.exists(kaggle_json_drive):\n",
    "                print('copy from drive')\n",
    "                # Copy from Drive\n",
    "                !mkdir -p /root/.kaggle\n",
    "                !cp \"{kaggle_json_drive}\" \"{kaggle_json_local}\"\n",
    "                !chmod 600 \"{kaggle_json_local}\"\n",
    "                print(\"✅ Kaggle credentials loaded from Drive\")\n",
    "            else:\n",
    "                print(\"⚠️  Please upload kaggle.json to your Google Drive root folder\")\n",
    "                print(\"   Or manually upload it when prompted below\")\n",
    "                from google.colab import files\n",
    "                uploaded = files.upload()\n",
    "                for fn in uploaded.keys():\n",
    "                    !mkdir -p /root/.kaggle\n",
    "                    !mv \"{fn}\" \"/root/.kaggle/kaggle.json\"\n",
    "                    !chmod 600 \"/root/.kaggle/kaggle.json\"\n",
    "                    print(f\"✅ Kaggle credentials uploaded: {fn}\")\n",
    "\n",
    "            # Download dataset\n",
    "            !kaggle competitions download -c {dataset_name} -p /content\n",
    "\n",
    "            # Extract dataset\n",
    "            import zipfile\n",
    "            zip_path = f\"/content/{dataset_name}.zip\"\n",
    "            if os.path.exists(zip_path):\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(local_path)\n",
    "                print(f\"✅ Dataset extracted to {local_path}\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Dataset zip not found: {zip_path}\")\n",
    "\n",
    "        return {\n",
    "            'img_dir': f\"{local_path}/train/images\",\n",
    "            'label_dir': f\"{local_path}/train/labels\",\n",
    "            'test_dir': f\"{local_path}/test/images\",\n",
    "            'save_dir': \"/content/drive/MyDrive/PROJECTS/Cognivio/models\"\n",
    "        }\n",
    "\n",
    "    elif env == 'kaggle':\n",
    "        # Kaggle paths\n",
    "        return {\n",
    "            'img_dir': \"/kaggle/input/penyisihan-hology-8-0-2025-data-mining/train/images\",\n",
    "            'label_dir': \"/kaggle/input/penyisihan-hology-8-0-2025-data-mining/train/labels\",\n",
    "            'test_dir': \"/kaggle/input/penyisihan-hology-8-0-2025-data-mining/test/images\",\n",
    "            'save_dir': \"/kaggle/working\"\n",
    "        }\n",
    "\n",
    "    else:  # local\n",
    "        # Local paths - modify these for your setup\n",
    "        base_path = \"data\"  # Change this to your local dataset path\n",
    "        return {\n",
    "            'img_dir': f\"{base_path}/train/images\",\n",
    "            'label_dir': f\"{base_path}/train/labels\",\n",
    "            'test_dir': f\"{base_path}/test/images\",\n",
    "            'save_dir': \"models\"\n",
    "        }\n",
    "\n",
    "# Setup paths\n",
    "paths = setup_dataset_paths(ENV)\n",
    "print(f\"📁 Dataset paths configured for {ENV}:\")\n",
    "for key, path in paths.items():\n",
    "    exists = \"✅\" if os.path.exists(path) else \"⚠️\"\n",
    "    print(f\"   {key}: {path} {exists}\")\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(paths['save_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc7e279",
   "metadata": {},
   "source": [
    "# Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data paths (modify these for local setup)\n",
    "    'img_dir': paths['img_dir'],\n",
    "    'label_dir': paths['label_dir'],\n",
    "    'test_dir': paths['test_dir'],\n",
    "    'save_dir': paths['save_dir'],\n",
    "\n",
    "    # Model parameters\n",
    "    'base_size': 1024,\n",
    "    'down': 8,\n",
    "    'patch_size': 384,\n",
    "    'patches_per_image': 4,\n",
    "    'avoid_empty_patches': True,\n",
    "\n",
    "    # Training parameters\n",
    "    'batch_size': 12, # (default=12, testing=4) change to higher in high computational environment\n",
    "    'epochs': 120,  # (default=120, testing=2) Increase for full training\n",
    "    'lr': 2e-4, # or 1e-4\n",
    "    'criterion': 'mse',  # 'mse' or 'huber'\n",
    "    'count_loss_alpha': 0.0,  # Auxiliary count loss weight\n",
    "    'early_stop_patience': 15,\n",
    "\n",
    "    # Optimization\n",
    "    'num_workers': 4, # change to higher in high computational environment\n",
    "    'amp': True,  # Automatic Mixed Precision\n",
    "    'accum_steps': 2,  # Gradient accumulation steps\n",
    "\n",
    "    # Augmentation and preprocessing\n",
    "    'sigma_mode': 'adaptive',  # 'adaptive' or 'constant'\n",
    "\n",
    "    # Model saving\n",
    "    'save_path': os.path.join(paths['save_dir'], 'sfcn_best.pth'),\n",
    "    'seed': 1337\n",
    "}\n",
    "\n",
    "print(\"📋 Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed: int = 1337) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config['seed'])\n",
    "print(f\"🎲 Random seed set to {config['seed']}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔧 Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449169e",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(\n",
    "    seed: int = 1337\n",
    ") -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def imread_rgb(\n",
    "    path: str\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Read an RGB image using OpenCV and convert BGR→RGB.\"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read image: {path}\")\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def letterbox(\n",
    "    img: np.ndarray,\n",
    "    target: int = 512\n",
    ") -> Tuple[np.ndarray, float, int, int]:\n",
    "    \"\"\"Resize and pad an image to a square canvas without distortion.\n",
    "\n",
    "    Returns the padded image, the scale factor used, and the left/top\n",
    "    padding applied. The output size is (target, target).\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    scale = min(target / h, target / w)\n",
    "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "    img_rs = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "    top = (target - nh) // 2\n",
    "    left = (target - nw) // 2\n",
    "    canvas = np.zeros((target, target, 3), dtype=img_rs.dtype)\n",
    "    canvas[top:top + nh, left:left + nw] = img_rs\n",
    "    return canvas, scale, left, top\n",
    "\n",
    "def parse_points_from_json(\n",
    "    path: str\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Parse annotated points from a JSON file.\n",
    "\n",
    "    Supports several common crowd counting annotation formats. Returns an\n",
    "    array of shape (N, 2) containing [x, y] coordinates and, if present\n",
    "    in the JSON, the declared number of people.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    pts: List[List[float]]\n",
    "    num = None\n",
    "    if isinstance(obj, dict) and \"points\" in obj:\n",
    "        pts = obj[\"points\"]\n",
    "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
    "        if len(pts) > 0 and isinstance(pts[0], dict):\n",
    "            pts = [[p[\"x\"], p[\"y\"]] for p in pts if \"x\" in p and \"y\" in p]\n",
    "    elif isinstance(obj, dict) and \"annotations\" in obj:\n",
    "        pts = [[a[\"x\"], a[\"y\"]] for a in obj[\"annotations\"] if \"x\" in a and \"y\" in a]\n",
    "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
    "    elif isinstance(obj, list):\n",
    "        pts = obj\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown JSON schema: {path}\")\n",
    "    pts_arr = np.array(pts, dtype=np.float32) if len(pts) > 0 else np.zeros((0, 2), np.float32)\n",
    "    return pts_arr, num\n",
    "\n",
    "def derive_json_path(\n",
    "    lbl_dir: str,\n",
    "    img_path: str\n",
    ") -> str:\n",
    "    \"\"\"Derive the corresponding JSON label path for a given image path.\"\"\"\n",
    "    name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    cand = os.path.join(lbl_dir, name + \".json\")\n",
    "    if os.path.exists(cand):\n",
    "        return cand\n",
    "    # Try matching trailing digits\n",
    "    m = re.findall(r\"\\d+\", name)\n",
    "    if m:\n",
    "        alt = os.path.join(lbl_dir, f\"{m[-1]}.json\")\n",
    "        if os.path.exists(alt):\n",
    "            return alt\n",
    "    # Fallback: any file starting with the same name\n",
    "    lst = glob(os.path.join(lbl_dir, f\"{name}*.json\"))\n",
    "    if lst:\n",
    "        return lst[0]\n",
    "    raise FileNotFoundError(f\"JSON label not found for {img_path}\")\n",
    "\n",
    "def make_density_map(\n",
    "    points_xy: np.ndarray,\n",
    "    grid_size: int,\n",
    "    down: int = 8,\n",
    "    sigma_mode: str = \"adaptive\",\n",
    "    knn: int = 3,\n",
    "    beta: float = 0.3,\n",
    "    const_sigma: float = 2.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate a density map on a grid given annotated points.\n",
    "\n",
    "    The density map is of shape (grid_size//down, grid_size//down). Each\n",
    "    point is represented by a Gaussian whose sigma is either constant or\n",
    "    computed from the k-nearest neighbours.\n",
    "    \"\"\"\n",
    "    target = grid_size\n",
    "    dh, dw = target // down, target // down\n",
    "    den = np.zeros((dh, dw), dtype=np.float32)\n",
    "\n",
    "    if len(points_xy) == 0:\n",
    "        return den\n",
    "\n",
    "    # Scale points to the density map resolution\n",
    "    pts = points_xy.copy()\n",
    "    pts[:, 0] = pts[:, 0] * (dw / target)\n",
    "    pts[:, 1] = pts[:, 1] * (dh / target)\n",
    "    tree = KDTree(pts) if len(pts) > 1 else None\n",
    "\n",
    "    for (x, y) in pts:\n",
    "        # Determine sigma\n",
    "        if sigma_mode == \"adaptive\" and tree is not None and len(pts) > 3:\n",
    "            dists, _ = tree.query([x, y], k=min(knn + 1, len(pts)))\n",
    "            sigma = max(1.0, float(np.mean(dists[1:])) * beta)\n",
    "        else:\n",
    "            sigma = const_sigma\n",
    "\n",
    "        cx, cy = float(x), float(y)\n",
    "        rad = int(max(1, math.ceil(3 * sigma)))\n",
    "        x0, x1 = max(0, int(math.floor(cx - rad))), min(dw, int(math.ceil(cx + rad + 1)))\n",
    "        y0, y1 = max(0, int(math.floor(cy - rad))), min(dh, int(math.ceil(cy + rad + 1)))\n",
    "\n",
    "        if x1 <= x0 or y1 <= y0:\n",
    "            continue\n",
    "\n",
    "        xs = np.arange(x0, x1) - cx\n",
    "        ys = np.arange(y0, y1) - cy\n",
    "        xx, yy = np.meshgrid(xs, ys)\n",
    "        g = np.exp(-(xx**2 + yy**2) / (2 * sigma * sigma))\n",
    "        s = g.sum()\n",
    "\n",
    "        if s > 0:\n",
    "            den[y0:y1, x0:x1] += (g / s).astype(np.float32)\n",
    "\n",
    "    return den\n",
    "\n",
    "print(\"✅ Basic utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed13f38",
   "metadata": {},
   "source": [
    "# Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c87e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdDataset(Dataset):\n",
    "    \"\"\"Custom dataset for crowd counting.\n",
    "\n",
    "    Supports optional random patch cropping on training data. When\n",
    "    `avoid_empty_patches` is set, the dataset will attempt to choose a\n",
    "    patch containing at least one point, falling back to a random crop if\n",
    "    it fails after a number of retries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir: str,\n",
    "        lbl_dir: str,\n",
    "        base_size: int = 768,\n",
    "        down: int = 8,\n",
    "        aug: bool = True,\n",
    "        mode: str = \"train\",\n",
    "        patch_size: int = 0,\n",
    "        patches_per_image: int = 1,\n",
    "        sigma_mode: str = \"adaptive\",\n",
    "        avoid_empty_patches: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_paths = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "        if len(self.img_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {img_dir}\")\n",
    "        self.lbl_dir = lbl_dir\n",
    "        self.base_size = base_size\n",
    "        self.down = down\n",
    "        self.aug = aug\n",
    "        self.mode = mode\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_per_image = max(1, int(patches_per_image))\n",
    "        self.sigma_mode = sigma_mode\n",
    "        self.avoid_empty_patches = avoid_empty_patches\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        self.color_jit = transforms.ColorJitter(0.1, 0.1, 0.1, 0.05)\n",
    "\n",
    "        # Compute effective length for patch training\n",
    "        if self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1:\n",
    "            self.effective_len = len(self.img_paths) * self.patches_per_image\n",
    "        else:\n",
    "            self.effective_len = len(self.img_paths)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.effective_len\n",
    "\n",
    "    def _load_img_pts(\n",
    "        self,\n",
    "        idx_base: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load an image and its transformed annotation points.\"\"\"\n",
    "        pimg = self.img_paths[idx_base]\n",
    "        img = imread_rgb(pimg)\n",
    "        h, w = img.shape[:2]\n",
    "        plbl = derive_json_path(self.lbl_dir, pimg)\n",
    "        pts, _ = parse_points_from_json(plbl)\n",
    "\n",
    "        # Augment: horizontal flip\n",
    "        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n",
    "            img = img[:, ::-1, :].copy()\n",
    "            if len(pts) > 0:\n",
    "                pts = pts.copy()\n",
    "                pts[:, 0] = (w - 1) - pts[:, 0]\n",
    "\n",
    "        # Augment: colour jitter\n",
    "        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n",
    "            pil = transforms.ToPILImage()(img)\n",
    "            pil = self.color_jit(pil)\n",
    "            img = np.array(pil)\n",
    "\n",
    "        # Letterbox to base size\n",
    "        canvas, scale, left, top = letterbox(img, target=self.base_size)\n",
    "        if len(pts) > 0:\n",
    "            pts_tr = pts.copy()\n",
    "            pts_tr[:, 0] = pts_tr[:, 0] * scale + left\n",
    "            pts_tr[:, 1] = pts_tr[:, 1] * scale + top\n",
    "            # Clamp to canvas bounds\n",
    "            m = (\n",
    "                (pts_tr[:, 0] >= 0)\n",
    "                & (pts_tr[:, 0] < self.base_size)\n",
    "                & (pts_tr[:, 1] >= 0)\n",
    "                & (pts_tr[:, 1] < self.base_size)\n",
    "            )\n",
    "            pts_tr = pts_tr[m]\n",
    "        else:\n",
    "            pts_tr = np.zeros((0, 2), np.float32)\n",
    "        return canvas, pts_tr\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Map global index to an image index (for patch training)\n",
    "        if self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1:\n",
    "            idx_base = index // self.patches_per_image\n",
    "        else:\n",
    "            idx_base = index\n",
    "        idx_base %= len(self.img_paths)\n",
    "\n",
    "        img_lb, pts_tr = self._load_img_pts(idx_base)\n",
    "\n",
    "        # Optional patch cropping\n",
    "        if self.mode == \"train\" and self.patch_size > 0:\n",
    "            ps = self.patch_size\n",
    "            if ps > self.base_size:\n",
    "                raise ValueError(\"patch_size must be <= base_size\")\n",
    "\n",
    "            max_off = self.base_size - ps\n",
    "            pts_out = np.zeros((0, 2), np.float32)\n",
    "\n",
    "            # Attempt to find a non-empty patch when requested\n",
    "            for attempt in range(10) if self.avoid_empty_patches else [0]:\n",
    "                ox = 0 if max_off <= 0 else random.randint(0, max_off)\n",
    "                oy = 0 if max_off <= 0 else random.randint(0, max_off)\n",
    "                crop = img_lb[oy : oy + ps, ox : ox + ps, :]\n",
    "                if len(pts_tr) > 0:\n",
    "                    pts_c = pts_tr.copy()\n",
    "                    pts_c[:, 0] -= ox\n",
    "                    pts_c[:, 1] -= oy\n",
    "                    m = (\n",
    "                        (pts_c[:, 0] >= 0)\n",
    "                        & (pts_c[:, 0] < ps)\n",
    "                        & (pts_c[:, 1] >= 0)\n",
    "                        & (pts_c[:, 1] < ps)\n",
    "                    )\n",
    "                    pts_c = pts_c[m]\n",
    "                else:\n",
    "                    pts_c = np.zeros((0, 2), np.float32)\n",
    "                # If avoid_empty_patches is False, we break immediately (no retries)\n",
    "                if not self.avoid_empty_patches or len(pts_c) > 0 or attempt == 9:\n",
    "                    pts_out = pts_c\n",
    "                    img_out = crop\n",
    "                    break\n",
    "            grid = ps\n",
    "        else:\n",
    "            img_out = img_lb\n",
    "            pts_out = pts_tr\n",
    "            grid = self.base_size\n",
    "\n",
    "        # Build density map\n",
    "        den = make_density_map(\n",
    "            pts_out,\n",
    "            grid_size=grid,\n",
    "            down=self.down,\n",
    "            sigma_mode=self.sigma_mode,\n",
    "        )\n",
    "\n",
    "        # Convert to tensors and normalise\n",
    "        t = self.to_tensor(img_out)\n",
    "        t = self.normalize(t)\n",
    "        d = torch.from_numpy(den).unsqueeze(0)\n",
    "        c = torch.tensor([float(len(pts_out))], dtype=torch.float32)\n",
    "        return t, d, c\n",
    "\n",
    "print(\"✅ CrowdDataset class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b01a97",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEncoder(nn.Module):\n",
    "    \"\"\"Simple spatial encoder that propagates information in four directions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        k: int = 9\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        p = k // 2\n",
    "        self.h1 = nn.Conv2d(\n",
    "            in_channels=channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=(1, k),\n",
    "            padding=(0, p),\n",
    "            groups=channels,\n",
    "            bias=False\n",
    "        )\n",
    "        self.h2 = nn.Conv2d(\n",
    "            in_channels=channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=(1, k),\n",
    "            padding=(0, p),\n",
    "            groups=channels,\n",
    "            bias=False\n",
    "        )\n",
    "        self.v1 = nn.Conv2d(\n",
    "            in_channels=channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=(k, 1),\n",
    "            padding=(p, 0),\n",
    "            groups=channels,\n",
    "            bias=False\n",
    "        )\n",
    "        self.v2 = nn.Conv2d(\n",
    "            in_channels=channels,\n",
    "            out_channels=channels,\n",
    "            kernel_size=(k, 1),\n",
    "            padding=(p, 0),\n",
    "            groups=channels,\n",
    "            bias=False\n",
    "        )\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=channels * 4,\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = torch.cat([self.h1(x), self.h2(x), self.v1(x), self.v2(x)], dim=1)\n",
    "        return self.act(self.proj(y))\n",
    "\n",
    "\n",
    "class SFCN_VGG(nn.Module):\n",
    "    \"\"\"Simplified SFCN with VGG-16 backbone and spatial encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16_bn(\n",
    "            weights=models.VGG16_BN_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        )\n",
    "        # Use features up to conv4_3 (stride 8)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n",
    "        self.senc = SpatialEncoder(512, k=9)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.frontend(x)\n",
    "        x = self.senc(x)\n",
    "        x = self.head(x)\n",
    "        return torch.nn.functional.softplus(x)\n",
    "\n",
    "print(\"✅ SFCN model architecture implemented!\")\n",
    "\n",
    "# Test model instantiation\n",
    "model_test = SFCN_VGG(pretrained=False)\n",
    "print(f\"📊 Model parameters: {sum(p.numel() for p in model_test.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 3, 384, 384)\n",
    "with torch.no_grad():\n",
    "    output = model_test(dummy_input)\n",
    "    print(f\"🔍 Input shape: {dummy_input.shape}\")\n",
    "    print(f\"🎯 Output shape: {output.shape}\")\n",
    "\n",
    "del model_test, dummy_input, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d333605",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d4797",
   "metadata": {},
   "source": [
    "## Training Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: torch.cuda.amp.GradScaler = None,\n",
    "    accum_steps: int = 1,\n",
    "    criterion: str = \"mse\",\n",
    "    count_loss_alpha: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"Train the model for one epoch and return the mean absolute error.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    if criterion == \"mse\":\n",
    "        crit = nn.MSELoss()\n",
    "    elif criterion == \"huber\":\n",
    "        crit = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        raise ValueError(\"criterion must be 'mse' or 'huber'\")\n",
    "\n",
    "    running_mae, nimg = 0.0, 0\n",
    "    total_pred_count, total_gt_count = 0.0, 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (imgs, dens, _) in enumerate(tqdm(loader, desc=\"Train\", leave=False), 1):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        if scaler is not None:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                preds = model(imgs)\n",
    "                map_loss = crit(preds, dens)\n",
    "                if count_loss_alpha > 0.0:\n",
    "                    pred_cnt = preds.sum(dim=(1, 2, 3))\n",
    "                    gt_cnt = dens.sum(dim=(1, 2, 3))\n",
    "                    cnt_loss = F.mse_loss(pred_cnt, gt_cnt)\n",
    "                    total_loss = (map_loss + count_loss_alpha * cnt_loss) / accum_steps\n",
    "                else:\n",
    "                    total_loss = map_loss / accum_steps\n",
    "\n",
    "            scaler.scale(total_loss).backward()\n",
    "            if step % accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            preds = model(imgs)\n",
    "            map_loss = crit(preds, dens)\n",
    "            if count_loss_alpha > 0.0:\n",
    "                pred_cnt = preds.sum(dim=(1, 2, 3))\n",
    "                gt_cnt = dens.sum(dim=(1, 2, 3))\n",
    "                cnt_loss = F.mse_loss(pred_cnt, gt_cnt)\n",
    "                total_loss = (map_loss + count_loss_alpha * cnt_loss) / accum_steps\n",
    "            else:\n",
    "                total_loss = map_loss / accum_steps\n",
    "\n",
    "            total_loss.backward()\n",
    "\n",
    "            if step % accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pc = preds.sum(dim=(1, 2, 3)).detach().cpu().numpy()\n",
    "            gc = dens.sum(dim=(1, 2, 3)).detach().cpu().numpy()\n",
    "            running_mae += np.abs(pc - gc).sum()\n",
    "            total_pred_count += pc.sum()\n",
    "            total_gt_count += gc.sum()\n",
    "            nimg += imgs.size(0)\n",
    "\n",
    "    avg_pred = total_pred_count / max(1, nimg)\n",
    "    avg_gt = total_gt_count / max(1, nimg)\n",
    "    print(f\"Avg pred count: {avg_pred:.1f} vs GT {avg_gt:.1f}\")\n",
    "    return running_mae / max(1, nimg)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate the model on a validation set and compute MAE and RMSE.\"\"\"\n",
    "    model.eval()\n",
    "    mae, mse, nimg = 0.0, 0.0, 0\n",
    "\n",
    "    for imgs, dens, _ in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        pred = model(imgs)\n",
    "        diff = (pred.sum(dim=(1, 2, 3)) - dens.sum(dim=(1, 2, 3))).detach().cpu().numpy()\n",
    "        mae += np.abs(diff).sum()\n",
    "        mse += (diff ** 2).sum()\n",
    "        nimg += imgs.size(0)\n",
    "    return mae / max(1, nimg), math.sqrt(mse / max(1, nimg))\n",
    "\n",
    "print(\"✅ Training and evaluation functions implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d33e4",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34701acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data directories exist\n",
    "img_dir = config['img_dir']\n",
    "label_dir = config['label_dir']\n",
    "\n",
    "print(f\"🔍 Checking data directories...\")\n",
    "print(f\"   Images: {img_dir} -> {'✅ Exists' if os.path.exists(img_dir) else '❌ Not found'}\")\n",
    "print(f\"   Labels: {label_dir} -> {'✅ Exists' if os.path.exists(label_dir) else '❌ Not found'}\")\n",
    "\n",
    "if not os.path.exists(img_dir):\n",
    "    raise Exception(\"❌ Image directory not found. Please check the path and try again.\")\n",
    "\n",
    "# Build list of images and randomly shuffle before splitting\n",
    "all_imgs = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "random.shuffle(all_imgs)\n",
    "n_images = len(all_imgs)\n",
    "\n",
    "if n_images < 2:\n",
    "    raise ValueError(\"Need at least 2 images for training and validation\")\n",
    "\n",
    "n_val = max(1, int(0.1 * n_images))  # 10% for validation\n",
    "n_train = n_images - n_val\n",
    "train_imgs = all_imgs[:n_train]\n",
    "val_imgs = all_imgs[n_train:]\n",
    "\n",
    "print(f\"📊 Data split: {n_train} train, {n_val} validation\")\n",
    "\n",
    "# Instantiate datasets\n",
    "train_ds = CrowdDataset(\n",
    "    config['img_dir'],\n",
    "    config['label_dir'],\n",
    "    base_size=config['base_size'],\n",
    "    down=config['down'],\n",
    "    aug=True,\n",
    "    mode=\"train\",\n",
    "    patch_size=config['patch_size'],\n",
    "    patches_per_image=config['patches_per_image'],\n",
    "    sigma_mode=config['sigma_mode'],\n",
    "    avoid_empty_patches=config['avoid_empty_patches'],\n",
    ")\n",
    "val_ds = CrowdDataset(\n",
    "    config['img_dir'],\n",
    "    config['label_dir'],\n",
    "    base_size=config['base_size'],\n",
    "    down=config['down'],\n",
    "    aug=False,\n",
    "    mode=\"val\",\n",
    "    patch_size=0,\n",
    "    patches_per_image=1,\n",
    "    sigma_mode=config['sigma_mode'],\n",
    "    avoid_empty_patches=False,\n",
    ")\n",
    "\n",
    "# Override image paths after shuffling\n",
    "train_ds.img_paths = train_imgs\n",
    "val_ds.img_paths = val_imgs\n",
    "\n",
    "# Recompute effective lengths for patch training\n",
    "if train_ds.mode == \"train\" and train_ds.patch_size > 0 and train_ds.patches_per_image > 1:\n",
    "    train_ds.effective_len = len(train_ds.img_paths) * train_ds.patches_per_image\n",
    "else:\n",
    "    train_ds.effective_len = len(train_ds.img_paths)\n",
    "\n",
    "val_ds.effective_len = len(val_ds.img_paths)\n",
    "\n",
    "print(f\"📦 Training dataset: {len(train_ds)} samples\")\n",
    "print(f\"📦 Validation dataset: {len(val_ds)} samples\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"🚀 Data loaders created!\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152244f4",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and training components\n",
    "print(\"🏗️  Initializing model and training components...\")\n",
    "\n",
    "# Create model\n",
    "model = SFCN_VGG(pretrained=True).to(device)\n",
    "print(f\"📊 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['lr'],\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['epochs']\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if (config['amp'] and device.type == \"cuda\") else None\n",
    "print(f\"⚡ Mixed precision: {'Enabled' if scaler is not None else 'Disabled'}\")\n",
    "\n",
    "# Training state\n",
    "best_mae = float(\"inf\")\n",
    "patience = config['early_stop_patience']\n",
    "bad_epochs = 0\n",
    "training_history = {\n",
    "    'train_mae': [],\n",
    "    'val_mae': [],\n",
    "    'val_rmse': [],\n",
    "    'epoch': []\n",
    "}\n",
    "\n",
    "print(\"✅ Setup complete! Starting training...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55052e6c",
   "metadata": {},
   "source": [
    "# Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e33735",
   "metadata": {},
   "source": [
    "## Summarize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if not len(training_history['epoch']):\n",
    "    raise Exception(\"⚠️  No training history available. Please run the training loop first.\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training and validation MAE\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(training_history['epoch'], training_history['train_mae'], 'b-', label='Train MAE', linewidth=2)\n",
    "plt.plot(training_history['epoch'], training_history['val_mae'], 'r-', label='Val MAE', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation RMSE\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(training_history['epoch'], training_history['val_rmse'], 'g-', label='Val RMSE', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Validation RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning curve analysis\n",
    "plt.subplot(1, 3, 3)\n",
    "train_mae = np.array(training_history['train_mae'])\n",
    "val_mae = np.array(training_history['val_mae'])\n",
    "gap = val_mae - train_mae\n",
    "plt.plot(training_history['epoch'], gap, 'purple', label='Val-Train Gap', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE Gap')\n",
    "plt.title('Generalization Gap')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"📈 Training Statistics:\")\n",
    "print(f\"   Final Train MAE: {training_history['train_mae'][-1]:.3f}\")\n",
    "print(f\"   Final Val MAE: {training_history['val_mae'][-1]:.3f}\")\n",
    "print(f\"   Final Val RMSE: {training_history['val_rmse'][-1]:.3f}\")\n",
    "print(f\"   Best Val MAE: {min(training_history['val_mae']):.3f}\")\n",
    "print(f\"   Total Epochs: {len(training_history['epoch'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e92c7",
   "metadata": {},
   "source": [
    "## Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final evaluation\n",
    "if os.path.exists(config['save_path']):\n",
    "    print(\"📁 Loading best model checkpoint...\")\n",
    "    checkpoint = torch.load(config['save_path'], map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(f\"✅ Loaded model from epoch {checkpoint['epoch']} with MAE {checkpoint['val_mae']:.3f}\")\n",
    "\n",
    "# Final evaluation with detailed predictions\n",
    "print(\"\\n🎯 Final Evaluation - Ground Truth vs Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "image_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs, dens, _) in enumerate(val_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        pred = model(imgs)\n",
    "\n",
    "        pred_cnt_batch = pred.sum((1, 2, 3))\n",
    "        gt_cnt_batch = dens.sum((1, 2, 3))\n",
    "\n",
    "        # Store results\n",
    "        for j in range(pred_cnt_batch.size(0)):\n",
    "            idx = batch_idx * config['batch_size'] + j\n",
    "            if idx < len(val_ds.img_paths):\n",
    "                image_name = os.path.basename(val_ds.img_paths[idx])\n",
    "                image_names.append(image_name)\n",
    "                predictions.append(pred_cnt_batch[j].item())\n",
    "                ground_truths.append(gt_cnt_batch[j].item())\n",
    "\n",
    "                print(f\"{image_name:20s}: GT {gt_cnt_batch[j].item():6.1f}, Pred {pred_cnt_batch[j].item():6.1f}, \"\n",
    "                      f\"Error {abs(gt_cnt_batch[j].item() - pred_cnt_batch[j].item()):5.1f}\")\n",
    "\n",
    "# Calculate final metrics\n",
    "if len(predictions) <= 0:\n",
    "    raise Exception(\"⚠️  No validation data processed.\")\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "ground_truths = np.array(ground_truths)\n",
    "\n",
    "mae = np.mean(np.abs(predictions - ground_truths))\n",
    "mse = np.mean((predictions - ground_truths)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"\\n📊 Final Metrics:\")\n",
    "print(f\"   MAE:  {mae:.3f}\")\n",
    "print(f\"   MSE:  {mse:.3f}\")\n",
    "print(f\"   RMSE: {rmse:.3f}\")\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(ground_truths, predictions, alpha=0.6, s=50)\n",
    "\n",
    "# Perfect prediction line\n",
    "max_val = max(np.max(ground_truths), np.max(predictions))\n",
    "min_val = min(np.min(ground_truths), np.min(predictions))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "\n",
    "plt.xlabel('Ground Truth Count')\n",
    "plt.ylabel('Predicted Count')\n",
    "plt.title(f'Ground Truth vs Predictions (MAE: {mae:.2f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "correlation = np.corrcoef(ground_truths, predictions)[0, 1]\n",
    "plt.text(\n",
    "    0.05,\n",
    "    0.95,\n",
    "    f'Correlation: {correlation:.3f}',\n",
    "    transform=plt.gca().transAxes,\n",
    "    bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5)\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🔗 Correlation coefficient: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525b741",
   "metadata": {},
   "source": [
    "## Density Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions with density maps\n",
    "def visualize_predictions(\n",
    "    model,\n",
    "    dataset,\n",
    "    device,\n",
    "    num_samples=3\n",
    ") -> None:\n",
    "    \"\"\"Visualize model predictions with density maps.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            if i >= len(dataset):\n",
    "                break\n",
    "\n",
    "            # Get sample\n",
    "            img_tensor, gt_density, gt_count = dataset[i]\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            # Model prediction\n",
    "            pred_density = model(img_tensor)\n",
    "\n",
    "            # Convert to numpy\n",
    "            img_np = img_tensor.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "            img_np = img_np * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)  # Denormalize\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "            gt_density_np = gt_density.squeeze().numpy()\n",
    "            pred_density_np = pred_density.squeeze().cpu().numpy()\n",
    "\n",
    "            # Calculate counts\n",
    "            gt_count_val = gt_density_np.sum()\n",
    "            pred_count_val = pred_density_np.sum()\n",
    "\n",
    "            # Plot original image\n",
    "            axes[i, 0].imshow(img_np)\n",
    "            axes[i, 0].set_title(f'Original Image')\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            # Plot ground truth density\n",
    "            im1 = axes[i, 1].imshow(gt_density_np, cmap='jet')\n",
    "            axes[i, 1].set_title(f'GT Density (Count: {gt_count_val:.1f})')\n",
    "            axes[i, 1].axis('off')\n",
    "            plt.colorbar(im1, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
    "\n",
    "            # Plot predicted density\n",
    "            im2 = axes[i, 2].imshow(pred_density_np, cmap='jet')\n",
    "            axes[i, 2].set_title(f'Pred Density (Count: {pred_count_val:.1f})')\n",
    "            axes[i, 2].axis('off')\n",
    "            plt.colorbar(im2, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions if validation dataset exists\n",
    "try:\n",
    "    print(\"🎨 Visualizing sample predictions...\")\n",
    "    visualize_predictions(model, val_ds, device, num_samples=70)\n",
    "except NameError:\n",
    "    print(\"⚠️  Validation dataset not available. Please run the data preparation cells first.\")\n",
    "\n",
    "print(\"\\n✅ Training and evaluation complete!\")\n",
    "print(f\"🎯 Final model saved at: {config['save_path']}\")\n",
    "print(\"📊 Training history and visualizations have been generated above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc00417",
   "metadata": {},
   "source": [
    "# Test Prediction and Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e11a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset for inference\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir: str,\n",
    "        base_size: int = 768\n",
    "    ) -> None:\n",
    "        self.img_paths: List[str] = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "        self.base_size: int = base_size\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, str]:\n",
    "        img_path: str = self.img_paths[index]\n",
    "        img = imread_rgb(img_path)\n",
    "        canvas, _, _, _ = letterbox(img, target=self.base_size)\n",
    "\n",
    "        t: Tensor = self.to_tensor(canvas)\n",
    "        t = self.normalize(t)\n",
    "\n",
    "        image_name: str = os.path.basename(img_path)\n",
    "        return t, image_name\n",
    "\n",
    "\n",
    "# Setup test dataset and loader\n",
    "test_ds = TestDataset(config['test_dir'], base_size=config['base_size'])\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"📦 Test dataset: {len(test_ds)} samples\")\n",
    "print(f\"🚀 Test loader: {len(test_loader)} batches\")\n",
    "\n",
    "\n",
    "def generate_submission(\n",
    "    model: torch.nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    output_file: str = \"submission.csv\"\n",
    ") -> pd.DataFrame:\n",
    "    model.eval()\n",
    "    predictions: List[int] = []\n",
    "    image_names: List[str] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, names in test_loader:\n",
    "            images = images.to(device)\n",
    "            pred: Tensor = model(images)\n",
    "            batch_preds: Tensor = pred.sum((1, 2, 3))\n",
    "\n",
    "            # Round the result so predicted_count will store as int\n",
    "            predictions.extend(batch_preds.cpu().numpy().round().astype(int).tolist())\n",
    "            image_names.extend(names)\n",
    "\n",
    "    submission_df: pd.DataFrame = pd.DataFrame({\n",
    "        \"image_id\": image_names,\n",
    "        \"predicted_count\": predictions\n",
    "    })\n",
    "\n",
    "    # Sort by image ID (assuming numeric names)\n",
    "    submission_df[\"sort_key\"] = submission_df[\"image_id\"].apply(\n",
    "        lambda x: int(os.path.splitext(x)[0])\n",
    "    )\n",
    "    submission_df = (\n",
    "        submission_df\n",
    "        .sort_values(\"sort_key\")\n",
    "        .drop(\"sort_key\", axis=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Submission saved to {output_file}\")\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "# Generate submission\n",
    "submission_df = generate_submission(model, test_loader)\n",
    "print(\"\\n📋 Sample submission:\")\n",
    "submission_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
