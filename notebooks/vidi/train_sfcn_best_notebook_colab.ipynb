{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e571435",
   "metadata": {},
   "source": [
    "# SFCN Training Notebook\n",
    "# Scale-aware Feature Pyramid Network for Crowd Counting\n",
    "\n",
    "This notebook implements SFCN with VGG16-BN backbone for crowd counting tasks.\n",
    "\n",
    "## Features:\n",
    "- VGG16-BN backbone with spatial encoder\n",
    "- Adaptive density map generation with KNN-based sigma\n",
    "- Advanced data augmentation with patch-based training\n",
    "- Mixed precision training with gradient accumulation\n",
    "- Early stopping and model checkpointing\n",
    "- Optional empty patch avoidance for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c03130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries and Setup\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "from glob import glob\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Constants\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f983175",
   "metadata": {},
   "source": [
    "## Dataset Download and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Kaggle secret key\n",
    "\n",
    "\"\"\"\n",
    "You don't need to run this cell if you're not in Google Colab environment\n",
    "\"\"\"\n",
    "\n",
    "!pip install -q kaggle\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup dataset in Colab\n",
    "\n",
    "\"\"\"\n",
    "You don't need to run this cell if you're not in Google Colab environment\n",
    "\"\"\"\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "zip_path = \"/content/penyisihan-hology-8-0-2025-data-mining.zip\"\n",
    "drive_extract_path = \"/content/drive/MyDrive/PROJECTS/Cognivio/Percobaan Hology 8 2025/dataset\"\n",
    "local_dataset_path = \"/content/dataset\"  # for current session\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Download zip (if not exists in /content)\n",
    "# ---------------------------\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Dataset not found locally, downloading...\")\n",
    "    !kaggle competitions download -c penyisihan-hology-8-0-2025-data-mining -p /content\n",
    "else:\n",
    "    print(\"Dataset already exists, skipping download.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Extract to Google Drive (for backup)\n",
    "# ---------------------------\n",
    "os.makedirs(drive_extract_path, exist_ok=True)\n",
    "\n",
    "if not os.listdir(drive_extract_path):  # Check if folder is empty\n",
    "    print(\"Extracting dataset to Google Drive...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(drive_extract_path)\n",
    "    print(\"Dataset extracted to:\", drive_extract_path)\n",
    "else:\n",
    "    print(\"Dataset already extracted at:\", drive_extract_path)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Copy dataset to local /content (faster training)\n",
    "# ---------------------------\n",
    "if not os.path.exists(local_dataset_path):\n",
    "    print(\"Copying dataset to Colab local storage (/content)...\")\n",
    "    !cp -r \"$drive_extract_path\" \"$local_dataset_path\"\n",
    "else:\n",
    "    print(\"Dataset already available in Colab local storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb73757",
   "metadata": {},
   "source": [
    "## Configuration and Hyperparameters\n",
    "\n",
    "Set up all training configuration parameters. You can modify these values to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data paths (modify these for local setup)\n",
    "    'img_dir': f\"{local_dataset_path}/train/images\",  # 'data/train/images' for local path\n",
    "    'label_dir': f\"{local_dataset_path}/train/labels\",  # 'data/train/labels' for local path\n",
    "    'test_dir': f\"{local_dataset_path}/test/images\",  # 'data/test/images' for local path\n",
    "\n",
    "    # Model parameters\n",
    "    'base_size': 768,\n",
    "    'down': 8,\n",
    "    'patch_size': 384,\n",
    "    'patches_per_image': 4,\n",
    "    'avoid_empty_patches': True,\n",
    "\n",
    "    # Training parameters\n",
    "    'batch_size': 4,\n",
    "    'epochs': 120,  # Increase for full training\n",
    "    'lr': 1e-4,\n",
    "    'criterion': 'mse',  # 'mse' or 'huber'\n",
    "    'count_loss_alpha': 0.0,  # Auxiliary count loss weight\n",
    "    'early_stop_patience': 15,\n",
    "\n",
    "    # Optimization\n",
    "    'num_workers': 4,\n",
    "    'amp': True,  # Automatic Mixed Precision\n",
    "    'accum_steps': 2,  # Gradient accumulation steps\n",
    "\n",
    "    # Augmentation and preprocessing\n",
    "    'sigma_mode': 'adaptive',  # 'adaptive' or 'constant'\n",
    "\n",
    "    # Model saving\n",
    "    'save_path': 'sfcn_best.pth',\n",
    "    'seed': 1337\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed: int = 1337) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config['seed'])\n",
    "print(f\"üé≤ Random seed set to {config['seed']}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d390b9d",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Implement essential utility functions for image processing, point parsing, and density map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438670d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 1337) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def imread_rgb(path: str) -> np.ndarray:\n",
    "    \"\"\"Read an RGB image using OpenCV and convert BGR‚ÜíRGB.\"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read image: {path}\")\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def letterbox(img: np.ndarray, target: int = 512) -> Tuple[np.ndarray, float, int, int]:\n",
    "    \"\"\"Resize and pad an image to a square canvas without distortion.\n",
    "\n",
    "    Returns the padded image, the scale factor used, and the left/top\n",
    "    padding applied. The output size is (target, target).\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    scale = min(target / h, target / w)\n",
    "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "    img_rs = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "    top = (target - nh) // 2\n",
    "    left = (target - nw) // 2\n",
    "    canvas = np.zeros((target, target, 3), dtype=img_rs.dtype)\n",
    "    canvas[top:top + nh, left:left + nw] = img_rs\n",
    "    return canvas, scale, left, top\n",
    "\n",
    "def parse_points_from_json(path: str) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Parse annotated points from a JSON file.\n",
    "\n",
    "    Supports several common crowd counting annotation formats. Returns an\n",
    "    array of shape (N, 2) containing [x, y] coordinates and, if present\n",
    "    in the JSON, the declared number of people.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    pts: List[List[float]]\n",
    "    num = None\n",
    "    if isinstance(obj, dict) and \"points\" in obj:\n",
    "        pts = obj[\"points\"]\n",
    "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
    "        if len(pts) > 0 and isinstance(pts[0], dict):\n",
    "            pts = [[p[\"x\"], p[\"y\"]] for p in pts if \"x\" in p and \"y\" in p]\n",
    "    elif isinstance(obj, dict) and \"annotations\" in obj:\n",
    "        pts = [[a[\"x\"], a[\"y\"]] for a in obj[\"annotations\"] if \"x\" in a and \"y\" in a]\n",
    "        num = obj.get(\"human_num\", obj.get(\"num_human\", None))\n",
    "    elif isinstance(obj, list):\n",
    "        pts = obj\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown JSON schema: {path}\")\n",
    "    pts_arr = np.array(pts, dtype=np.float32) if len(pts) > 0 else np.zeros((0, 2), np.float32)\n",
    "    return pts_arr, num\n",
    "\n",
    "def derive_json_path(lbl_dir: str, img_path: str) -> str:\n",
    "    \"\"\"Derive the corresponding JSON label path for a given image path.\"\"\"\n",
    "    name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    cand = os.path.join(lbl_dir, name + \".json\")\n",
    "    if os.path.exists(cand):\n",
    "        return cand\n",
    "    # Try matching trailing digits\n",
    "    m = re.findall(r\"\\d+\", name)\n",
    "    if m:\n",
    "        alt = os.path.join(lbl_dir, f\"{m[-1]}.json\")\n",
    "        if os.path.exists(alt):\n",
    "            return alt\n",
    "    # Fallback: any file starting with the same name\n",
    "    lst = glob(os.path.join(lbl_dir, f\"{name}*.json\"))\n",
    "    if lst:\n",
    "        return lst[0]\n",
    "    raise FileNotFoundError(f\"JSON label not found for {img_path}\")\n",
    "\n",
    "print(\"‚úÖ Basic utility functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d244612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_map(\n",
    "    points_xy: np.ndarray,\n",
    "    grid_size: int,\n",
    "    down: int = 8,\n",
    "    sigma_mode: str = \"adaptive\",\n",
    "    knn: int = 3,\n",
    "    beta: float = 0.3,\n",
    "    const_sigma: float = 2.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate a density map on a grid given annotated points.\n",
    "\n",
    "    The density map is of shape (grid_size//down, grid_size//down). Each\n",
    "    point is represented by a Gaussian whose sigma is either constant or\n",
    "    computed from the k-nearest neighbours.\n",
    "    \"\"\"\n",
    "    target = grid_size\n",
    "    dh, dw = target // down, target // down\n",
    "    den = np.zeros((dh, dw), dtype=np.float32)\n",
    "    if len(points_xy) == 0:\n",
    "        return den\n",
    "    # Scale points to the density map resolution\n",
    "    pts = points_xy.copy()\n",
    "    pts[:, 0] = pts[:, 0] * (dw / target)\n",
    "    pts[:, 1] = pts[:, 1] * (dh / target)\n",
    "    tree = KDTree(pts) if len(pts) > 1 else None\n",
    "    for (x, y) in pts:\n",
    "        # Determine sigma\n",
    "        if sigma_mode == \"adaptive\" and tree is not None and len(pts) > 3:\n",
    "            dists, _ = tree.query([x, y], k=min(knn + 1, len(pts)))\n",
    "            sigma = max(1.0, float(np.mean(dists[1:])) * beta)\n",
    "        else:\n",
    "            sigma = const_sigma\n",
    "        cx, cy = float(x), float(y)\n",
    "        rad = int(max(1, math.ceil(3 * sigma)))\n",
    "        x0, x1 = max(0, int(math.floor(cx - rad))), min(dw, int(math.ceil(cx + rad + 1)))\n",
    "        y0, y1 = max(0, int(math.floor(cy - rad))), min(dh, int(math.ceil(cy + rad + 1)))\n",
    "        if x1 <= x0 or y1 <= y0:\n",
    "            continue\n",
    "        xs = np.arange(x0, x1) - cx\n",
    "        ys = np.arange(y0, y1) - cy\n",
    "        xx, yy = np.meshgrid(xs, ys)\n",
    "        g = np.exp(-(xx**2 + yy**2) / (2 * sigma * sigma))\n",
    "        s = g.sum()\n",
    "        if s > 0:\n",
    "            den[y0:y1, x0:x1] += (g / s).astype(np.float32)\n",
    "    return den\n",
    "\n",
    "print(\"‚úÖ Density map generation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa65ce",
   "metadata": {},
   "source": [
    "## Dataset Implementation\n",
    "\n",
    "Create the CrowdDataset class with comprehensive data augmentation and preprocessing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdDataset(Dataset):\n",
    "    \"\"\"Custom dataset for crowd counting.\n",
    "\n",
    "    Supports optional random patch cropping on training data. When\n",
    "    `avoid_empty_patches` is set, the dataset will attempt to choose a\n",
    "    patch containing at least one point, falling back to a random crop if\n",
    "    it fails after a number of retries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir: str,\n",
    "        lbl_dir: str,\n",
    "        base_size: int = 768,\n",
    "        down: int = 8,\n",
    "        aug: bool = True,\n",
    "        mode: str = \"train\",\n",
    "        patch_size: int = 0,\n",
    "        patches_per_image: int = 1,\n",
    "        sigma_mode: str = \"adaptive\",\n",
    "        avoid_empty_patches: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_paths = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "        if len(self.img_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {img_dir}\")\n",
    "        self.lbl_dir = lbl_dir\n",
    "        self.base_size = base_size\n",
    "        self.down = down\n",
    "        self.aug = aug\n",
    "        self.mode = mode\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_per_image = max(1, int(patches_per_image))\n",
    "        self.sigma_mode = sigma_mode\n",
    "        self.avoid_empty_patches = avoid_empty_patches\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        self.color_jit = transforms.ColorJitter(0.1, 0.1, 0.1, 0.05)\n",
    "\n",
    "        # Compute effective length for patch training\n",
    "        if self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1:\n",
    "            self.effective_len = len(self.img_paths) * self.patches_per_image\n",
    "        else:\n",
    "            self.effective_len = len(self.img_paths)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.effective_len\n",
    "\n",
    "    def _load_img_pts(self, idx_base: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load an image and its transformed annotation points.\"\"\"\n",
    "        pimg = self.img_paths[idx_base]\n",
    "        img = imread_rgb(pimg)\n",
    "        h, w = img.shape[:2]\n",
    "        plbl = derive_json_path(self.lbl_dir, pimg)\n",
    "        pts, _ = parse_points_from_json(plbl)\n",
    "        # Augment: horizontal flip\n",
    "        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n",
    "            img = img[:, ::-1, :].copy()\n",
    "            if len(pts) > 0:\n",
    "                pts = pts.copy()\n",
    "                pts[:, 0] = (w - 1) - pts[:, 0]\n",
    "        # Augment: colour jitter\n",
    "        if self.mode == \"train\" and self.aug and random.random() < 0.5:\n",
    "            pil = transforms.ToPILImage()(img)\n",
    "            pil = self.color_jit(pil)\n",
    "            img = np.array(pil)\n",
    "        # Letterbox to base size\n",
    "        canvas, scale, left, top = letterbox(img, target=self.base_size)\n",
    "        if len(pts) > 0:\n",
    "            pts_tr = pts.copy()\n",
    "            pts_tr[:, 0] = pts_tr[:, 0] * scale + left\n",
    "            pts_tr[:, 1] = pts_tr[:, 1] * scale + top\n",
    "            # Clamp to canvas bounds\n",
    "            m = (\n",
    "                (pts_tr[:, 0] >= 0)\n",
    "                & (pts_tr[:, 0] < self.base_size)\n",
    "                & (pts_tr[:, 1] >= 0)\n",
    "                & (pts_tr[:, 1] < self.base_size)\n",
    "            )\n",
    "            pts_tr = pts_tr[m]\n",
    "        else:\n",
    "            pts_tr = np.zeros((0, 2), np.float32)\n",
    "        return canvas, pts_tr\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Map global index to an image index (for patch training)\n",
    "        if self.mode == \"train\" and self.patch_size > 0 and self.patches_per_image > 1:\n",
    "            idx_base = index // self.patches_per_image\n",
    "        else:\n",
    "            idx_base = index\n",
    "        idx_base %= len(self.img_paths)\n",
    "\n",
    "        img_lb, pts_tr = self._load_img_pts(idx_base)\n",
    "\n",
    "        # Optional patch cropping\n",
    "        if self.mode == \"train\" and self.patch_size > 0:\n",
    "            ps = self.patch_size\n",
    "            if ps > self.base_size:\n",
    "                raise ValueError(\"patch_size must be <= base_size\")\n",
    "            max_off = self.base_size - ps\n",
    "            pts_out = np.zeros((0, 2), np.float32)\n",
    "            # Attempt to find a non-empty patch when requested\n",
    "            for attempt in range(10) if self.avoid_empty_patches else [0]:\n",
    "                ox = 0 if max_off <= 0 else random.randint(0, max_off)\n",
    "                oy = 0 if max_off <= 0 else random.randint(0, max_off)\n",
    "                crop = img_lb[oy : oy + ps, ox : ox + ps, :]\n",
    "                if len(pts_tr) > 0:\n",
    "                    pts_c = pts_tr.copy()\n",
    "                    pts_c[:, 0] -= ox\n",
    "                    pts_c[:, 1] -= oy\n",
    "                    m = (\n",
    "                        (pts_c[:, 0] >= 0)\n",
    "                        & (pts_c[:, 0] < ps)\n",
    "                        & (pts_c[:, 1] >= 0)\n",
    "                        & (pts_c[:, 1] < ps)\n",
    "                    )\n",
    "                    pts_c = pts_c[m]\n",
    "                else:\n",
    "                    pts_c = np.zeros((0, 2), np.float32)\n",
    "                # If avoid_empty_patches is False, we break immediately (no retries)\n",
    "                if not self.avoid_empty_patches or len(pts_c) > 0 or attempt == 9:\n",
    "                    pts_out = pts_c\n",
    "                    img_out = crop\n",
    "                    break\n",
    "            grid = ps\n",
    "        else:\n",
    "            img_out = img_lb\n",
    "            pts_out = pts_tr\n",
    "            grid = self.base_size\n",
    "\n",
    "        # Build density map\n",
    "        den = make_density_map(\n",
    "            pts_out,\n",
    "            grid_size=grid,\n",
    "            down=self.down,\n",
    "            sigma_mode=self.sigma_mode,\n",
    "        )\n",
    "\n",
    "        # Convert to tensors and normalise\n",
    "        t = self.to_tensor(img_out)\n",
    "        t = self.normalize(t)\n",
    "        d = torch.from_numpy(den).unsqueeze(0)\n",
    "        c = torch.tensor([float(len(pts_out))], dtype=torch.float32)\n",
    "        return t, d, c\n",
    "\n",
    "print(\"‚úÖ CrowdDataset class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c2776",
   "metadata": {},
   "source": [
    "## Model Architecture (SFCN with VGG16)\n",
    "\n",
    "Implement the SFCN model with VGG16-BN backbone and spatial encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bcf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEncoder(nn.Module):\n",
    "    \"\"\"Simple spatial encoder that propagates information in four directions.\"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, k: int = 9) -> None:\n",
    "        super().__init__()\n",
    "        p = k // 2\n",
    "        self.h1 = nn.Conv2d(channels, channels, (1, k), padding=(0, p), groups=channels, bias=False)\n",
    "        self.h2 = nn.Conv2d(channels, channels, (1, k), padding=(0, p), groups=channels, bias=False)\n",
    "        self.v1 = nn.Conv2d(channels, channels, (k, 1), padding=(p, 0), groups=channels, bias=False)\n",
    "        self.v2 = nn.Conv2d(channels, channels, (k, 1), padding=(p, 0), groups=channels, bias=False)\n",
    "        self.proj = nn.Conv2d(channels * 4, channels, 1, bias=False)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = torch.cat([self.h1(x), self.h2(x), self.v1(x), self.v2(x)], dim=1)\n",
    "        return self.act(self.proj(y))\n",
    "\n",
    "\n",
    "class SFCN_VGG(nn.Module):\n",
    "    \"\"\"Simplified SFCN with VGG-16 backbone and spatial encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, pretrained: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16_bn(\n",
    "            weights=models.VGG16_BN_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        )\n",
    "        # Use features up to conv4_3 (stride 8)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33])\n",
    "        self.senc = SpatialEncoder(512, k=9)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.frontend(x)\n",
    "        x = self.senc(x)\n",
    "        x = self.head(x)\n",
    "        return torch.nn.functional.softplus(x)\n",
    "\n",
    "print(\"‚úÖ SFCN model architecture implemented!\")\n",
    "\n",
    "# Test model instantiation\n",
    "model_test = SFCN_VGG(pretrained=False)\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model_test.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 3, 384, 384)\n",
    "with torch.no_grad():\n",
    "    output = model_test(dummy_input)\n",
    "    print(f\"üîç Input shape: {dummy_input.shape}\")\n",
    "    print(f\"üéØ Output shape: {output.shape}\")\n",
    "\n",
    "del model_test, dummy_input, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910e90ad",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n",
    "\n",
    "Implement training and evaluation functions with mixed precision support and auxiliary count loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler: torch.cuda.amp.GradScaler = None,\n",
    "    accum_steps: int = 1,\n",
    "    criterion: str = \"mse\",\n",
    "    count_loss_alpha: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"Train the model for one epoch and return the mean absolute error.\"\"\"\n",
    "    model.train()\n",
    "    if criterion == \"mse\":\n",
    "        crit = nn.MSELoss()\n",
    "    elif criterion == \"huber\":\n",
    "        crit = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        raise ValueError(\"criterion must be 'mse' or 'huber'\")\n",
    "    running_mae, nimg = 0.0, 0\n",
    "    total_pred_count, total_gt_count = 0.0, 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, (imgs, dens, _) in enumerate(tqdm(loader, desc=\"Train\", leave=False), 1):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        if scaler is not None:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                preds = model(imgs)\n",
    "                map_loss = crit(preds, dens)\n",
    "                if count_loss_alpha > 0.0:\n",
    "                    pred_cnt = preds.sum(dim=(1, 2, 3))\n",
    "                    gt_cnt = dens.sum(dim=(1, 2, 3))\n",
    "                    cnt_loss = F.mse_loss(pred_cnt, gt_cnt)\n",
    "                    total_loss = (map_loss + count_loss_alpha * cnt_loss) / accum_steps\n",
    "                else:\n",
    "                    total_loss = map_loss / accum_steps\n",
    "            scaler.scale(total_loss).backward()\n",
    "            if step % accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            preds = model(imgs)\n",
    "            map_loss = crit(preds, dens)\n",
    "            if count_loss_alpha > 0.0:\n",
    "                pred_cnt = preds.sum(dim=(1, 2, 3))\n",
    "                gt_cnt = dens.sum(dim=(1, 2, 3))\n",
    "                cnt_loss = F.mse_loss(pred_cnt, gt_cnt)\n",
    "                total_loss = (map_loss + count_loss_alpha * cnt_loss) / accum_steps\n",
    "            else:\n",
    "                total_loss = map_loss / accum_steps\n",
    "            total_loss.backward()\n",
    "            if step % accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.no_grad():\n",
    "            pc = preds.sum(dim=(1, 2, 3)).detach().cpu().numpy()\n",
    "            gc = dens.sum(dim=(1, 2, 3)).detach().cpu().numpy()\n",
    "            running_mae += np.abs(pc - gc).sum()\n",
    "            total_pred_count += pc.sum()\n",
    "            total_gt_count += gc.sum()\n",
    "            nimg += imgs.size(0)\n",
    "    avg_pred = total_pred_count / max(1, nimg)\n",
    "    avg_gt = total_gt_count / max(1, nimg)\n",
    "    print(f\"Avg pred count: {avg_pred:.1f} vs GT {avg_gt:.1f}\")\n",
    "    return running_mae / max(1, nimg)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate the model on a validation set and compute MAE and RMSE.\"\"\"\n",
    "    model.eval()\n",
    "    mae, mse, nimg = 0.0, 0.0, 0\n",
    "    for imgs, dens, _ in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        pred = model(imgs)\n",
    "        diff = (pred.sum(dim=(1, 2, 3)) - dens.sum(dim=(1, 2, 3))).detach().cpu().numpy()\n",
    "        mae += np.abs(diff).sum()\n",
    "        mse += (diff ** 2).sum()\n",
    "        nimg += imgs.size(0)\n",
    "    return mae / max(1, nimg), math.sqrt(mse / max(1, nimg))\n",
    "\n",
    "print(\"‚úÖ Training and evaluation functions implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0e474",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Set up datasets and data loaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data directories exist\n",
    "img_dir = config['img_dir']\n",
    "label_dir = config['label_dir']\n",
    "\n",
    "print(f\"üîç Checking data directories...\")\n",
    "print(f\"   Images: {img_dir} -> {'‚úÖ Exists' if os.path.exists(img_dir) else '‚ùå Not found'}\")\n",
    "print(f\"   Labels: {label_dir} -> {'‚úÖ Exists' if os.path.exists(label_dir) else '‚ùå Not found'}\")\n",
    "\n",
    "if not os.path.exists(img_dir):\n",
    "    raise Exception(\"‚ùå Image directory not found. Please check the path and try again.\")\n",
    "\n",
    "# Build list of images and randomly shuffle before splitting\n",
    "all_imgs = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "random.shuffle(all_imgs)\n",
    "n_images = len(all_imgs)\n",
    "if n_images < 2:\n",
    "    raise ValueError(\"Need at least 2 images for training and validation\")\n",
    "n_val = max(1, int(0.1 * n_images))  # 10% for validation\n",
    "n_train = n_images - n_val\n",
    "train_imgs = all_imgs[:n_train]\n",
    "val_imgs = all_imgs[n_train:]\n",
    "\n",
    "print(f\"üìä Data split: {n_train} train, {n_val} validation\")\n",
    "\n",
    "# Instantiate datasets\n",
    "train_ds = CrowdDataset(\n",
    "    config['img_dir'],\n",
    "    config['label_dir'],\n",
    "    base_size=config['base_size'],\n",
    "    down=config['down'],\n",
    "    aug=True,\n",
    "    mode=\"train\",\n",
    "    patch_size=config['patch_size'],\n",
    "    patches_per_image=config['patches_per_image'],\n",
    "    sigma_mode=config['sigma_mode'],\n",
    "    avoid_empty_patches=config['avoid_empty_patches'],\n",
    ")\n",
    "val_ds = CrowdDataset(\n",
    "    config['img_dir'],\n",
    "    config['label_dir'],\n",
    "    base_size=config['base_size'],\n",
    "    down=config['down'],\n",
    "    aug=False,\n",
    "    mode=\"val\",\n",
    "    patch_size=0,\n",
    "    patches_per_image=1,\n",
    "    sigma_mode=config['sigma_mode'],\n",
    "    avoid_empty_patches=False,\n",
    ")\n",
    "\n",
    "# Override image paths after shuffling\n",
    "train_ds.img_paths = train_imgs\n",
    "val_ds.img_paths = val_imgs\n",
    "# Recompute effective lengths for patch training\n",
    "if train_ds.mode == \"train\" and train_ds.patch_size > 0 and train_ds.patches_per_image > 1:\n",
    "    train_ds.effective_len = len(train_ds.img_paths) * train_ds.patches_per_image\n",
    "else:\n",
    "    train_ds.effective_len = len(train_ds.img_paths)\n",
    "val_ds.effective_len = len(val_ds.img_paths)\n",
    "\n",
    "print(f\"üì¶ Training dataset: {len(train_ds)} samples\")\n",
    "print(f\"üì¶ Validation dataset: {len(val_ds)} samples\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Data loaders created!\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a2842",
   "metadata": {},
   "source": [
    "## Model Training Loop\n",
    "\n",
    "Initialize the model, optimizer, and execute the main training loop with early stopping and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and training components\n",
    "print(\"üèóÔ∏è  Initializing model and training components...\")\n",
    "\n",
    "# Create model\n",
    "model = SFCN_VGG(pretrained=True).to(device)\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if (config['amp'] and device.type == \"cuda\") else None\n",
    "print(f\"‚ö° Mixed precision: {'Enabled' if scaler is not None else 'Disabled'}\")\n",
    "\n",
    "# Training state\n",
    "best_mae = float(\"inf\")\n",
    "patience = config['early_stop_patience']\n",
    "bad_epochs = 0\n",
    "training_history = {\n",
    "    'train_mae': [],\n",
    "    'val_mae': [],\n",
    "    'val_rmse': [],\n",
    "    'epoch': []\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Setup complete! Starting training...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579dcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Loop\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    print(f\"\\nüìà Epoch {epoch}/{config['epochs']}\")\n",
    "    \n",
    "    # Training phase\n",
    "    tr_mae = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        device,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        accum_steps=config['accum_steps'],\n",
    "        criterion=config['criterion'],\n",
    "        count_loss_alpha=config['count_loss_alpha'],\n",
    "    )\n",
    "    \n",
    "    # Validation phase\n",
    "    va_mae, va_rmse = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Update training history\n",
    "    training_history['train_mae'].append(tr_mae)\n",
    "    training_history['val_mae'].append(va_mae)\n",
    "    training_history['val_rmse'].append(va_rmse)\n",
    "    training_history['epoch'].append(epoch)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"üìä Train MAE: {tr_mae:.3f} | Val MAE: {va_mae:.3f} | Val RMSE: {va_rmse:.3f}\")\n",
    "    print(f\"üéØ Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Model checkpointing\n",
    "    if va_mae + 1e-6 < best_mae:\n",
    "        best_mae = va_mae\n",
    "        bad_epochs = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"val_mae\": va_mae,\n",
    "            \"config\": config,\n",
    "            \"training_history\": training_history\n",
    "        }\n",
    "        torch.save(checkpoint, config['save_path'])\n",
    "        print(f\"‚úÖ New best model saved! MAE: {best_mae:.3f}\")\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        print(f\"‚è≥ No improvement for {bad_epochs} epoch(s)\")\n",
    "        \n",
    "        if bad_epochs >= patience:\n",
    "            print(f\"üõë Early stopping at epoch {epoch}. Best Val MAE: {best_mae:.3f}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üéâ Training completed! Best validation MAE: {best_mae:.3f}\")\n",
    "print(f\"üíæ Best model saved to: {config['save_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920c727",
   "metadata": {},
   "source": [
    "## Results Visualization and Analysis\n",
    "\n",
    "Visualize training progress and analyze model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046524a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if len(training_history['epoch']) > 0:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Training and validation MAE\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(training_history['epoch'], training_history['train_mae'], 'b-', label='Train MAE', linewidth=2)\n",
    "    plt.plot(training_history['epoch'], training_history['val_mae'], 'r-', label='Val MAE', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Validation RMSE\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(training_history['epoch'], training_history['val_rmse'], 'g-', label='Val RMSE', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Validation RMSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning curve analysis\n",
    "    plt.subplot(1, 3, 3)\n",
    "    train_mae = np.array(training_history['train_mae'])\n",
    "    val_mae = np.array(training_history['val_mae'])\n",
    "    gap = val_mae - train_mae\n",
    "    plt.plot(training_history['epoch'], gap, 'purple', label='Val-Train Gap', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE Gap')\n",
    "    plt.title('Generalization Gap')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print final statistics\n",
    "    print(\"üìà Training Statistics:\")\n",
    "    print(f\"   Final Train MAE: {training_history['train_mae'][-1]:.3f}\")\n",
    "    print(f\"   Final Val MAE: {training_history['val_mae'][-1]:.3f}\")\n",
    "    print(f\"   Final Val RMSE: {training_history['val_rmse'][-1]:.3f}\")\n",
    "    print(f\"   Best Val MAE: {min(training_history['val_mae']):.3f}\")\n",
    "    print(f\"   Total Epochs: {len(training_history['epoch'])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training history available. Please run the training loop first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74add868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final evaluation\n",
    "if os.path.exists(config['save_path']):\n",
    "    print(\"üìÅ Loading best model checkpoint...\")\n",
    "    checkpoint = torch.load(config['save_path'], map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(f\"‚úÖ Loaded model from epoch {checkpoint['epoch']} with MAE {checkpoint['val_mae']:.3f}\")\n",
    "\n",
    "# Final evaluation with detailed predictions\n",
    "print(\"\\nüéØ Final Evaluation - Ground Truth vs Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "image_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs, dens, _) in enumerate(val_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        dens = dens.to(device)\n",
    "        pred = model(imgs)\n",
    "        \n",
    "        pred_cnt_batch = pred.sum((1, 2, 3))\n",
    "        gt_cnt_batch = dens.sum((1, 2, 3))\n",
    "        \n",
    "        # Store results\n",
    "        for j in range(pred_cnt_batch.size(0)):\n",
    "            idx = batch_idx * config['batch_size'] + j\n",
    "            if idx < len(val_ds.img_paths):\n",
    "                image_name = os.path.basename(val_ds.img_paths[idx])\n",
    "                image_names.append(image_name)\n",
    "                predictions.append(pred_cnt_batch[j].item())\n",
    "                ground_truths.append(gt_cnt_batch[j].item())\n",
    "                \n",
    "                print(f\"{image_name:20s}: GT {gt_cnt_batch[j].item():6.1f}, Pred {pred_cnt_batch[j].item():6.1f}, \"\n",
    "                      f\"Error {abs(gt_cnt_batch[j].item() - pred_cnt_batch[j].item()):5.1f}\")\n",
    "\n",
    "# Calculate final metrics\n",
    "if len(predictions) > 0:\n",
    "    predictions = np.array(predictions)\n",
    "    ground_truths = np.array(ground_truths)\n",
    "    \n",
    "    mae = np.mean(np.abs(predictions - ground_truths))\n",
    "    mse = np.mean((predictions - ground_truths)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(\"\\nüìä Final Metrics:\")\n",
    "    print(f\"   MAE:  {mae:.3f}\")\n",
    "    print(f\"   MSE:  {mse:.3f}\")\n",
    "    print(f\"   RMSE: {rmse:.3f}\")\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(ground_truths, predictions, alpha=0.6, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    max_val = max(np.max(ground_truths), np.max(predictions))\n",
    "    min_val = min(np.min(ground_truths), np.min(predictions))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    plt.xlabel('Ground Truth Count')\n",
    "    plt.ylabel('Predicted Count')\n",
    "    plt.title(f'Ground Truth vs Predictions (MAE: {mae:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    correlation = np.corrcoef(ground_truths, predictions)[0, 1]\n",
    "    plt.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        f'Correlation: {correlation:.3f}',\n",
    "        transform=plt.gca().transAxes,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5)\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüîó Correlation coefficient: {correlation:.3f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No validation data processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe385f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions with density maps\n",
    "def visualize_predictions(\n",
    "    model,\n",
    "    dataset,\n",
    "    device,\n",
    "    num_samples=3\n",
    "):\n",
    "    \"\"\"Visualize model predictions with density maps.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            if i >= len(dataset):\n",
    "                break\n",
    "\n",
    "            # Get sample\n",
    "            img_tensor, gt_density, gt_count = dataset[i]\n",
    "            img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            # Model prediction\n",
    "            pred_density = model(img_tensor)\n",
    "\n",
    "            # Convert to numpy\n",
    "            img_np = img_tensor.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "            img_np = img_np * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)  # Denormalize\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "            gt_density_np = gt_density.squeeze().numpy()\n",
    "            pred_density_np = pred_density.squeeze().cpu().numpy()\n",
    "\n",
    "            # Calculate counts\n",
    "            gt_count_val = gt_density_np.sum()\n",
    "            pred_count_val = pred_density_np.sum()\n",
    "\n",
    "            # Plot original image\n",
    "            axes[i, 0].imshow(img_np)\n",
    "            axes[i, 0].set_title(f'Original Image')\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            # Plot ground truth density\n",
    "            im1 = axes[i, 1].imshow(gt_density_np, cmap='jet')\n",
    "            axes[i, 1].set_title(f'GT Density (Count: {gt_count_val:.1f})')\n",
    "            axes[i, 1].axis('off')\n",
    "            plt.colorbar(im1, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
    "\n",
    "            # Plot predicted density\n",
    "            im2 = axes[i, 2].imshow(pred_density_np, cmap='jet')\n",
    "            axes[i, 2].set_title(f'Pred Density (Count: {pred_count_val:.1f})')\n",
    "            axes[i, 2].axis('off')\n",
    "            plt.colorbar(im2, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions if validation dataset exists\n",
    "try:\n",
    "    print(\"üé® Visualizing sample predictions...\")\n",
    "    visualize_predictions(model, val_ds, device, num_samples=3)\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è  Validation dataset not available. Please run the data preparation cells first.\")\n",
    "\n",
    "print(\"\\n‚úÖ Training and evaluation complete!\")\n",
    "print(f\"üéØ Final model saved at: {config['save_path']}\")\n",
    "print(\"üìä Training history and visualizations have been generated above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b4c38",
   "metadata": {},
   "source": [
    "## Test Prediction and Generate Submission\n",
    "\n",
    "Create predictions for test set and generate submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d729f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset for inference\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, base_size: int = 768):\n",
    "        self.img_paths = sorted(glob(os.path.join(img_dir, \"*.*\")))\n",
    "        self.base_size = base_size\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        img = imread_rgb(img_path)\n",
    "        canvas, _, _, _ = letterbox(img, target=self.base_size)\n",
    "        t = self.to_tensor(canvas)\n",
    "        t = self.normalize(t)\n",
    "        image_name = os.path.basename(img_path)\n",
    "        return t, image_name\n",
    "\n",
    "# Setup test dataset and loader\n",
    "test_ds = TestDataset(config['test_dir'], base_size=config['base_size'])\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"üì¶ Test dataset: {len(test_ds)} samples\")\n",
    "print(f\"üöÄ Test loader: {len(test_loader)} batches\")\n",
    "\n",
    "def generate_submission(model, test_loader, output_file=\"submission.csv\"):\n",
    "    model.eval()\n",
    "    predictions, image_names = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, names in test_loader:\n",
    "            images = images.to(device)\n",
    "            pred = model(images)\n",
    "            batch_preds = pred.sum((1, 2, 3))\n",
    "            \n",
    "            # Round the result so predicted_count will store as int\n",
    "            predictions.extend(batch_preds.cpu().numpy().round().astype(int).tolist())\n",
    "            image_names.extend(names)\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"image_id\": image_names,\n",
    "        \"predicted_count\": predictions\n",
    "    })\n",
    "\n",
    "    # Sort by image ID (assuming numeric names)\n",
    "    submission_df[\"sort_key\"] = submission_df[\"image_id\"].apply(lambda x: int(os.path.splitext(x)[0]))\n",
    "    submission_df = submission_df.sort_values(\"sort_key\").drop(\"sort_key\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Submission saved to {output_file}\")\n",
    "    return submission_df\n",
    "\n",
    "# Generate submission\n",
    "submission_df = generate_submission(model, test_loader)\n",
    "print(\"\\nüìã Sample submission:\")\n",
    "print(submission_df.head(20))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
