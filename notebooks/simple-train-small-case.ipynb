{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97a71a8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/FarrelAD/Hology-8-2025-Data-Mining-PRIVATE/blob/main/notebooks/simple-train-small-case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c164df45",
   "metadata": {},
   "source": [
    "# Simple Object Detection - Small Case Training\n",
    "A minimal implementation for learning object detection with bounding boxes using 1-20 training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14296ec0",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c03bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DRIVE_DIR = '/content/drive/MyDrive/PROJECTS/Cognivio/Percobaan Hology 8 2025'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad04a14",
   "metadata": {},
   "source": [
    "## Simple Dataset Class for Small Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afeac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, max_samples=20, img_size=224):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Get only first max_samples images for small case training\n",
    "        all_images = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.image_files = all_images[:max_samples]\n",
    "        \n",
    "        print(f\"Using {len(self.image_files)} images for training\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        # Load and resize image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Load labels (YOLO format)\n",
    "        label_name = img_name.replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "\n",
    "        boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        # class_id, x_center, y_center, width, height (normalized)\n",
    "                        boxes.append([float(x) for x in parts[1:]])  # Skip class_id for simplicity\n",
    "\n",
    "        # If no boxes, add dummy box\n",
    "        if len(boxes) == 0:\n",
    "            boxes = [[0.5, 0.5, 0.1, 0.1]]  # center dummy box\n",
    "\n",
    "        # Convert to tensor\n",
    "        image_tensor = torch.FloatTensor(image).permute(2, 0, 1)\n",
    "        boxes_tensor = torch.FloatTensor(boxes)\n",
    "        \n",
    "        # For simplicity, just return first box (single object detection)\n",
    "        if len(boxes) > 0:\n",
    "            target_box = boxes_tensor[0]  # [x_center, y_center, width, height]\n",
    "        else:\n",
    "            target_box = torch.FloatTensor([0.5, 0.5, 0.1, 0.1])\n",
    "            \n",
    "        # Add object confidence (1 if object exists, 0 if dummy)\n",
    "        has_object = 1.0 if len(boxes) > 0 and not (boxes[0] == [0.5, 0.5, 0.1, 0.1]) else 0.0\n",
    "        target = torch.cat([torch.tensor([has_object]), target_box])  # [conf, x, y, w, h]\n",
    "\n",
    "        return image_tensor, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a44f0",
   "metadata": {},
   "source": [
    "## Simple CNN Model for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc199fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDetector, self).__init__()\n",
    "        \n",
    "        # Simple CNN backbone\n",
    "        self.features = nn.Sequential(\n",
    "            # First block\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 224 -> 112\n",
    "            \n",
    "            # Second block\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 112 -> 56\n",
    "            \n",
    "            # Third block\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 56 -> 28\n",
    "            \n",
    "            # Fourth block\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28 -> 14\n",
    "        )\n",
    "        \n",
    "        # Detection head - outputs 5 values: [confidence, x, y, w, h]\n",
    "        self.detector = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 5)  # [confidence, x_center, y_center, width, height]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        detection = self.detector(features)\n",
    "        \n",
    "        # Apply activations\n",
    "        confidence = torch.sigmoid(detection[:, 0:1])  # confidence between 0-1\n",
    "        bbox = torch.sigmoid(detection[:, 1:])          # bbox coordinates between 0-1\n",
    "        \n",
    "        return torch.cat([confidence, bbox], dim=1)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleDetector().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6cb78",
   "metadata": {},
   "source": [
    "## Load Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small training dataset (adjust paths as needed)\n",
    "train_dataset = SmallObjectDetectionDataset(\n",
    "    image_dir='/content/dataset/train/images',  # Update this path\n",
    "    label_dir='/content/dataset/train/labels',  # Update this path\n",
    "    max_samples=20,  # Use only 20 samples for small case\n",
    "    img_size=224\n",
    ")\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Training with {len(train_dataset)} samples\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15dece",
   "metadata": {},
   "source": [
    "## Simple Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_detection_loss(predictions, targets, lambda_coord=5.0):\n",
    "    \"\"\"\n",
    "    Simple loss function for object detection\n",
    "    predictions: [batch_size, 5] - [conf, x, y, w, h]\n",
    "    targets: [batch_size, 5] - [conf, x, y, w, h]\n",
    "    \"\"\"\n",
    "    # Confidence loss (binary cross entropy)\n",
    "    conf_loss = nn.BCELoss()(predictions[:, 0], targets[:, 0])\n",
    "    \n",
    "    # Coordinate loss (only for samples with objects)\n",
    "    has_object = targets[:, 0] > 0.5\n",
    "    if has_object.sum() > 0:\n",
    "        coord_loss = nn.MSELoss()(\n",
    "            predictions[has_object, 1:], \n",
    "            targets[has_object, 1:]\n",
    "        )\n",
    "    else:\n",
    "        coord_loss = torch.tensor(0.0, device=device)\n",
    "    \n",
    "    total_loss = conf_loss + lambda_coord * coord_loss\n",
    "    \n",
    "    return total_loss, conf_loss, coord_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7e3dc",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50  # More epochs for small dataset\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "training_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_conf_loss = 0.0\n",
    "    epoch_coord_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        total_loss, conf_loss, coord_loss = simple_detection_loss(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_conf_loss += conf_loss.item()\n",
    "        epoch_coord_loss += coord_loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    avg_conf_loss = epoch_conf_loss / num_batches\n",
    "    avg_coord_loss = epoch_coord_loss / num_batches\n",
    "    \n",
    "    training_losses.append(avg_loss)\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Total Loss: {avg_loss:.4f}')\n",
    "        print(f'  Conf Loss: {avg_conf_loss:.4f}')\n",
    "        print(f'  Coord Loss: {avg_coord_loss:.4f}')\n",
    "        print()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_losses)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fc5e5",
   "metadata": {},
   "source": [
    "## Prediction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5185d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, image_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict object in a single image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    \n",
    "    # Resize and normalize\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    input_tensor = torch.FloatTensor(resized_image).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "    \n",
    "    # Extract prediction values\n",
    "    confidence = prediction[0, 0].item()\n",
    "    x_center = prediction[0, 1].item()\n",
    "    y_center = prediction[0, 2].item()\n",
    "    width = prediction[0, 3].item()\n",
    "    height = prediction[0, 4].item()\n",
    "    \n",
    "    # Convert to absolute coordinates\n",
    "    x_center_abs = x_center * orig_w\n",
    "    y_center_abs = y_center * orig_h\n",
    "    width_abs = width * orig_w\n",
    "    height_abs = height * orig_h\n",
    "    \n",
    "    # Calculate bounding box corners\n",
    "    x1 = int(x_center_abs - width_abs/2)\n",
    "    y1 = int(y_center_abs - height_abs/2)\n",
    "    x2 = int(x_center_abs + width_abs/2)\n",
    "    y2 = int(y_center_abs + height_abs/2)\n",
    "    \n",
    "    detection = {\n",
    "        'confidence': confidence,\n",
    "        'bbox': [x1, y1, x2, y2],\n",
    "        'has_object': confidence > conf_threshold\n",
    "    }\n",
    "    \n",
    "    return image, detection\n",
    "\n",
    "def visualize_detection(image, detection, title=\"Object Detection Result\"):\n",
    "    \"\"\"\n",
    "    Visualize detection result\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    if detection['has_object']:\n",
    "        x1, y1, x2, y2 = detection['bbox']\n",
    "        confidence = detection['confidence']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                           fill=False, color='red', linewidth=3)\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        # Add confidence text\n",
    "        plt.text(x1, y1-10, f'Person: {confidence:.3f}',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"red\", alpha=0.8),\n",
    "                fontsize=12, color='white', weight='bold')\n",
    "        \n",
    "        count_text = \"1 person detected\"\n",
    "    else:\n",
    "        count_text = \"No person detected\"\n",
    "    \n",
    "    plt.title(f'{title}\\n{count_text}', fontsize=14, weight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return count_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae29e32",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on training images to see if model learned\n",
    "print(\"Testing model on training images:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_image_dir = '/content/dataset/train/images'  # Update this path\n",
    "sample_images = [f for f in os.listdir(test_image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "# Test on first 5 training images\n",
    "for i, img_name in enumerate(sample_images[:5]):\n",
    "    img_path = os.path.join(test_image_dir, img_name)\n",
    "    print(f\"\\nTesting image {i+1}: {img_name}\")\n",
    "    \n",
    "    try:\n",
    "        image, detection = predict_single_image(model, img_path, conf_threshold=0.3)\n",
    "        \n",
    "        print(f\"Confidence: {detection['confidence']:.3f}\")\n",
    "        print(f\"Has object: {detection['has_object']}\")\n",
    "        if detection['has_object']:\n",
    "            print(f\"Bounding box: {detection['bbox']}\")\n",
    "        \n",
    "        # Visualize\n",
    "        count_result = visualize_detection(image, detection, f\"Test Image {i+1}: {img_name}\")\n",
    "        print(f\"Result: {count_result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_name}: {e}\")\n",
    "    \n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eae193",
   "metadata": {},
   "source": [
    "## Model Summary and People Counting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08756ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_people_in_image(model, image_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Count people in an image (simplified version - single detection)\n",
    "    \"\"\"\n",
    "    image, detection = predict_single_image(model, image_path, conf_threshold)\n",
    "    \n",
    "    # For this simple model, we can only detect 1 person max\n",
    "    people_count = 1 if detection['has_object'] else 0\n",
    "    \n",
    "    result = {\n",
    "        'image': image,\n",
    "        'people_count': people_count,\n",
    "        'confidence': detection['confidence'],\n",
    "        'detection': detection\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nPeople Counting Example:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if len(sample_images) > 0:\n",
    "    test_image = os.path.join(test_image_dir, sample_images[0])\n",
    "    result = count_people_in_image(model, test_image, conf_threshold=0.3)\n",
    "    \n",
    "    print(f\"People count: {result['people_count']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    \n",
    "    visualize_detection(\n",
    "        result['image'], \n",
    "        result['detection'], \n",
    "        f\"People Count: {result['people_count']}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nModel Training Complete!\")\n",
    "print(f\"This simple model can detect single objects with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(\"For multiple object detection, you would need a more complex architecture like YOLO or R-CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc23d0",
   "metadata": {},
   "source": [
    "## Save the Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'training_loss': training_losses,\n",
    "    'model_config': {\n",
    "        'img_size': 224,\n",
    "        'num_epochs': num_epochs,\n",
    "        'num_training_samples': len(train_dataset)\n",
    "    }\n",
    "}, 'simple_detector_small_case.pth')\n",
    "\n",
    "print(\"Model saved as 'simple_detector_small_case.pth'\")\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"- Training samples: {len(train_dataset)}\")\n",
    "print(f\"- Epochs: {num_epochs}\")\n",
    "print(f\"- Final loss: {training_losses[-1]:.4f}\")\n",
    "print(f\"- Model size: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
